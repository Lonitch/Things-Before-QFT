\chapter{Tensor Calculus}

\section{Total and Partial Derivatives}
To understand the different kinds of derivatives, let's say we have a function $\rho(t, x(t), p(t))$ which, in general, depends on the location $x(t)$ and momentum $p(t)$ plus the time $t.$ A key observation is that the location $x(t)$ and momentum $p(t)$ are functions of $t$ too. Therefore, we need to be extremely careful what we mean when we calculate the derivative with respect to the time $t .$
$$
\frac{d \rho}{d t}=\lim _{\Delta t \rightarrow 0} \frac{\rho(t+\Delta t, x(t+\Delta t), p(t+\Delta t))-\rho(t, x(t), p(t))}{\Delta t}
$$
\textbf{The result is the total rate of change of $\rho$}.
$$
\frac{\partial \rho}{\partial t}=\lim _{\Delta t \rightarrow 0} \frac{\rho(t+\Delta t, x(t), p(t))-\rho(t, x(t), p(t))}{\Delta t}
$$
The key difference is that we only vary $t$ if it appears explicitly
in $\rho$ but not if it only appears implicitly because $x(t)$ and $p(t)$
also depend on $t .$ Thus
$$
\frac{d \rho}{d t}=\frac{\partial \rho}{\partial x} \frac{d x}{d t}+\frac{\partial \rho}{\partial p} \frac{d p}{d t}+\frac{\partial \rho}{\partial t}
$$
\section{Taylor Expansion}
In general, we want to estimate the value of some function $f(x)$ at some value of $x$ by using our knowledge of the function's value at some fixed point $a .$ The Taylor series then reads
\begin{equation}
\begin{aligned}
f(x)=& \sum_{n=0}^{\infty} \frac{f^{(n)}(a)(x-a)^{n}}{n !} \\
=& \frac{f^{(0)}(a)(x-a)^{0}}{0 !}+\frac{f^{(1)}(a)(x-a)^{1}}{1 !}+\frac{f^{(2)}(a)(x-a)^{2}}{2 !} \\
&+\frac{f^{(3)}(a)(x-a)^{3}}{3 !}+\ldots
\end{aligned}
\end{equation}
or
\begin{equation}
f(x+a)=f(x)+(a \cdot \partial) f(x)+\frac{1}{2}(a \cdot \partial)^{2} f(x)+\cdots
\end{equation}
Taylor expansion of a scalar field (function $f$ that maps $\mathbb{R}^{n}$ to $\mathbb{R}$).Now, identify $\partial f / \partial t$ as $\hat{\boldsymbol{n}} \cdot \nabla f .$ In addition, see that $t \hat{\boldsymbol{n}}=\boldsymbol{x}-\boldsymbol{x}_{0} .$ Some clever recombining of terms gives
\begin{equation}
f(x)=f\left(x_{0}\right)+\left.\left(x-x_{0}\right) \cdot \nabla f\right|_{x_{0}}+\left.\frac{1}{2}\left(\left[x-x_{0}\right] \cdot \nabla\right)^{2} f\right|_{x_{0}}+\ldots
\end{equation}
and
\begin{equation}
\hat{\boldsymbol{n}} \cdot \nabla=\partial_{t}
\end{equation}

\section{Vector Identities}
\begin{equation}
\begin{aligned}
&\vec{\nabla} \cdot(\vec{\nabla} \times \vec{A}) \equiv \operatorname{div}(\operatorname{rot} \vec{A})=(\vec{\nabla} \times \vec{\nabla}) \cdot \vec{A} \equiv 0\\
&\vec{\nabla} \times(\vec{\nabla} \varphi) \equiv \operatorname{rot} \operatorname{grad} \varphi=(\vec{\nabla} \times \vec{\nabla}) \varphi \equiv 0
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
&\vec{\nabla} \cdot(\vec{A} \varphi)=\varphi \vec{\nabla} \cdot \vec{A}+\vec{A} \cdot \vec{\nabla} \varphi \quad \Longleftrightarrow \quad \operatorname{div}(\vec{A} \varphi)=\varphi \operatorname{div} \vec{A}+\vec{A} \cdot \operatorname{grad} \varphi\\
&\vec{\nabla} \times(\vec{A} \varphi)=\varphi \vec{\nabla} \times \vec{A}-\vec{A} \times \vec{\nabla} \varphi \quad \Longleftrightarrow \quad \operatorname{rot}(\vec{A} \varphi)=\varphi \operatorname{rot} \vec{A}-\vec{A} \times \operatorname{grad} \varphi\\
&\vec{\nabla} \cdot(\vec{A} \times \vec{B})=\vec{B} \cdot(\vec{\nabla} \times \vec{A})-\vec{A} \cdot(\vec{\nabla} \times \vec{B}) \quad \Longleftrightarrow \quad \operatorname{div}(\vec{A} \times \vec{B})=\vec{B} \cdot \operatorname{rot} \vec{A}-\vec{A} \cdot \operatorname{rot} \vec{B}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\vec{\nabla} \times(\vec{A} \times \vec{B}) &=(\vec{B} \cdot \vec{\nabla}) \vec{A}-(\vec{A} \cdot \vec{\nabla}) \vec{B}+\vec{A}(\vec{\nabla} \cdot \vec{B})-\vec{B}(\vec{\nabla} \cdot \vec{A}) \\
& \Longleftrightarrow \operatorname{rot}(\vec{A} \times \vec{B})=(\vec{B} \operatorname{grad}) \vec{A}-(\vec{A} \operatorname{grad}) \vec{B}+\vec{A}(\operatorname{div} \vec{B})-\vec{B}(\operatorname{div} \vec{A})
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\vec{\nabla}(\vec{A} \cdot \vec{B})=&(\vec{B} \cdot \vec{\nabla}) \vec{A}+(\vec{A} \cdot \vec{\nabla}) \vec{B}+\vec{A} \times(\vec{\nabla} \times \vec{B})+\vec{B} \times(\vec{\nabla} \times \vec{A}) \\
& \Longleftrightarrow \operatorname{grad}(\vec{A} \cdot \vec{B})=(\vec{B} \cdot \operatorname{grad}) \vec{A}+(\vec{A} \cdot \operatorname{grad}) \vec{B}+\vec{A} \times \operatorname{rot} \vec{B}+\vec{B} \times \operatorname{rot} \vec{A}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
&\vec{\nabla} \cdot(\vec{\nabla} \varphi) \equiv \operatorname{div}(\operatorname{grad} \varphi) \equiv \Delta \varphi=\frac{\partial^{2} \varphi}{\partial x^{2}}+\frac{\partial^{2} \varphi}{\partial y^{2}}+\frac{\partial^{2} \varphi}{\partial z^{2}}, \quad \Delta=\text { Laplace Operator }\\
&\vec{\nabla} \times(\vec{\nabla} \times \vec{A}) \equiv \operatorname{rot}(\operatorname{rot} \vec{A})=\vec{\nabla}(\vec{\nabla} \cdot \vec{A})-(\vec{\nabla} \cdot \vec{\nabla}) \vec{A} \equiv \operatorname{grad} \operatorname{div} \vec{A}-\Delta \vec{A}
\end{aligned}
\end{equation}

\section{Linear Spaces, Vectors, and Tensors}
The linear space, say $L$, consists of the elements (vectors) that \textbf{permit linear operations with the properties described below}:
\begin{itemize}
    \item Summing up the vectors
    \item Multiplication by a number
\end{itemize}
\begin{qt}
    Consider the set of vectors, elements of the linear space L
    $$
\left\{\mathbf{a}_{1}, \mathbf{a}_{2}, \ldots, \mathbf{a}_{n}\right\}=\left\{\mathbf{a}_{i} | i=1, \ldots, n\right\}
$$
and the set of real numbers $k_{1}, k_{2}, \ldots k_{n} .$ Vector
$$
\mathbf{k}=\sum_{i=1}^{n} k^{i} \mathbf{a}_{i}=\mathbf{k}^{i} \mathbf{a}_{i}=0\Longrightarrow \sum_{i=1}^{n}\left(k^{i}\right)^{2}=0
$$
then vectors $\left\{\mathbf{a}_{\mathbf{i}}, i=1, \ldots, n\right\}$ are called \textbf{linearly independent}. The last condition means that no one of the coefficients $k_{i}$ can be different from zero.
\end{qt}
For example, in 2D, two vectors are linearly dependent if and only if they are parallel; in 3D, three vectors are linearly dependent if and only if they belong to the same plane, etc.
\begin{defi}
        The maximal number of linearly independent elements of the linear space $L$ is called its dimension. It proves useful to denote $L_{D}$ a linear space of dimension $D .$
\end{defi}
\begin{thm}
Consider a $D$-dimensional linear space $L_{D}$ and the set of linearly independent vectors $\mathbf{e}_{i}=\left(\mathbf{e}_{1}, \mathbf{e}_{2}, \ldots, \mathbf{e}_{D}\right) .$ Then, for any vector a one can write
\begin{equation}
    \mathbf{a}=\sum_{i=1}^{D} a^{i} \mathbf{e}_{i}=a^{i} \mathbf{e}_{i}
    \label{vector-basis}
\end{equation}
where the coefficients $a^{i}$ are defined in a unique way.
\end{thm}
\bluep{The coefficients $a^{i}$ are called components or \textbf{contravariant components of the vector $\mathbf{a}$}}.The word “contravariant” here means that the components $a^i$ have upper indices.

\section{Direct product of the two linear spaces}
Let's consider the example of phase space in the classic mechanics. The coordinate system in the linear space $L_{D}$ consists of the initial point $O$ and the basis $\mathbf{e}_{i} .$ The position of a point $P$ can be characterized by \textbf{its position vector or radius vector $\mathbf{r}=\overrightarrow{O P}=x^{i} \mathbf{e}_{i}$}.

If the phase space is describing one particle, the space is composed of the radius vectors $\mathbf{r}$ and velocities $\mathbf{v}$ of the particle:
$$
\mathbf{r}=x^{i} \mathbf{e}_{i}, \quad \mathbf{v}=v^{j} \mathbf{f}_{j}
$$
The two set of bases can be related and independent. This example is a particular case of the linear space which is called \redp{a direct product
of the two linear spaces}. The element of the direct product of the two linear spaces $L_{D_{1}}$ and $L_{D_{2}}$ (\textbf{they can have different dimensions}) is the ordered set of the elements of each of the two spaces $L_{D_{1}}$ and $L_{D_{2}} .$
\begin{qt}
    The notation for the basis in the case of configuration space is $\mathbf{e}_{i} \otimes \mathbf{f}_{j}$. Hence, the state of the point-like particle in the phase space is characterized by the element of this linear space, which can be presented as:
    $$
x^{i} v^{j} \mathbf{e}_{i} \otimes \mathbf{f}_{j}
$$
In general, one can define the space that is a direct product of several linear spaces with different individual basis sets $\mathbf{e}_{i}$ each. In this case, we will have $\mathbf{e}_{i}^{(1)}, \mathbf{e}_{i}^{(2)}, \ldots$ $\mathbf{e}_{i}^{(N)} .$ The basis in the direct product space will be
$$
\mathbf{e}_{i_{1}}^{(1)} \otimes \mathbf{e}_{i_{2}}^{(2)} \otimes \ldots \otimes \mathbf{e}_{i_{N}}^{(N)}
$$
and the element becomes
$$
T^{i_{1} i_{2} \ldots i_{N}} \mathbf{e}_{i_{1}}^{(1)} \otimes \mathbf{e}_{i_{2}}^{(2)} \otimes \ldots \otimes \mathbf{e}_{i_{N}}^{(N)}
$$
\end{qt}
\section{Vector basis and its transformation}
Let us start from the components of the vector, which were defined in (\ref{vector-basis}). Consider, along with the original basis $\mathbf{e}_{i},$ another basis $\mathbf{e}_{i}^{\prime} .$ since each vector of the new basis belongs to the same space, it can be expanded using the original basis as
\begin{equation}
\mathbf{e}_{i}^{\prime}=\wedge_{i^{\prime}}^{j} \mathbf{e}_{j}
\label{basis-transform}
\end{equation}
and
$$
\mathbf{a}=a^{i} \mathbf{e}_{i}=a^{j^{\prime}} \mathbf{e}_{j^{\prime}}=a^{j^{\prime}} \wedge_{j^{\prime}}^{i} \mathbf{e}_{i}
$$
\begin{qt}
\begin{equation}
    a^{i}=a^{j^{\prime}} \wedge_{j^{\prime}}^{i}
    \label{cotravariant-coord-transform}
\end{equation}
\end{qt}

Similarly, we can make inverse transformation:
$$
a^{k^{\prime}}=\left(\wedge^{-1}\right)_{l}^{k^{\prime}} a^{l}
$$
and
\[
\left(\wedge^{-1}\right)_{l}^{k^{\prime}} \cdot \wedge_{i^{\prime}}^{l}=\delta_{i^{\prime}}^{k^{\prime}} \quad \text { and } \quad \wedge_{i^{\prime}}^{l} \cdot\left(\wedge^{-1}\right)_{k}^{i^{\prime}}=\delta_{k}^{l}
\]
\begin{qt}
Taking the partial derivatives, we arrive at the relations
\[
\wedge_{j^{\prime}}^{i}=\frac{\partial x^{i}}{\partial x^{j^{\prime}}} \quad \text { and } \quad\left(\wedge^{-1}\right)_{l}^{k^{\prime}}=\frac{\partial x^{k^{\prime}}}{\partial x^{l}}
\]
and
$$
\left(\wedge^{-1}\right)_{l}^{k^{\prime}} \cdot \wedge_{k^{\prime}}^{i}=\frac{\partial x^{k^{\prime}}}{\partial x^{l}} \frac{\partial x^{i}}{\partial x^{k^{\prime}}}=\frac{\partial x^{i}}{\partial x^{l}}=\delta_{l}^{i}
$$
is nothing but the chain rule for partial derivatives.
\end{qt}

\section{Scalar, vector, and tensor fields}
\begin{defi}
        Function $\varphi(x)$ is called scalar field or simply scalar if it does not transform under the change of coordinates
        \begin{equation}
\varphi(x)=\varphi^{\prime}\left(x^{\prime}\right)
\end{equation}
\end{defi}
Let us give a clarifying example in 1D. Consider a function
$$
y=x^2
$$
Now, let us change the variables 
$$
x^{\prime}=x+1
$$
The function $y=\left(x^{\prime}\right)^{2},$ obviously, represents another parabola. \bluep{In order to preserve the plot intact, we need to modify the form of the function, that is, to go from $\varphi$ to $\varphi^{\prime}$.} The new function $y^{\prime}=\left(x^{\prime}-1\right)^{2}$ will represent the original parabola,because the change of the variable is completely compensated by the change of the form of the function.
\begin{mybox}
\begin{center}
    Discuss whether the three numbers temperature $T(x)$, pressure $p(x)$ and density $\rho(x)$ form a contravariant vector
\end{center}
\end{mybox}
\begin{mybox2}
We can only form contravariant vector if the numbers transform by following the rule (\ref{cotravariant-coord-transform}). Since these numbers are scalar fields, it transforms like $T(\mathbf{r})=T^{\prime}(\mathbf{r}^{\prime})$. Thus, these three parameters can not form a contravariant vector.
\end{mybox2}
\begin{example}
From the definition above we know $\varphi^{\prime}(x) \neq \varphi(x)$ and $\varphi\left(x^{\prime}\right) \neq \varphi(x)$. Let us calculate these quantities explicitly for the special case of infinitesimal transformation $x^{\prime i}=x^{i}+\xi^{i}$ where $\xi$ are constant coefficients. Now we have
$$
\varphi\left(x^{\prime}\right)=\varphi\left(x+\xi\right)\overset{Taylor}{=}\varphi(x)+\frac{\partial \varphi}{\partial x^{i}} \xi^{i}
$$
and,
$$
\varphi^{\prime}\left(x^{i}\right)=\varphi^{\prime}\left(x^{i}-\xi^{i}\right)=\varphi^{\prime}\left(x^{\prime}\right)-\frac{\partial \varphi^{\prime}}{\partial x^{i^{\prime}}} \cdot \xi^{i}
$$
rewrite the two equations above, we find:
$$
\frac{\partial \varphi^{\prime}}{\partial x^{\prime i}}=\frac{\varphi^{\prime}\left(x^{\prime i}\right)-\varphi^{\prime}\left(x^{i}-\xi^{i}\right)}{\xi^i}
$$
$$
\frac{\partial \varphi}{\partial x^{i}}=\frac{\varphi(x^{\prime i})-\varphi(x^i)}{\xi^i}
$$
At the limit of $\xi\rightarrow0$, we have
$$
\frac{\partial \varphi^{\prime}}{\partial x^{\prime i}}=\frac{\partial \varphi(x)}{\partial x^{i}}+\mathcal{O}(\xi)
$$
As mentioned above, 
$$
\varphi^{\prime}\left(x^{i}\right)=\varphi^{\prime}\left(x^{\prime}\right)-\frac{\partial \varphi^{\prime}}{\partial x^{\prime i}} \cdot \xi^{i}\\
=\varphi^{\prime}\left(x^{\prime}\right)-\frac{\partial \varphi(x)}{\partial x^{i}}\cdot\xi^i
$$
Since $\varphi(x)=\varphi^{\prime}\left(x^{\prime}\right)$, we have
$$
\varphi^{\prime}(x)=\varphi(x)-\xi^{i} \partial_{i} \varphi
$$
\textbf{where we have introduced a useful notation $\partial_{i}=\partial / \partial x^{i}$.}
\end{example}
\textbf{For the vector field, we have the following rule for the coordinate transformation}. The set of three functions $\left\{a^{i}(x)\right\}=\left\{a^{1}(x), a^{2}(x), a^{3}(x)\right\}$ forms contravariant vector field (or simply vector field) if they transform, under the change of coordinates $\left\{x^{i}\right\} \rightarrow\left\{x^{\prime} j\right\},$ as
\begin{qt}
\begin{equation}
a^{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}}}{\partial x^{i}} \cdot a^{i}(x)
\end{equation}
\end{qt}
\bluep{The components of the vector in a given geometrical point of space modify under the coordinate transformation, while the scalar field does not.}The scalar and vector fields can be considered as examples of the more general objects called tensors. Tensors are also defined through their transformation rules.
\begin{qt}
The set of $3^n$ functions $\left\{a^{i_{1} \ldots i_{n}}(x)\right\}$ is called a contravariant tensor of rank $n,$ if these functions transform, under $x^{i} \rightarrow x^{i},$ as
\begin{equation}
a^{i_{1}^{i} \ldots i_{n}^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i_{1}^{\prime}}}{\partial x^{j_{1}}} \ldots \frac{\partial x^{i_{n}^{\prime}}}{\partial x^{j_{n}}} a^{j_{1} \ldots j_{n}}(x)
\end{equation}
\end{qt}

\section{Orthonormal Basis and Cartesian Coordiantes}
The scalar product of two vectors $\mathbf{a}$ and $\mathbf{b}$ in 3D is defined in a ususal way,
\begin{equation}
(\mathbf{a}, \mathbf{b})=\mathbf{a}\cdot\mathbf{b}=|\mathbf{a}| \cdot|\mathbf{b}| \cdot \cos \theta
\end{equation}
Special orthonormal basis $\left\{\hat{\mathbf{n}}_{a}\right\}$ is the one with
\begin{equation}
\left(\hat{\mathbf{n}}_{a}, \hat{\mathbf{n}}_{b}\right)=\delta_{a b}=\left\{\begin{array}{l}
{1 \text { if } a=b} \\
{0 \text { if } a \neq b}
\end{array}\right.
\end{equation}
\begin{example}
Making transformations of the basis vectors, verify that the change of coordinates
$$
x^{\prime}=\frac{x+y}{\sqrt{2}}+3, \quad y^{\prime}=\frac{x-y}{\sqrt{2}}-5
$$
does not modify the type of coordinates $x^{\prime}, y^{\prime},$ which remains Cartesian.
\textbf{Solution}:
$$
x^{\prime}=\frac{x+y}{\sqrt{2}}+3, \quad y^{\prime}=\frac{x-y}{\sqrt{2}}-5
$$
The change of initial point $(0,0) \rightarrow(3,-5)$ does not have relation to the change of basis. Then
$$
\begin{aligned}
x^{\prime} \hat{i}^{\prime}+y^{\prime} \hat{j}^{\prime} &=\left(\frac{x+y}{\sqrt{2}}\right) \hat{i}^{\prime}+\left(\frac{x-y}{\sqrt{2}}\right) \hat{j}^{\prime} \\
&=\frac{x}{\sqrt{2}}\left(\hat{i}^{\prime}+\hat{j}^{\prime}\right)+\frac{y}{\sqrt{2}}\left(\hat{i}^{\prime}-\hat{j}^{\prime}\right)=x \hat{i}+y \hat{j}
\end{aligned}
$$
Thus, $\hat{i}=\frac{1}{\sqrt{2}}\left(\hat{i}^{\prime}+\hat{j}^{\prime}\right), \hat{j}=\frac{1}{\sqrt{2}}\left(\hat{i}^{\prime}-\hat{j}^{\prime}\right) .$ Obviously, $\hat{i}^{2}=\hat{j}^{2}=1$ and $\hat{i} \cdot \hat{j}=0$
\end{example}
Now we can introduce a conjugated covariant basis.
\begin{qt}
Consider basis $\left\{\mathbf{e}_{i}\right\} .$ The conjugated basis is defined as a set of vectors $\left\{\mathbf{e}^{j}\right\}$ which satisfy the relations
\begin{equation}
\mathbf{e}_{i} \cdot \mathbf{e}^{j}=\delta_{i}^{j}
\end{equation}
The special property of the orthonormal basis is that $\hat{\mathbf{n}}^{a}=\hat{\mathbf{n}}_{a}$.

Any vector a can be expanded using the conjugated basis $\mathbf{a}=a_{i} \mathbf{e}^{i} .$ \redp{The coefficients $a_{i}$ are called covariant components of the vector $\mathbf{a}$}.
\end{qt}
In the case of covariant vector components, the transformation is done by means of the matrix inverse to the one for the contravariant components.
\begin{thm}
If we change the basis of the coordinate system from $\mathbf{e}_{i}$ to $\mathbf{e}_{i}^{\prime},$ then the covariant components of the vector a transform as
\begin{equation}
a_{i}^{\prime}=\frac{\partial x^{j}}{\partial x^{i}} a_{j}
\end{equation}
\end{thm}

 The set of three functions $\left\{A_{i}(x)\right\}$ forms a covariant vector field, if they transform from one coordinate system to another one as
 \begin{equation}
A_{i}^{\prime}\left(x^{\prime}\right)=\frac{\partial x^{j}}{\partial x^{i}} A_{j}(x)
\end{equation}
The set of $3^{n}$ functions $\left\{A_{i_{1} i_{2} \ldots i_{n}}(x)\right\}$ form a covariant tensor of rank $n$ if they transform from one coordinate system to another as
\begin{equation}
A_{i_{1} i_{2} \ldots i_{n}}^{\prime}\left(x^{\prime}\right)=\frac{\partial x^{j_{1}}}{\partial x^{i_{1}}} \frac{\partial x^{j_{2}}}{\partial x^{i_{2}}} \cdots \frac{\partial x^{j_{n}}}{\partial x^{n_{n}}} A_{j_{1} j_{2} \ldots j_{n}}(x)
\end{equation}
In general
\begin{qt}
The set of $3^{n+m}$ functions $\left\{B_{i_{1} \ldots i_{n}} j_{1 \ldots j_{n}}(x)\right\}$ forms the tensor of the type $(m, n),$ if these functions transform, under the change of coordinate basis, as
\begin{equation}
B_{i_{1}^{\prime} \ldots i_{n}^{\prime}} ^{j_{1}^{\prime} \ldots j_{m}^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}_1}}{\partial x^{l_{1}}} \cdots \frac{\partial x^{j_{m}^{\prime}}}{\partial x^{l_{m}}} \frac{\partial x^{k_{1}}}{\partial x^{\prime i_{1}}} \cdots \frac{\partial x^{k_{n}}}{\partial x^{\prime i_{n}}} B_{k_{1} \ldots k_{n}}^{l_{1} \ldots l_{m}}(x)
\end{equation}
Other possible names are the mixed tensor of covariant rank $n$ and contravariant rank $m,$ or simply $(m, n)$ -tensor.
\end{qt}
\textbf{Tensors are important due to the fact that they offer the coordinate-independent description of geometrical and physical laws.} The following example shows this observation:
\begin{example}
For an arbitrary $\mathbf{a}(x)=a^{i}(x) \mathbf{e}_{i}$ we have
$$
a^{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}}}{\partial x^{i}} a^{i}(x), \quad \mathbf{e}_{j^{\prime}}\left(x^{\prime}\right)=\mathbf{e}_{k}(x) \frac{\partial x^{k}}{\partial x^{j^{\prime}}}
$$
Then
$$
a^{j^{\prime}}\left(x^{\prime}\right) \mathbf{e}_{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}}}{\partial x^{i}} a^{i}(x) \mathbf{e}_{k}(x) \frac{\partial x^{k}}{\partial x^{j^{\prime}}}=\delta_{l}^{k} a^{i}(x) \mathbf{e}_{k}(x)=a^{i}(x) \mathbf{e}_{i}(x)
$$
\end{example}

If the Kronecker symbol transforms as a mixed $(1,1)$ tensor,
\begin{qt}
\begin{equation}
\delta_{j^{\prime}}^{i^{\prime}}=\frac{\partial x^{i^{\prime}}}{\partial x^{k}} \frac{\partial x^{l}}{\partial x^{j^{\prime}}} \delta_{l}^{k}=\frac{\partial x^{i^{\prime}}}{\partial x^{j^{\prime}}}
\end{equation}
\end{qt}
then in any coordinates $x^i$ it has the same form 
$$
\delta_{j}^{i}=\left\{\begin{array}{l}
{1 \text { if } i=j} \\
{0 \text { if } i \neq j}
\end{array}\right.
$$
\textbf{This property is very important, as it enables us to use the Kronecker symbol in any coordinates}

\begin{example}
Show that the product $A^{i}(x) B_{j}(x)$ of covariant and contravariant vectors transforms as a $(1,1)$ -type mixed tensor.
$$
A^{i^{\prime}}\left(x^{\prime}\right) A_{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{k}} A^{k}(x) \frac{\partial x^{l}}{\partial x^{j^{\prime}}} B_{l}(x)=\frac{\partial x^{i^{\prime}}}{\partial x^{k}} \frac{\partial x^{l}}{\partial x^{j^{\prime}}} A^{k}(x) B_{l}(x)
$$
\end{example}

\section{Orthogonal transformation}
The rotation transformation around $\hat{\mathbf{z}}-$axis is given by the following relation:
\begin{qt}
\begin{equation}
\left(\begin{array}{l}
{x} \\
{y} \\
{z}
\end{array}\right)=\hat{\wedge}_{z}\left(\begin{array}{l}
{x^{\prime}} \\
{y^{\prime}} \\
{z^{\prime}}
\end{array}\right), \quad \text { where } \quad \hat{\wedge}_{z}=\hat{\wedge}_{z}(\alpha)=\left(\begin{array}{ccc}
{\cos \alpha} & {-\sin \alpha} & {0} \\
{\sin \alpha} & {\cos \alpha} & {0} \\
{0} & {0} & {1}
\end{array}\right)
\end{equation}
the matrix above has the following property:
\begin{equation}
\hat{\wedge}_{z}^{T}=\hat{\wedge}_{z}^{-1}
\end{equation}
\end{qt}
\begin{defi}
        The matrix $\hat{\wedge}_{z}$ which satisfies $\hat{\wedge}_{z}^{-1}=\hat{\wedge}_{z}^{T}$ and the corresponding coordinate transformation is called \textbf{orthogonal}.
\end{defi}
Similarly, we can write the rotation matrix around other axis:
\begin{equation}
\hat{\wedge}_{x}(\gamma)=\left(\begin{array}{ccc}
{1} & {0} & {0}\\
{0} & {\cos \gamma} & {-\sin \gamma} \\
{0} & {\sin \gamma} & {\cos \gamma}
\end{array}\right)
\end{equation}
\begin{equation}
\hat{\wedge}_{y}(\beta)=\left(\begin{array}{ccc}
{\cos \beta} & {0} & {-\sin \beta} \\
{0} & {1} & {0}\\
{\sin \beta} & {0} &  {\cos \beta}
\end{array}\right)
\end{equation}
In 3D space, any rotation of the rigid body may be represented as a combination of the rotation around the axes $\hat{\mathbf{z}}, \hat{\mathbf{y}},$ and $\hat{\mathbf{x}}$ to the angles $ \alpha, \beta,$ and $\gamma$:
$$
\hat{\wedge}=\hat{\wedge}_{z}(\alpha) \hat{\wedge}_{y}(\beta) \hat{\wedge}_{x}(\gamma)
$$
Since $(A \cdot B)^{T}=B^{T} \cdot A^{T} \quad \text { and } \quad(A \cdot B)^{-1}=B^{-1} \cdot A^{-1}$, we can easily obtain that \bluep{the general 3D rotation matrix satisfies the orthogonal relation.}
\begin{qt}
For the orthogonal matrix, one can take the determinant and arrive at $det\hat{\wedge}=det\hat{\wedge}^{-1}$. Therefore, 
$$
det\hat{\wedge}=\pm1
$$
As far as any rotation matrix has a determinant equal to one, there must be some other orthogonal matrices with the determinant equal to −1.
\end{qt}
In case where the matrix elements are allowed to be complex, the matrix that satisfies the property $U^{\dagger}=U^{-1}$ is called unitary. The operation $U^{\dagger}$ is called \textbf{Hermitian conjugation} and consists of complex conjugation plus transposition $U^{\dagger}=\left(U^{*}\right)^{T}$.
\begin{example}
Consider the transformation from one orthonormal basis $\hat{\mathbf{n}}_{i}$ to another such basis $\hat{\mathbf{n}}_{k}^{\prime},$ namely, $\hat{\mathbf{n}}_{k}^{\prime}=R_{k}^{i} \hat{\mathbf{n}}_{i} .$ Prove that the matrix $\left\|R_{k}^{i}\right\|$ is orthogonal.

 $\hat{\mathbf{n}}_{k}^{\prime}=R_{k}^{i} \hat{\mathbf{n}}_{i}$ and also $\hat{\mathbf{n}}^{\prime \prime}=\left(R^{-1}\right)_{j}^{l} \hat{\mathbf{n}}^{j} .$ since $\hat{\mathbf{n}}_{k}^{\prime}=\hat{\mathbf{n}}^{k^{\prime}}$ and $\hat{\mathbf{n}}_{i}=\hat{\mathbf{n}}^{i}$
also have $\left(R^{-1}\right)_{j}^{l}=\left(R^{T}\right)_{j}^{l}$
\end{example}

\section{Operations over Tensors, Metric Tensor}
\textbf{Multiplication of a tensor by a number produces a tensor of the same type.} This operation is equivalent to the multiplication of all tensor components to the same number $\alpha$, namely:
\begin{equation}
(\alpha A)_{i_{1} \ldots i_{n}}=\alpha \cdot A_{i_{1} \ldots i_{n}}^{j_{1} \ldots j_{m}}
\end{equation}
\textbf{Multiplication of two tensors is defined for a couple of tensors of any type.} The product of a (m, n)-tensor and a (t, s)-tensor results in the (m + t, n + s)- tensor, e.g.,
\begin{equation}
A_{i_{1} \ldots i_{n}} {}^{j_1 \cdots j_{m}} \cdot C_{l_{1} \ldots l_{s}} {}^{k_1 \ldots k_{i}}=D_{i_{1} \ldots i_{n}} {}^{j_1 \cdots j_{m}} {}_{l_1 \ldots l_{s}}{}^{k_1 \ldots k_{l}}
\end{equation}
\textbf{The order of indices is important here, because $a_{i j}$ may be different from $a_{j i}$.}
\begin{example}
 Prove, by checking the transformation law, that the product of the contravariant vector $a^{i}$ and mixed tensor $b_{i}^{k}$ is a mixed $(2,1)$ -type tensor.
 
$$
a^{i^{\prime}}\left(x^{\prime}\right) b_{j^{\prime}}^{k^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{m}} a^{m}(x) \frac{\partial x^{k^{\prime}}}{\partial x^{l}} \frac{\partial x^{n}}{\partial x^{j^{\prime}}} b_{n}^{l}(x)
$$
\end{example}
\textbf{Contraction reduces the (n,m)-tensor to the (n-1,m-1)-tensor through the summation over two (always upper and lower, of course) indices.} For example,
\begin{equation}
A_{i j k}^{l n} \longrightarrow A_{i j k}^{l k}=\sum_{k=1}^{3} A_{i j k}^{l k}
\end{equation}

The internal product of the two tensors consists in their multiplication with the consequent contraction over some couple of indices. \textbf{Internal product of (m, n) and (r, s)-type tensors results in the (m+r-1, n+s-1)-type tensor.}
\begin{equation}
A_{i j k} \cdot B^{l j}=\sum_{j=1}^{3} A_{i j k} B^{l j}
\end{equation}
\begin{example}
Prove that the internal product $a_{i} \cdot b^{i}$ is a scalar if $a_{i}(x)$ and $b^{i}(x)$ are co- and contravariant vectors.

$$
b^{i^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{l}} b^{l}(x), \quad a_{i^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{k}}{\partial x^{i^{\prime}}} a_{k}(x)
$$
Then
$$
a_{i^{\prime}}\left(x^{\prime}\right) b^{i^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{l}} \frac{\partial x^{k}}{\partial x^{i^{\prime}}} b^{l}(x) a_{k}(x)=\delta_{l}^{k} b^{l}(x) a_{k}(x)=b^{k}(x) a_{k}(x)
$$
\end{example}
\begin{defi}
        Consider a basis $\left\{\mathbf{e}_{i}\right\} .$ The \textbf{scalar product} of the two basis vectors,
        \begin{equation}
g_{i j}=\left(\mathbf{e}_{i}, \mathbf{e}_{j}\right)
\end{equation}
is called \textbf{metric}. Here \textbf{the scalar product is not always inner product.}
\end{defi}
\begin{qt}
\textbf{Properties of metric}:

1. Symmetry of the metric $g_{i j}=g_{j i}$ follows from the symmetry of a scalar product $\left(\mathbf{e}_{i}, \mathbf{e}_{j}\right)=\left(\mathbf{e}_{j}, \mathbf{e}_{i}\right)$

2. For the orthonormal basis $\hat{\mathbf{n}}_{a}$, the metric is nothing but the Kronecker symbol $g_{a b}=\left(\hat{\mathbf{n}}_{a}, \hat{\mathbf{n}}_{b}\right)=\delta_{a b}$

3. Metric is a (0, 2) - tensor, as
$$
\begin{aligned}
g_{i^{\prime} j^{\prime}} &=\left(\mathbf{e}_{i}^{\prime}, \mathbf{e}_{j}^{\prime}\right)=\left(\frac{\partial x^{l}}{\partial x^{i^{\prime}}} \mathbf{e}_{l}, \frac{\partial x^{k}}{\partial x^{\prime j}} \mathbf{e}_{k}\right) \\
&=\frac{\partial x^{l}}{\partial x^{\prime i}} \frac{\partial x^{k}}{\partial x^{\prime j}}\left(\mathbf{e}_{l}, \mathbf{e}_{k}\right)=\frac{\partial x^{l}}{\partial x^{\prime i}} \frac{\partial x^{k}}{\partial x^{\prime j}} \cdot g_{k l}
\end{aligned}
$$

4. The distance between two points: $M_{1}\left(x^{i}\right)$ and $M_{2}\left(y^{i}\right)$ is defined by the inner product:
$$
S_{12}^{2}=g_{i j}\left(x^{i}-y^{i}\right)\left(x^{j}-y^{j}\right)
$$
\end{qt}
\bluep{Since $g_{i j}$ is $(0,2)$ - tensor and $\left(x^{i}-y^{i}\right)$ is $(1,0)$ -tensor (contravariant vector) $-S_{12}^{2}$ is a scalar. Therefore, $S_{12}^{2}$ is the same in any coordinate system.}

\textbf{The conjugated metric} is defined as
\begin{equation}
g^{i j}=\left(\mathbf{e}^{i}, \mathbf{e}^{j}\right) \quad \text { where } \quad\left(\mathbf{e}^{i}, \mathbf{e}_{k}\right)=\delta_{k}^{i}
\end{equation}
\begin{qt}
\begin{equation}
g^{i k} g_{k j}=\delta_{j}^{i}
\end{equation}
\end{qt}

We can\textbf{ Raise and lower indices of a tensor} by taking an appropriate internal product of a given tensor and the corresponding metric tensor.
\begin{equation}
\begin{aligned}
&\text { Lowering the index, } A_{i}(x)=g_{i j} A^{j}(x), \quad B_{i k}(x)=g_{i j} B_{ k}^{j}(x)\\
&\text { Raising the index, } \quad C^{l}(x)=g^{l j} C_{j}(x), \quad D^{i k}(x)=g^{i j} D_{j}^{k}(x)
\end{aligned}
\end{equation}
\begin{equation}
    \mathbf{e}_{i} g^{i j}=\mathbf{e}^{j}, \quad \mathbf{e}^{k} g_{k l}=\mathbf{e}_{l}
\end{equation}
\begin{qt}
Let the metric $g_{ij}$ correspond to the basis ek and to the coordinates $x^{k} .$ The determinants of the metric tensors and of the matrices of the transformations to Cartesian coordinates $X^{a}$ satisfy the following relations:
\begin{equation}
g=\operatorname{det}\left(g_{i j}\right)=\operatorname{det}\left(\frac{\partial X^{a}}{\partial x^{k}}\right)^{2}, \quad g^{-1}=\operatorname{det}\left(g^{k l}\right)=\operatorname{det}\left(\frac{\partial x^{l}}{\partial X^{b}}\right)^{2}
\end{equation}
\end{qt}
The possibility of lowering and raising the indices may help us in contracting two contravariant or two covariant indices of a tensor. 
\begin{example}
Suppose we need to contract the two first indices of the $(3,0)$ -tensor $B^{i j k} .$ After we lower the second index, we arrive at the tensor
$$
B^i{}_{l}{}^{k}=B^{i j k} g_{j l}
$$
And now we can contract the indices $i,j$. But, if we forget to indicate the order of indices, we obtain:$B_{l}^{i k}$, and it is not immediately clear which index was lowered and in which couple of indices one has to perform the contraction.
\end{example}

\section{Symmetric, Skew(Anti) Symmetric Tensors, and Determinants}
\subsection{Definitions and general considerations}
Tensor $A^{i j k \ldots}$ is called \textbf{symmetric} in the indices $i$ and $j,$ if
\begin{equation}
A^{i j k \ldots}=A^{j i k \ldots}
\end{equation}
Tensor $A^{i_{1} i_{2} \ldots i_{n}}$ is called \textbf{completely (or absolutely) symmetric} in the indices $\left(i_{1}, i_{2}, \ldots, i_{n}\right),$ if it is symmetric in any couple of these indices. 

Tensor $A^{i j}$ is called skew-symmetric or antisymmetric, if
\begin{equation}
A^{i j}=-A^{j i}
\end{equation}
\bluep{The advantage of tensors is that their (anti)symmetry holds under the transformation from one basis to another.}
\begin{example}
Consider the basis in 2D,
$$
\mathbf{e}_{1}=\hat{\mathbf{i}}+\hat{\mathbf{j}}, \quad \mathbf{e}_{2}=\hat{\mathbf{i}}-\hat{\mathbf{j}}
$$
(i) Derive all components of the absolutely antisymmetric tensor $\varepsilon^{i j}$ in the new basis, taking into account that $\varepsilon^{a b}=\epsilon^{a b},$ where $\epsilon^{12}=1$ in the orthonormal basis $\hat{\mathbf{n}}_{1}=\hat{\mathbf{i}}, \quad \hat{\mathbf{n}}_{2}=\hat{\mathbf{j}} .$ The calculation should be performed directly and also by using the formula for antisymmetric tensor. Explain the difference between the two results. 

(ii) Repeat the calculation for $\varepsilon_{i j} .$ Calculate the metric components and verify that $\varepsilon_{i j}=g_{i k} g_{j l} \varepsilon^{k l}$ and that $\varepsilon^{i j}=g^{i k} g^{j l} \varepsilon_{k l}$

\end{example}
\textbf{Solution:}
From the basis relation, we have:
$$
x^{\prime}=x+y
$$
$$
y^{\prime}=x-y
$$
Thus, 
$$
\frac{\partial x^{j^{\prime}}}{\partial x^i}=\left(\begin{array}{cc}
{1} & {1}\\
{1} & {-1}
\end{array}\right)
$$
and $\varepsilon^{12}=\frac{\partial x^{\prime 1}}{\partial x^k}\frac{\partial x^{\prime 2}}{\partial x^l}\epsilon^{kl}=\epsilon^{21}-\epsilon^{12}=-2$. Since \textbf{the metric tensor of the orthonormal basis is}
$$
g_{ab}=\left(\begin{array}{cc}
{1} & {0}\\
{0} & {1}
\end{array}\right)
$$
we have $\epsilon_{ab}=\epsilon^{ab}$. Thus, $\varepsilon_{12}=\frac{\partial x^k}{\partial x^{\prime 1}}\frac{\partial x^l}{\partial x^{\prime 2}}\epsilon_{kl}=-1/2$. Also, the metric tensor for our new basis (\textbf{w.r.t. the orthonormalized vector basis}) is
$$
g_{ij}=\frac{\partial x^{a}}{\partial x^i}\frac{\partial x^{b}}{\partial x^j}g_{ab}=\left(\begin{array}{cc}
{1/2} & {0}\\
{0} & {1/2}
\end{array}\right)
$$
Using $g^{i k} g_{k j}=\delta_{j}^{i}$, we also have $g^{ij}=\left(\begin{array}{cc}
{2} & {0}\\{0} & {2}\end{array}\right)$. Obviously, we have:
\begin{qt}
$$
\varepsilon_{i j}=g_{i k} g_{j l} \varepsilon^{k l}
$$
$$
\varepsilon^{i j}=g^{i k} g^{j l} \varepsilon_{k l}
$$
\end{qt}

\subsection{Completely Antisymmetric Tensors}
$(n, 0)$ -tensor $A^{i_{1} \ldots i_{n}}$ is called completely (or absolutely) antisymmetric, if it is antisymmetric in any couple of its indices
\[
\forall(k, l), \quad A^{i_{1} \ldots i_{l} \ldots i_{k} \ldots i_{n}}=-A^{i_{1} \ldots i_{k} \ldots i_{l} \ldots i_{n}}
\]
In the case of an absolutely antisymmetric tensor, the sign changes when we perform permutation of any two indices.
\begin{qt}
\begin{itemize}
    \item In a D-dimensional space, all completely antisymmetric tensors of rank $n>D$ are equal to zero.

    \item Theorem 2 An absolutely antisymmetric tensor $A^{i_{1} \ldots i_{D}}$ in a D-dimensional space has only one independent component.
    
    \item \bluep{Prove that for symmetric $a^{i j}$ and antisymmetric $b_{i j}$ tensors the scalar internal product is zero $a^{i j} \cdot b_{i j}=0$}
\end{itemize}
\end{qt}
For the last point, we can use the following argument to prove it:
$$
a^{j i} b_{j i}=\left(+a^{i j}\right)\left(-b_{i j}\right)\substack{i\rightarrow j,j\rightarrow i\\=}-a^{i j} b_{i j}
$$
The quantity that equals itself with an opposite sign is zero.
\begin{example}
(i) Prove that if $b^{i j}(x)$ is a tensor and $a_{l}(x)$ and $c_{k}(x)$ are covariant vector fields, then $f(x)=b^{i j}(x) \cdot a_{i}(x) \cdot c_{j}(x)$ is a scalar field.

(ii) In case $a_{i}(x) \equiv c_{i}(x),$ formulate the sufficient condition for $b^{i j}(x),$ such that the product $f(x) \equiv 0$
\end{example}
\textbf{Solution:}

(i) $b^{i j}(x) \cdot a_{i}(x) \cdot c_{j}(x)=d^{ij}{}_{ij}(x)$, which is a scalar.

(ii) when $b$ is anti-symmetric: $b^{i j}=b^{[i j]}$

\subsubsection{Determinants}
Consider first $2 D$ and special coordinates $X^{1}$ and $X^{2}$ corresponding to the orthonormal basis $\hat{\mathbf{n}}_{1}$ and $\hat{\mathbf{n}}_{2}$. We first define a special maximal absolutely antisymmetric object in 2D using the relations:
$$
E_{a b}=-E_{b a} \quad \text { and } \quad E_{12}=1
$$
\begin{qt}
For and matrix $\left\|C_{b}^{a}\right\|$ we can write its determinant as
\begin{equation}
\operatorname{det}\left\|C_{b}^{a}\right\|=E_{a b} C_{1}^{a} C_{2}^{b}
\end{equation}
and
\begin{equation}
\operatorname{det}\left(C_{b}^{a}\right)=\frac{1}{2} E_{a b} E^{d e} C_{d}^{a} C_{e}^{b}
\end{equation}
The difference between these two equations is that the latter admits two expressions $C_{1}^{a} C_{2}^{b}$ and $C_{2}^{a} C_{1}^{b}$. Thus, we have the $1/2$ factor in the second equation.
\end{qt}
Because $E_{ab}$ is maximal absolutely anti-symmetric, we also have:
\begin{equation}
E_{a b} C_{e}^{a} C_{d}^{b}=-E_{a b} C_{e}^{b} C_{d}^{a}
\end{equation}
\textit{Proof}
$$
E_{a b} C_{e}^{a} C_{d}^{b}=E_{b a} C_{e}^{b} C_{d}^{a}=-E_{a b} C_{d}^{a} C_{e}^{b}
$$
Here in the first equality, we have exchanged the names of the umbral(dummy) indices $a \leftrightarrow b$ and in the second one used antisymmetry $E_{a b}=-E_{b a}$. \bluep{Now we consider an arbitrary dimension of space D.}
\begin{qt}
Consider $a, b=1,2,3, \ldots, D . \quad \forall$ matrix $\left\|C_{b}^{a}\right\|,$ the determinant is
\begin{equation}
\operatorname{det}\left\|C_{b}^{a}\right\|=E_{a_{1} a_{2} \dots a_{D}} \cdot C_{1}^{a_{1}} C_{2}^{a_{2}} \cdots C_{D}^{a_{D}}
\end{equation}
where $E_{a_1a_2\dots a_D}$ is absolutely antisymmetric and $E_{123\dots D}=1$. \redp{The determinant in the equation above is a sum of D! terms, each of which is a product of elements from different lines and columns, taken with the positive sign for even and with the negative sign for odd parity of permutations.}
\end{qt}
In general, the repression
\begin{equation}
\mathcal{A}_{a_{1} \ldots a_{D}}=E_{b_{1} \ldots b_{D}} A_{a_{1}}^{b_{1}} A_{a_{2}}^{b_{2}} \ldots A_{a_{D}}^{b_{D}}
\end{equation}
\bluep{is absolutely antisymmetric in the indices $\left\{a_{1}, \ldots, a_{D}\right\}$ and therefore is proportional to $E_{a_{1} \ldots a_{D}}$}.
\begin{qt}
In an arbitrary dimension D
\begin{equation}
\operatorname{det}\left\|C_{b}^{a}\right\|=\frac{1}{D !} E_{a_{1} a_{2} \ldots a_{D}} E^{b_{1} b_{2} \ldots b_{D}} \cdot C_{b_{1}}^{a_{1}} C_{b_{2}}^{a_{2}} \ldots C_{b_{D}}^{a_{D}}
\end{equation}
and
\begin{equation}
E^{a_{1} a_{2} \ldots a_{D}} \cdot E_{b_{1} b_{2} \ldots b_{D}}=\left|\begin{array}{cccc}
{\delta_{b_{1}}^{a_{1}}} & {\delta_{b_{2}}^{a_{1}}} & {\ldots}&{ \delta_{b_{D}}^{a_{1}}} \\
{\delta_{b_{1}}^{a_{2}}} & {\delta_{b_{2}}^{a_{2}}} & {\ldots} & {\delta_{b_{D}}^{a_{2}}} \\
{\ldots} & {\cdots} & {\cdots} & {\ldots} \\
{\delta_{b_{1}}^{a_{D}}} & {\ldots} & {\cdots} & {\delta_{b_{D}}^{a_{D}}}
\end{array}\right|
\end{equation}
\end{qt}
Following the theorem above, we have for the special dimension 3D:
$$
E^{a b c} E_{d e f}=\left|\begin{array}{ccc}
{\delta_{d}^{a}} & {\delta_{e}^{a}} & {\delta_{f}^{a}} \\
{\delta_{d}^{b}} & {\delta_{e}^{b}} & {\delta_{f}^{b}} \\
{\delta_{d}^{c}} & {\delta_{e}^{c}} & {\delta_{f}^{c}}
\end{array}\right|
$$
Now, let's contract the indices $c,f$:
\begin{qt}
\begin{equation}
E^{a b c} E_{d e c}=\left|\begin{array}{ccc}
{\delta_{d}^{a}} & {\delta_{e}^{a}} & {\delta_{c}^{a}} \\
{\delta_{d}^{b}} & {\delta_{e}^{b}} & {\delta_{c}^{b}} \\
{\delta_{d}^{c}} & {\delta_{e}^{c}} & {3}
\end{array}\right|=\delta_{d}^{a} \delta_{e}^{b}-\delta_{e}^{a} \delta_{d}^{b}
\end{equation}
\end{qt}
The last formula is sometimes called magic, as it is an extremely useful tool for the calculations in analytic geometry and vector calculus. Let us proceed and contract indices b and e. We obtain
$$
E^{a b c} E_{d b c}=3 \delta_{d}^{a}-\delta_{d}^{a}=2 \delta_{d}^{a}
$$
Finally, contracting the last remaining couple of indices, we arrive at
$$
E^{a b c} E_{a b c}=6
$$
In general
\begin{qt}
\begin{equation}
E^{a_{1} a_{2} \ldots a_{D}} E_{a_{1} a_{2} \ldots a_{D}}=D !
\end{equation}
Because for the first index, we have $D$ choices, for the second $D-1$ choices, for the third $D-2$ choices, etc. 
\end{qt}
\begin{example}
Using definition of the maximal antisymmetric symbol $E^{a_{1} a_{2} \ldots a_{D}}$ prove the rule for the product of matrix determinants:
$$
\operatorname{det}(A \cdot B)=\operatorname{det} A \cdot \operatorname{det} B
$$
Here both $A=\left\|a_{k}^{i}\right\|$ and $B=\left\|b_{k}^{i}\right\|$ are $n \times n$ matrices.
\end{example}
\textbf{Solution:}
$$
\operatorname{det}(A \cdot B)=E_{i_{1} i_{2} \ldots i_{D}}(A \cdot B)_{1}^{i_{1}}(A \cdot B)_{2}^{i_{2}} \ldots(A \cdot B)_{D}^{i_{D}}
$$
expand, we have
$$
\operatorname{det}(A \cdot B)=E_{i_{1} i_{2} \ldots i_{D}} a_{k_{1}}^{i_{1}} a_{k_{2}}^{i_{2}} \ldots a_{k_{D}}^{i_{D}} b_{1}^{k_{1}} b_{2}^{k_{2}} \ldots b_{D}^{k_{D}}
$$
According to (5.11.8), we have
$$
E_{i_{1} i_{2} \ldots i_{D}} a_{k_{1}}^{i_{1}} a_{k_{2}}^{i_{2}} \ldots a_{k_{D}}^{i_{D}}=E_{k_{1} k_{2} \ldots k_{D}} \cdot \operatorname{det} A
$$
Therefore,
$$
\operatorname{det}(A \cdot B)=\operatorname{det} A E_{k_{1} k_{2} \ldots k_{D}} b_{1}^{k_{1}} b_{2}^{k_{2}} \ldots b_{D}^{k_{D}}
$$
Finally, let's consider two more statements
\begin{lemma}
For any nondegenerate $D \times D$ matrix $M_{b}^{a}$ with the determinant $M,$ the elements of the inverse matrix $\left(M^{-1}\right)_{c}^{b}$ are given by the expressions
\begin{equation}
\left(M^{-1}\right)_{a}^{b}=\frac{1}{M(D-1) !} E_{a a_{2} \ldots a_{D}} E^{b b_{2} \ldots b_{D}} M_{b_{2}}^{a_{2}} M_{b_{3}}^{a_{3}} \ldots M_{b_{D}}^{a_{D}}
\end{equation}
\end{lemma}
Consider the nondegenerate $D \times D$ matrix $\left\|M_{b}^{a}\right\|$ with the elements $M_{b}^{a}=M_{b}^{a}(\kappa)$ being functions of some parameter $\kappa .$In general, the determinant of this matrix $M=\operatorname{det}\left\|M_{b}^{a}\right\|$ also depends on $\kappa .$ Suppose all functions $M_{b}^{a}(\kappa)$ are differentiable. Then the derivative of the determinant equals
\begin{qt}
\begin{equation}
\dot{M}=\frac{d M}{d \kappa}=M\left(M^{-1}\right)_{a}^{b} \frac{d M_{b}^{a}}{d \kappa}
\end{equation}
\end{qt}
where $\left(M^{-1}\right)_{a}^{b}$ are the elements of the matrix imverse to $\left(M_{b}^{a}\right)$ and the dot over $a$ function indicates its derivative with respect to $\kappa$.

\subsection{Applications to Vector Algebra}
Let's come back to the following definition for a \textbf{special object}:
$$
E^{a b c} E_{d e c}=\left|\begin{array}{ccc}
{\delta_{d}^{a}} & {\delta_{e}^{a}} & {\delta_{c}^{a}} \\
{\delta_{d}^{b}} & {\delta_{e}^{b}} & {\delta_{c}^{b}} \\
{\delta_{d}^{c}} & {\delta_{e}^{c}} & {3}
\end{array}\right|=\delta_{d}^{a} \delta_{e}^{b}-\delta_{e}^{a} \delta_{d}^{b}
$$
We first working on a special orthonormal basis $\hat{\mathbf{n}}_{a}=(\hat{\mathbf{i}}, \hat{\mathbf{j}}, \hat{\mathbf{k}})$, corresponding to the Cartesian coordinates $X^a$. In a vector product notations, we have
$$
\begin{aligned}
&\left[\hat{\mathbf{n}}_{1}, \hat{\mathbf{n}}_{2}\right]=\hat{\mathbf{n}}_{1} \times \hat{\mathbf{n}}_{2}=\hat{\mathbf{n}}_{3}\\
&\left[\hat{\mathbf{n}}_{2}, \hat{\mathbf{n}}_{3}\right]=\hat{\mathbf{n}}_{2} \times \hat{\mathbf{n}}_{3}=\hat{\mathbf{n}}_{1}\\
&\left[\hat{\mathbf{n}}_{3}, \hat{\mathbf{n}}_{1}\right]=\hat{\mathbf{n}}_{3} \times \hat{\mathbf{n}}_{1}=\hat{\mathbf{n}}_{2}
\end{aligned}
$$
Using the previously defined $E^{abc}$, we can write this in a more compact way:
\begin{equation}
\left[\hat{\mathbf{n}}_{a}, \hat{\mathbf{n}}_{b}\right]=E_{a b c} \cdot \hat{\mathbf{n}}^{c}
\end{equation}
For orthonormal basis, $\hat{\mathbf{n}}_{a}=\hat{\mathbf{n}}^{a}$ and $E^{a b c}=E_{a b c}$. In general, if we consider two vectors $\mathbf{V}=V^{a} \hat{\mathbf{n}}_{a}$ and $\mathbf{W}=W^{a} \hat{\mathbf{n}}_{a}$, then
\redp{$$
[\mathbf{V}, \mathbf{W}]=E_{a b c} \cdot V^{a} \cdot W^{b} \cdot \hat{\mathbf{n}}^{c}
$$}
Using this object, we can solve many problems of vector algebra:
\begin{example}
Consider the mixed product of the three vectors
$$
(\mathbf{U}, \mathbf{V}, \mathbf{W})=(\mathbf{U},[\mathbf{V}, \mathbf{W}])=U^{a} \cdot[\mathbf{V}, \mathbf{W}]_{a}
$$
$$
=U^{a} \cdot E_{a b c} V^{b} W^{c}=E_{a b c} U^{a} V^{b} W^{c}=\left|\begin{array}{ccc}
{U^{1}} & {U^{2}} & {U^{3}} \\
{V^{1}} & {V^{2}} & {V^{3}} \\
{W^{1}} & {W^{2}} & {W^{3}}
\end{array}\right|
$$
\end{example}
The similar arguments can be made to prove the following properties of the \textbf{mixed product}:
\begin{qt}
(i) Cyclic identity: $(\mathbf{U}, \mathbf{V}, \mathbf{W})=(\mathbf{W}, \mathbf{U}, \mathbf{V})=(\mathbf{V}, \mathbf{W}, \mathbf{U})$

(ii) Antisymmetry: $(\mathbf{U}, \mathbf{V}, \mathbf{W})=-(\mathbf{V}, \mathbf{U}, \mathbf{W})=-(\mathbf{U}, \mathbf{W}, \mathbf{V})$
\end{qt}
\begin{example}
Prove $[\mathbf{U}, \mathbf{V}] \times[\mathbf{W}, \mathbf{Y}]=\mathbf{V} \cdot(\mathbf{U}, \mathbf{W}, \mathbf{Y})-\mathbf{U} \cdot(\mathbf{V}, \mathbf{W}, \mathbf{Y})$
$$
([\mathbf{U}, \mathbf{V}] \times[\mathbf{W}, \mathbf{Y}])_{a}=E_{a b c}[\mathbf{U}, \mathbf{V}]^{b}[\mathbf{W}, \mathbf{Y}]^{c}=E_{a b c} E^{b d e} U_{d} V_{e} E^{c f g} W_{f} Y_{g}
$$
$$
=-E_{b a c} E^{b d e} E^{c f g} U_{d} V_{e} W_{f} Y_{g}=-\left(\delta_{a}^{d} \delta_{c}^{e}-\delta_{a}^{e} \delta_{c}^{d}\right) E^{c f g} U_{d} V_{e} W_{f} Y_{g}
$$
$$
=E^{c f g}\left(U_{c} V_{a} W_{f} Y_{g}-U_{a} V_{c} W_{f} Y_{g}\right)=V_{a}(\mathbf{U}, \mathbf{W}, \mathbf{Y})-U_{a}(\mathbf{V}, \mathbf{W}, \mathbf{Y})
$$
\end{example}
Some more identities
\begin{qt}
$$
[[\mathbf{U}, \mathbf{V}],[\mathbf{W}, \mathbf{Y}]]=\mathbf{W} \cdot(\mathbf{Y}, \mathbf{U}, \mathbf{V})-\mathbf{Y} \cdot(\mathbf{W}, \mathbf{U}, \mathbf{V})
$$
$$
\mathbf{V}(\mathbf{W}, \mathbf{Y}, \mathbf{U})-\mathbf{U}(\mathbf{W}, \mathbf{Y}, \mathbf{V})=\mathbf{W}(\mathbf{U}, \mathbf{V}, \mathbf{Y})-\mathbf{Y}(\mathbf{U}, \mathbf{V}, \mathbf{W})
$$
$$
[\mathbf{U} \times \mathbf{V}] \cdot[\mathbf{W} \times \mathbf{Y}]=(\mathbf{U} \cdot \mathbf{W})(\mathbf{V} \cdot \mathbf{Y})-(\mathbf{U} \cdot \mathbf{Y})(\mathbf{V} \cdot \mathbf{W})
$$
$$
[\mathbf{A},[\mathbf{B}, \mathbf{C}]]=\mathbf{B}(\mathbf{A}, \mathbf{C})-\mathbf{C}(\mathbf{A}, \mathbf{B})
$$
$$
[\mathbf{U},[\mathbf{V}, \mathbf{W}]]+[\mathbf{W},[\mathbf{U}, \mathbf{V}]]+[\mathbf{V},[\mathbf{W}, \mathbf{U}]]=0
$$
\end{qt}
Now, let's construct a tensor version of the object $E_{abc}$. To do so, one has to use the \textbf{transformation rule for the covariant tensor of the third rank,starting from the special Cartesian coordinates $X^a$}:
\begin{equation}
\varepsilon_{i j k}=\frac{\partial X^{a}}{\partial x^{i}} \frac{\partial X^{b}}{\partial x^{j}} \frac{\partial X^{c}}{\partial x^{k}} E_{a b c}
\end{equation}
\begin{qt}
The component $\varepsilon_{123}$ is a square root of the metric determinant,
\begin{equation}
    \varepsilon_{123}=g^{1 / 2}, \quad \text { where } g=\operatorname{det}\left\|g_{i j}\right\|
    \label{eps123-!}
\end{equation}
or
\begin{equation}
    \varepsilon_{123}=\operatorname{det}\left\|\frac{\partial X^{a}}{\partial x^{i}}\right\|
    \label{eps123}
\end{equation}
while we can use (\ref{eps123}) without much thought, we need to be careful using (\ref{eps123-!}). The reason is that \redp{the coordinate transformations that break parity can change the sign in the r.h.s. in Eq.(\ref{eps123-!})}
\end{qt}
Despite $E_{i j k}$ is not being a tensor, we can easily control its transformation from one coordinate system to another:
$$
E_{i j k}=\frac{1}{\sqrt{g}} \varepsilon_{i j k}
$$
$E_{i j k}$ is a particular example of objects which are called \textbf{tensor densities}.
\begin{defi}
        The quantity $A_{i_{1} \ldots i_{n}}{}^{j_{1} \ldots j_{n}}$ is a tensor density of the $(m, n)$ -type with the weight $r,$ if the quantity
        $$
g^{-r / 2} \cdot A_{i_{1} \ldots i_{n}}{}^{j_{1} \ldots j_{m}}
$$
is a tensor.
\end{defi}
In general,
\begin{equation}
\varepsilon_{i_{1} i_{2} \cdots i_{D}}=\frac{\partial x^{a_{1}}}{\partial x^{i_{1}}} \frac{\partial x^{a_{2}}}{\partial x^{i_{2}}} \cdots \frac{\partial x^{a_{D}}}{\partial x^{i_{D}}} E_{a_{1} a_{2} \cdots a_{D}}
\end{equation}
Since
\begin{equation}
g_{i j}=\frac{\partial x^{a}}{\partial x^{i}} \frac{\partial x^{b}}{\partial x^{j}} \delta_{b}^{a}
\end{equation}
assuming that the orientation of axes is such that $\operatorname{det}\left(\frac{\partial x^{a}}{\partial x^{i}}\right)>0,$ we get
\begin{equation}
\operatorname{det}\left\|\frac{\partial x^{a}}{\partial x^{i}}\right\|=\sqrt{g}
\end{equation}
Then
\begin{qt}
\begin{equation}
\varepsilon_{123 \dots D}=\frac{\partial x^{a_{1}}}{\partial x^{1}} \frac{\partial x^{a_{2}}}{\partial x^{2}} \cdots \frac{\partial x^{a_{D}}}{\partial x^{D}} E_{a_{1} a_{2} \cdots a_{D}}=\operatorname{det}\left(\frac{\partial x^{a}}{\partial x^{i}}\right)=\sqrt{g}
\end{equation}
Similarly, $\varepsilon^{12 \cdots D}=\frac{1}{\sqrt{g}}$.
\end{qt}

\section{Curvilinear Coordinates, Local Coordinate Transformations}
So far, we have learned the following transformation rule for scalar field, vector, and tensor:
$$
\varphi^{\prime}\left(x^{\prime}\right)=\varphi(x)
$$
$$
a^{\prime i}\left(x^{\prime}\right)=\frac{\partial x^{i}}{\partial x^{j}} a^{j}(x)
$$
$$
b_{l}^{\prime}\left(x^{\prime}\right)=\frac{\partial x^{k}}{\partial x^{\prime l}} b_{k}(x)
$$
$$
A_{j^{\prime}}^{i^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{k}} \frac{\partial x^{l}}{\partial x^{\prime j}} A_{l}^{k}(x)
$$
and for metric tensor:
\begin{equation}
    g_{i j}(x)=\frac{\partial X^{a}}{\partial x^{i}} \frac{\partial X^{b}}{\partial x^{j}} g_{a b}
    \label{metric-orthonormal}
\end{equation}
\begin{equation}
    g^{i j}=\frac{\partial x^{i}}{\partial X^{a}} \frac{\partial x^{j}}{\partial X^{b}} \delta^{a b}
\end{equation}


where $g_{a b}=\delta_{a b}$ is a metric in Cartesian coordinates.

It is so easy to generalize the notion of tensor and algebraic operations over tensors, \textbf{because all these operations are defined in the same point of the space.} Thus, the main difference between general coordinate transformation $x^{\prime \alpha}=x^{\prime \alpha}(x)$ and the special one $x^{\prime \alpha}=\wedge_{\beta}^{\alpha^{\prime}} x^{\beta}+B^{\alpha^{\prime}}$ with $\wedge_{\beta}^{\alpha^{\prime}}=$ const and $B^{\alpha^{\prime}}=$ const is that \textbf{in the general case, the transition coefficients $\partial x^{i} / \partial x^{\prime j}$ are not necessary constants.}

One of important consequences is that \bluep{the metric tensor $g_{i j}$ also depends on the point. Additionally, the antisymmetric tensor $\varepsilon^{i j k}$ also depends on the coordinates}.

\subsection{Polar coordinates on the plane}
Our purpose in this section is to learn how to transform an arbitrary tensor to polar coordinates. From $(x,y)$ to $(r,\varphi)$, we have
$$
\begin{array}{l}
{\mathbf{e}_{r}=\frac{\partial x}{\partial r} \hat{\mathbf{n}}_{x}+\frac{\partial y}{\partial r} \hat{\mathbf{n}}_{y}=\hat{\mathbf{i}} \cos \varphi+\hat{\mathbf{j}} \sin \varphi} \\
{\mathbf{e}_{\varphi}=\frac{\partial x}{\partial \varphi} \hat{\mathbf{n}}_{x}+\frac{\partial y}{\partial \varphi} \hat{\mathbf{n}}_{y}=r(-\hat{\mathbf{i}} \sin \varphi+\hat{\mathbf{j}} \cos \varphi)}
\end{array}
$$
\textbf{Because these basis vectors are orthogonal,} the metric in polar coordinates is diagonal
\begin{equation}
g_{i j}=\left(\begin{array}{ll}
{g_{r r}} & {g_{r \varphi}} \\
{g_{\varphi r}} & {g_{\varphi \varphi}}
\end{array}\right)=\left(\begin{array}{cc}
{\mathbf{e}_{r} \cdot \mathbf{e}_{r}} & {\mathbf{e}_{r} \cdot \mathbf{e}_{\varphi}} \\
{\mathbf{e}_{\varphi} \cdot \mathbf{e}_{r}} & {\mathbf{e}_{\varphi} \cdot \mathbf{e}_{\varphi}}
\end{array}\right)=\left(\begin{array}{cc}
{1} & {0} \\
{0} & {r^{2}}
\end{array}\right)
\end{equation}
The first observation is that the basic vectors $\mathbf{e}_{r}$ and $\mathbf{e}_{\varphi}$ are orthogonal $\mathbf{e}_{r} \cdot \mathbf{e}_{\varphi}=0 .$ As a result, the metric in polar coordinates is diagonal. Again, \redp{this metric tensor is w.r.t. the coordinate basis, not w.r.t. orthonormalized vector basis}. To get metrix tensor w.r.t. orthonormalized vector, we can simply use Eq.(\ref{metric-orthonormal}).

In what follows we shall mark the components of the vector in this basis by tilde. The simpler notations without tilde are always reserved for the components of the vector in the normalized basis. Generally, \bluep{using the normalized basis means that all components of the vector have the same dimension.}

\begin{example}
For the polar coordinates on the 2D plane, find the metric by performing transformation of the metric in Cartesian coordinates
\end{example}
\textbf{Solution}:
$$
g_{a b}=\delta_{a b}, \quad \text { that is } \quad g_{x x}=1=g_{y y}, \quad g_{x y}=g_{y x}=0
$$
$$
\begin{aligned}
g_{\varphi \varphi} &=\frac{\partial x}{\partial \varphi} \frac{\partial x}{\partial \varphi} g_{x x}+\frac{\partial x}{\partial \varphi} \frac{\partial y}{\partial \varphi} g_{x y}+\frac{\partial y}{\partial \varphi} \frac{\partial x}{\partial \varphi} g_{y x}+\frac{\partial y}{\partial \varphi} \frac{\partial y}{\partial \varphi} g_{y y}=\\
&=\left(\frac{\partial x}{\partial \varphi}\right)^{2}+\left(\frac{\partial y}{\partial \varphi}\right)^{2}=r^{2} \sin ^{2} \varphi+r^{2} \cos ^{2} \varphi=r^{2}
\end{aligned}
$$
and
$$
g_{r r}=\frac{\partial x}{\partial r} \frac{\partial x}{\partial r} g_{x x}+\frac{\partial y}{\partial r} \frac{\partial y}{\partial r} g_{y y}=\left(\frac{\partial x}{\partial r}\right)^{2}+\left(\frac{\partial y}{\partial r}\right)^{2}=\cos ^{2} \varphi+\sin ^{2} \varphi=1
$$
Thus, the metric is
$$
\left(\begin{array}{l}
{g_{r r} g_{r \varphi}} \\
{g_{\varphi r} g_{\varphi \varphi}}
\end{array}\right)=\left(\begin{array}{ll}
{1} & {0} \\
{0} & {r^{2}}
\end{array}\right)
$$
\bluep{This derivation of the metric has a simple geometric interpretation. Consider two points that have infinitesimally close $\varphi \text { and } r.$ The distance between a these two points $d s$ is defined by the relation}
$$
\begin{aligned}
d s^{2} &=d x^{2}+d y^{2}=g_{a b} d X^{a} d X^{b}=g_{i j} d x^{i} d x^{j}=d r^{2}+r^{2} d \varphi^{2} \\
&=g_{r r} d r d r+g_{\varphi \varphi} d \varphi d \varphi+2 g_{\varphi r} d \varphi d r
\end{aligned}
$$
\begin{qt}
\redp{The tensor form of the transformation of the metric corresponds to the
coordinate-independent distance between two infinitesimally close points}
\end{qt}

\subsection{Cylindrical and Spherical Coordinates}
 Cylindrical coordinates in 3D are defined by the relations:
 $$
\begin{aligned}
x=r \cos \varphi, & y=r \sin \varphi, \quad z=z \\
\text { where } \quad 0 \leq r<\infty, & 0 \leq \varphi<2 \pi \quad \text { and } \quad-\infty<z<\infty
\end{aligned}
$$
The basic vectors are
$$
\mathbf{e}_{r}=\hat{\mathbf{i}} \cos \varphi+\hat{\mathbf{j}} \sin \varphi, \quad \mathbf{e}_{\varphi}=-\hat{\mathbf{i}} r \sin \varphi+\hat{\mathbf{j}} r \cos \varphi, \quad \mathbf{e}_{z}=\hat{\mathbf{k}}
$$
and the metric tensor is
\begin{equation}
g_{i j}=\left(\begin{array}{l}
{g_{r r} g_{r \varphi} g_{r z}} \\
{g_{\varphi r} g_{\varphi \varphi} g_{\varphi z}} \\
{g_{z r} g_{z \varphi} g_{z z}}
\end{array}\right)=\left(\begin{array}{ccc}
{1} & {0} & {0} \\
{0} & {r^{2}} & {0} \\
{0} & {0} & {1}
\end{array}\right)
\end{equation}

For spherical coordinates, we have
the following relation with Cartesian coordiantes
$$
\begin{aligned}
x=r \cos \varphi \sin \chi, & y=r \sin \varphi \sin \chi, \quad z=r \cos \chi \\
\text { where } \quad 0 \leq r<\infty, & 0 \leq \varphi<2 \pi \quad \text { and } \quad 0 \leq \chi \leq \pi
\end{aligned}
$$
The basic vector is then
$$
\begin{aligned}
&\mathbf{e}_{r}=\hat{\mathbf{i}} \cos \varphi \sin \chi+\hat{\mathbf{j}} \sin \varphi \sin \chi+\hat{\mathbf{k}} \cos \chi\\
&\mathbf{e}_{x}=\hat{\mathbf{i}} r \cos \varphi \cos \chi+\hat{\mathbf{j}} r \sin \varphi \cos \chi-\hat{\mathbf{k}} r \sin \chi\\
&\mathbf{e}_{\varphi}=-\hat{\mathbf{i}} r \sin \varphi \sin \chi+\hat{\mathbf{j}} r \cos \varphi \sin \chi
\end{aligned}
$$
with the metric tensor as
$$
g_{i j}=\left(\begin{array}{lll}
{g_{r r}} & {g_{r x}} & {g_{r \varphi}} \\
{g_{\chi r}} & {g_{\chi x}} & {g_{x \varphi}} \\
{g_{\varphi r}} & {g_{\varphi x}} & {g_{\varphi \varphi}}
\end{array}\right)=\left(\begin{array}{ccc}
{1} & {0} & {0} \\
{0} & {r^{2}} & {0} \\
{0} & {0} & {r^{2} \sin ^{2} x}
\end{array}\right)
$$
And the normalized basis is
\begin{equation}
    \begin{aligned}
&\hat{\mathbf{n}}_{r}=\hat{\mathbf{i}} \cos \varphi \sin \chi+\hat{\mathbf{j}} \sin \varphi \sin \chi+\hat{\mathbf{k}} \cos \chi\\
&\hat{\mathbf{n}}_{\varphi}=-\hat{\mathbf{i}} \sin \varphi+\hat{\mathbf{j}} \cos \varphi\\
&\hat{\mathbf{n}}_{\chi}=\hat{\mathbf{i}} \cos \varphi \cos \chi+\hat{\mathbf{j}} \sin \varphi \cos \chi-\hat{\mathbf{k}} \sin \chi
\end{aligned}
\label{unitvec-spherical}
\end{equation}
Now let us derive the expression for the velocity and acceleration in 3D for the case of the spherical coordinates. The starting point is Eq.(\ref{unitvec-spherical}). A simple calculus gives the following result for the first derivatives:
$$
\begin{aligned}
\dot{\mathbf{n}}_{r} &=\dot{\varphi} \sin \chi(-\hat{\mathbf{i}} \sin \varphi+\hat{\mathbf{j}} \cos \varphi) \\
&+\dot{\chi}(\hat{\mathbf{i}} \cos \varphi \cos \chi+\hat{\mathbf{j}} \sin \varphi \cos \chi-\hat{\mathbf{k}} \sin \chi) \\
\dot{\mathbf{n}}_{\varphi} &=-\dot{\varphi}(\hat{\mathbf{i}} \cos \varphi+\hat{\mathbf{j}} \sin \varphi) \\
\dot{\mathbf{n}}_{\chi} &=\dot{\varphi} \cos \chi(-\hat{\mathbf{i}} \sin \varphi+\hat{\mathbf{j}} \cos \varphi) \\
&-\dot{\chi}(\hat{\mathbf{i}} \sin \chi \cos \varphi+\hat{\mathbf{j}} \sin \chi \sin \varphi+\hat{\mathbf{k}} \cos \chi)
\end{aligned}
$$
Next, we need to perform an inverse transformation from the Cartesian basis to the new basis:
$$
\begin{aligned}
&\hat{\mathbf{i}}=\frac{\partial r}{\partial x} \mathbf{e}_{r}+\frac{\partial \varphi}{\partial x} \mathbf{e}_{\varphi}+\frac{\partial \chi}{\partial x} \mathbf{e}_{x}\\
&\hat{\mathbf{j}}=\frac{\partial r}{\partial y} \mathbf{e}_{r}+\frac{\partial \varphi}{\partial y} \mathbf{e}_{\varphi}+\frac{\partial \chi}{\partial y} \mathbf{e}_{\chi}\\
&\mathbf{k}=\frac{\partial r}{\partial z} \mathbf{e}_{r}+\frac{\partial \varphi}{\partial z} \mathbf{e}_{\varphi}+\frac{\partial \chi}{\partial z} \mathbf{e}_{\chi}
\end{aligned}
$$
Using these relations, one can derive the first derivatives of the vector $\hat{\mathbf{n}}_{r}, \hat{\mathbf{n}}_{\varphi}, \hat{\mathbf{n}}_{\chi}$ in the final form:
$$
\begin{aligned}
&\dot{\mathbf{n}}_{r}=\dot{\varphi} \sin \chi \hat{\mathbf{n}}_{\varphi}+\dot{\chi} \hat{\mathbf{n}}_{\chi}\\
&\dot{\mathbf{n}}_{\varphi}=-\dot{\varphi}\left(\sin \chi \hat{\mathbf{n}}_{r}+\cos \chi \hat{\mathbf{n}}_{\chi}\right)\\
&\dot{\mathbf{n}}_{\chi}=\dot{\varphi} \cos \chi \hat{\mathbf{n}}_{\varphi}-\dot{\chi} \hat{\mathbf{n}}_{r}
\end{aligned}
$$
Now the particle's velocity and acceleration are
\begin{equation}
\mathbf{v}=\dot{\mathbf{r}}=\frac{d}{d t}\left(r \hat{\mathbf{n}}_{r}\right)=\dot{r} \hat{\mathbf{n}}_{r}+r \dot{\chi} \hat{\mathbf{n}}_{\chi}+r \dot{\varphi} \sin \chi \hat{\mathbf{n}}_{\varphi}
\end{equation}
\begin{equation}
\begin{aligned}
\mathbf{a} &=\ddot{\mathbf{r}}=\left(\ddot{r}-r \dot{\chi}^{2}-r \dot{\varphi}^{2} \sin ^{2} \chi\right) \hat{\mathbf{n}}_{r}+(2 r \dot{\varphi} \dot{\chi} \cos \chi+2 \dot{\varphi} \dot{r} \sin \chi+r \ddot{\varphi} \sin \chi) \hat{\mathbf{n}}_{\varphi} \\
&+\left(2 \dot{r} \dot{\chi}+r \ddot{\chi}-r \dot{\varphi}^{2} \sin \chi \cos \chi\right) \hat{\mathbf{n}}_{\chi}
\end{aligned}
\end{equation}

One can also find the components of Levi-Civita tensor $\epsilon_{123}$ and $\epsilon^{123}$ for cylindric and spherical coordinates using the following relations:
\begin{equation}
g=\operatorname{det}\left\|g_{i j}\right\| \quad \text { and } \quad \operatorname{det}\left\|g^{i j}\right\|=\frac{1}{g}
\end{equation}
$$
\varepsilon^{123}=\frac{1}{\sqrt{g}}
$$
$$
\varepsilon_{123}=g^{1 / 2}
$$
For the \bluep{hyperbolic coordinates,} we have
$$
x=r \cdot \cosh \varphi, \quad y=r \cdot \sinh \varphi
$$
Since
$$
\cosh ^{2} \varphi-\sinh ^{2} \varphi=1 ; \quad \cosh ^{\prime} \varphi=\sinh \varphi ; \quad \sinh ^{\prime} \varphi=\cosh \varphi
$$
we find
$$
g_{r r}=\cosh 2 \varphi, \quad g_{\varphi \varphi}=r^{2} \cosh 2 \varphi, \quad g_{r \varphi}=r \sinh 2 \varphi
$$
\textbf{in this case the basis is not orthogonal.}
\section{Deivatives of Tensors,Covariant Derivatives}
Let us start from a scalar field $\varphi$. Consider its partial derivative
\begin{equation}
\partial_{i} \varphi=\varphi_{, i}=\frac{\partial \varphi}{\partial x^{i}}
\end{equation}
\redp{In the transformed coordinates $x^{i}=x^{i i}\left(x^{j}\right)$ we obtain, using the chain rule and $\varphi^{\prime}\left(x^{\prime}\right)=\varphi(x)$}
$$
\partial_{i^{\prime}} \varphi^{\prime}=\frac{\partial \varphi^{\prime}}{\partial x^{i^{\prime}}}=\frac{\partial x^{j}}{\partial x^{\prime i}} \frac{\partial \varphi}{\partial x^{j}}=\frac{\partial x^{j}}{\partial x^{i}} \partial_{j} \varphi
$$
\textbf{The last formula shows that the partial derivative of a scalar field $\varphi_{, i}=\partial_{i} \varphi$ transforms as a covariant vector field. Certainly, there is no need to modify a partial derivative in this case.}

Now we introduce \redp{vector differential operator $\nabla$}. 
\begin{qt}
When acting on a scalar, $\nabla$ produces a covariant vector which is called gradient.
\end{qt}

\textbf{The next step is to consider a partial derivative of a covariant vector $b_i(x)$}.Let us make a corresponding transformation
\begin{qt}
\begin{equation}
\partial_{j^{\prime}} b_{i^{\prime}}=\frac{\partial}{\partial x^{\prime j}} b_{i^{\prime}}=\frac{\partial}{\partial x^{\prime j}}\left(\frac{\partial x^{k}}{\partial x^{\prime i}} b_{k}\right)=\frac{\partial x^{k}}{\partial x^{i^{\prime}}} \frac{\partial x^{l}}{\partial x^{\prime j}} \partial_{l} b_{k}+\frac{\partial^{2} x^{k}}{\partial x^{\prime j} \partial x^{\prime i}} b_{k}
\label{vector-grad}
\end{equation}
\end{qt}
The parenthesis contains a product of the two expressions that are functions of different coordinates $x^{\prime j}$ and $x^j$. \bluep{When the partial derivative $\partial / \partial x^{\prime j}$  acts on the function of the coordinates  $x^{i}$, it must be applied following the chain rule}
\begin{equation}
\frac{\partial}{\partial x^{\prime j}}=\frac{\partial x^{l}}{\partial x^{\prime j}} \frac{\partial}{\partial x^{l}}
\label{chain-rule}
\end{equation}
The last term in Eq.(\ref{vector-grad}) can be zero when $\partial x^{k} / \partial x^{\prime i}$ is constant. Then its derivatives are zeros and the gradient of a covariant vector is a tensor. But, \textbf{for a general case of curvilinear coordinates (e.g., polar in 2D or spherical
in 3D), the formula (\ref{vector-grad}) shows the non-tensor nature of the transformation.}

For contravariant vector $a^i(x)$ we have similar relations to (\ref{vector-grad}):
\begin{qt}
\begin{equation}
\partial_{k^{\prime}} a^{i^{\prime}}=\frac{\partial}{\partial x^{k^{\prime}}}\left(\frac{\partial x^{i^{\prime}}}{\partial x^{j}} a^{j}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{j}} \frac{\partial x^{l}}{\partial x^{k^{\prime}}} \partial_{l} a^{j}+\frac{\partial^{2} x^{i^{\prime}}}{\partial x^{j} \partial x^{l}} \frac{\partial x^{l}}{\partial x^{k^{\prime}}} a^{j}
\end{equation}
\textbf{Remember, $\frac{\partial x^{i^{\prime}}}{\partial x^{j}}$ is a function of $x^j$ and chain rule (\ref{chain-rule}) must apply here.}
\end{qt}
Similarly, for mixed tensor $T_i^j(x)$
\begin{qt}
\begin{equation}
\begin{aligned}
\partial_{k^{\prime}} T_{j^{\prime}}^{i^{\prime}} &=\frac{\partial}{\partial x^{k^{\prime}}}\left(\frac{\partial x^{i^{\prime}}}{\partial x^{l}} \frac{\partial x^{m}}{\partial x^{j^{\prime}}} T_{m}^{l}\right) \\
&=\frac{\partial x^{n}}{\partial x^{k^{\prime}}} \frac{\partial x^{i^{\prime}}}{\partial x^{l}} \frac{\partial x^{m}}{\partial x^{j^{\prime}}} \partial_{n} T_{m}^{l}+\frac{\partial^{2} x^{i^{\prime}}}{\partial x^{n} \partial x^{l}} \frac{\partial x^{n}}{\partial x^{k^{\prime}}} \frac{\partial x^{m}}{\partial x^{j^{\prime}}} T_{m}^{l}+\frac{\partial x^{i^{\prime}}}{\partial x^{l}} \frac{\partial^{2} x^{m}}{\partial x^{k^{\prime}} \partial x^{j^{\prime}}} T_{m}^{l}
\end{aligned}
\end{equation}
\end{qt}