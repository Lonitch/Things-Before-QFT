\chapter{Tensor Calculus}

\section{Total and Partial Derivatives}
To understand the different kinds of derivatives, let's say we have a function $\rho(t, x(t), p(t))$ which, in general, depends on the location $x(t)$ and momentum $p(t)$ plus the time $t.$ A key observation is that the location $x(t)$ and momentum $p(t)$ are functions of $t$ too. Therefore, we need to be extremely careful what we mean when we calculate the derivative with respect to the time $t .$
$$
\frac{d \rho}{d t}=\lim _{\Delta t \rightarrow 0} \frac{\rho(t+\Delta t, x(t+\Delta t), p(t+\Delta t))-\rho(t, x(t), p(t))}{\Delta t}
$$
\textbf{The result is the total rate of change of $\rho$}.
$$
\frac{\partial \rho}{\partial t}=\lim _{\Delta t \rightarrow 0} \frac{\rho(t+\Delta t, x(t), p(t))-\rho(t, x(t), p(t))}{\Delta t}
$$
The key difference is that we only vary $t$ if it appears explicitly
in $\rho$ but not if it only appears implicitly because $x(t)$ and $p(t)$
also depend on $t .$ Thus
$$
\frac{d \rho}{d t}=\frac{\partial \rho}{\partial x} \frac{d x}{d t}+\frac{\partial \rho}{\partial p} \frac{d p}{d t}+\frac{\partial \rho}{\partial t}
$$
\section{Taylor Expansion}
In general, we want to estimate the value of some function $f(x)$ at some value of $x$ by using our knowledge of the function's value at some fixed point $a .$ The Taylor series then reads
\begin{equation}
\begin{aligned}
f(x)=& \sum_{n=0}^{\infty} \frac{f^{(n)}(a)(x-a)^{n}}{n !} \\
=& \frac{f^{(0)}(a)(x-a)^{0}}{0 !}+\frac{f^{(1)}(a)(x-a)^{1}}{1 !}+\frac{f^{(2)}(a)(x-a)^{2}}{2 !} \\
&+\frac{f^{(3)}(a)(x-a)^{3}}{3 !}+\ldots
\end{aligned}
\end{equation}
or
\begin{equation}
f(x+a)=f(x)+(a \cdot \partial) f(x)+\frac{1}{2}(a \cdot \partial)^{2} f(x)+\cdots
\end{equation}
Taylor expansion of a scalar field (function $f$ that maps $\mathbb{R}^{n}$ to $\mathbb{R}$).Now, identify $\partial f / \partial t$ as $\hat{\boldsymbol{n}} \cdot \nabla f .$ In addition, see that $t \hat{\boldsymbol{n}}=\boldsymbol{x}-\boldsymbol{x}_{0} .$ Some clever recombining of terms gives
\begin{equation}
f(x)=f\left(x_{0}\right)+\left.\left(x-x_{0}\right) \cdot \nabla f\right|_{x_{0}}+\left.\frac{1}{2}\left(\left[x-x_{0}\right] \cdot \nabla\right)^{2} f\right|_{x_{0}}+\ldots
\end{equation}
and
\begin{equation}
\hat{\boldsymbol{n}} \cdot \nabla=\partial_{t}
\end{equation}

\section{Vector Identities}
\begin{equation}
\begin{aligned}
&\vec{\nabla} \cdot(\vec{\nabla} \times \vec{A}) \equiv \operatorname{div}(\operatorname{rot} \vec{A})=(\vec{\nabla} \times \vec{\nabla}) \cdot \vec{A} \equiv 0\\
&\vec{\nabla} \times(\vec{\nabla} \varphi) \equiv \operatorname{rot} \operatorname{grad} \varphi=(\vec{\nabla} \times \vec{\nabla}) \varphi \equiv 0
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
&\vec{\nabla} \cdot(\vec{A} \varphi)=\varphi \vec{\nabla} \cdot \vec{A}+\vec{A} \cdot \vec{\nabla} \varphi \quad \Longleftrightarrow \quad \operatorname{div}(\vec{A} \varphi)=\varphi \operatorname{div} \vec{A}+\vec{A} \cdot \operatorname{grad} \varphi\\
&\vec{\nabla} \times(\vec{A} \varphi)=\varphi \vec{\nabla} \times \vec{A}-\vec{A} \times \vec{\nabla} \varphi \quad \Longleftrightarrow \quad \operatorname{rot}(\vec{A} \varphi)=\varphi \operatorname{rot} \vec{A}-\vec{A} \times \operatorname{grad} \varphi\\
&\vec{\nabla} \cdot(\vec{A} \times \vec{B})=\vec{B} \cdot(\vec{\nabla} \times \vec{A})-\vec{A} \cdot(\vec{\nabla} \times \vec{B}) \quad \Longleftrightarrow \quad \operatorname{div}(\vec{A} \times \vec{B})=\vec{B} \cdot \operatorname{rot} \vec{A}-\vec{A} \cdot \operatorname{rot} \vec{B}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\vec{\nabla} \times(\vec{A} \times \vec{B}) &=(\vec{B} \cdot \vec{\nabla}) \vec{A}-(\vec{A} \cdot \vec{\nabla}) \vec{B}+\vec{A}(\vec{\nabla} \cdot \vec{B})-\vec{B}(\vec{\nabla} \cdot \vec{A}) \\
& \Longleftrightarrow \operatorname{rot}(\vec{A} \times \vec{B})=(\vec{B} \operatorname{grad}) \vec{A}-(\vec{A} \operatorname{grad}) \vec{B}+\vec{A}(\operatorname{div} \vec{B})-\vec{B}(\operatorname{div} \vec{A})
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\vec{\nabla}(\vec{A} \cdot \vec{B})=&(\vec{B} \cdot \vec{\nabla}) \vec{A}+(\vec{A} \cdot \vec{\nabla}) \vec{B}+\vec{A} \times(\vec{\nabla} \times \vec{B})+\vec{B} \times(\vec{\nabla} \times \vec{A}) \\
& \Longleftrightarrow \operatorname{grad}(\vec{A} \cdot \vec{B})=(\vec{B} \cdot \operatorname{grad}) \vec{A}+(\vec{A} \cdot \operatorname{grad}) \vec{B}+\vec{A} \times \operatorname{rot} \vec{B}+\vec{B} \times \operatorname{rot} \vec{A}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
&\vec{\nabla} \cdot(\vec{\nabla} \varphi) \equiv \operatorname{div}(\operatorname{grad} \varphi) \equiv \Delta \varphi=\frac{\partial^{2} \varphi}{\partial x^{2}}+\frac{\partial^{2} \varphi}{\partial y^{2}}+\frac{\partial^{2} \varphi}{\partial z^{2}}, \quad \Delta=\text { Laplace Operator }\\
&\vec{\nabla} \times(\vec{\nabla} \times \vec{A}) \equiv \operatorname{rot}(\operatorname{rot} \vec{A})=\vec{\nabla}(\vec{\nabla} \cdot \vec{A})-(\vec{\nabla} \cdot \vec{\nabla}) \vec{A} \equiv \operatorname{grad} \operatorname{div} \vec{A}-\Delta \vec{A}
\end{aligned}
\end{equation}

\section{Linear Spaces, Vectors, and Tensors}
The linear space, say $L$, consists of the elements (vectors) that \textbf{permit linear operations with the properties described below}:
\begin{itemize}
    \item Summing up the vectors
    \item Multiplication by a number
\end{itemize}
\begin{qt}
    Consider the set of vectors, elements of the linear space L
    $$
\left\{\mathbf{a}_{1}, \mathbf{a}_{2}, \ldots, \mathbf{a}_{n}\right\}=\left\{\mathbf{a}_{i} | i=1, \ldots, n\right\}
$$
and the set of real numbers $k_{1}, k_{2}, \ldots k_{n} .$ Vector
$$
\mathbf{k}=\sum_{i=1}^{n} k^{i} \mathbf{a}_{i}=\mathbf{k}^{i} \mathbf{a}_{i}=0\Longrightarrow \sum_{i=1}^{n}\left(k^{i}\right)^{2}=0
$$
then vectors $\left\{\mathbf{a}_{\mathbf{i}}, i=1, \ldots, n\right\}$ are called \textbf{linearly independent}. The last condition means that no one of the coefficients $k_{i}$ can be different from zero.
\end{qt}
For example, in 2D, two vectors are linearly dependent if and only if they are parallel; in 3D, three vectors are linearly dependent if and only if they belong to the same plane, etc.
\begin{defi}
        The maximal number of linearly independent elements of the linear space $L$ is called its dimension. It proves useful to denote $L_{D}$ a linear space of dimension $D .$
\end{defi}
\begin{thm}
Consider a $D$-dimensional linear space $L_{D}$ and the set of linearly independent vectors $\mathbf{e}_{i}=\left(\mathbf{e}_{1}, \mathbf{e}_{2}, \ldots, \mathbf{e}_{D}\right) .$ Then, for any vector a one can write
\begin{equation}
    \mathbf{a}=\sum_{i=1}^{D} a^{i} \mathbf{e}_{i}=a^{i} \mathbf{e}_{i}
    \label{vector-basis}
\end{equation}
where the coefficients $a^{i}$ are defined in a unique way.
\end{thm}
\bluep{The coefficients $a^{i}$ are called components or \textbf{contravariant components of the vector $\mathbf{a}$}}.The word “contravariant” here means that the components $a^i$ have upper indices.

\section{Direct product of the two linear spaces}
Let's consider the example of phase space in the classic mechanics. The coordinate system in the linear space $L_{D}$ consists of the initial point $O$ and the basis $\mathbf{e}_{i} .$ The position of a point $P$ can be characterized by \textbf{its position vector or radius vector $\mathbf{r}=\overrightarrow{O P}=x^{i} \mathbf{e}_{i}$}.

If the phase space is describing one particle, the space is composed of the radius vectors $\mathbf{r}$ and velocities $\mathbf{v}$ of the particle:
$$
\mathbf{r}=x^{i} \mathbf{e}_{i}, \quad \mathbf{v}=v^{j} \mathbf{f}_{j}
$$
The two set of bases can be related and independent. This example is a particular case of the linear space which is called \redp{a direct product
of the two linear spaces}. The element of the direct product of the two linear spaces $L_{D_{1}}$ and $L_{D_{2}}$ (\textbf{they can have different dimensions}) is the ordered set of the elements of each of the two spaces $L_{D_{1}}$ and $L_{D_{2}} .$
\begin{qt}
    The notation for the basis in the case of configuration space is $\mathbf{e}_{i} \otimes \mathbf{f}_{j}$. Hence, the state of the point-like particle in the phase space is characterized by the element of this linear space, which can be presented as:
    $$
x^{i} v^{j} \mathbf{e}_{i} \otimes \mathbf{f}_{j}
$$
In general, one can define the space that is a direct product of several linear spaces with different individual basis sets $\mathbf{e}_{i}$ each. In this case, we will have $\mathbf{e}_{i}^{(1)}, \mathbf{e}_{i}^{(2)}, \ldots$ $\mathbf{e}_{i}^{(N)} .$ The basis in the direct product space will be
$$
\mathbf{e}_{i_{1}}^{(1)} \otimes \mathbf{e}_{i_{2}}^{(2)} \otimes \ldots \otimes \mathbf{e}_{i_{N}}^{(N)}
$$
and the element becomes
$$
T^{i_{1} i_{2} \ldots i_{N}} \mathbf{e}_{i_{1}}^{(1)} \otimes \mathbf{e}_{i_{2}}^{(2)} \otimes \ldots \otimes \mathbf{e}_{i_{N}}^{(N)}
$$
\end{qt}
\section{Vector basis and its transformation}
Let us start from the components of the vector, which were defined in (\ref{vector-basis}). Consider, along with the original basis $\mathbf{e}_{i},$ another basis $\mathbf{e}_{i}^{\prime} .$ since each vector of the new basis belongs to the same space, it can be expanded using the original basis as
\begin{equation}
\mathbf{e}_{i}^{\prime}=\wedge_{i^{\prime}}^{j} \mathbf{e}_{j}
\label{basis-transform}
\end{equation}
and
$$
\mathbf{a}=a^{i} \mathbf{e}_{i}=a^{j^{\prime}} \mathbf{e}_{j^{\prime}}=a^{j^{\prime}} \wedge_{j^{\prime}}^{i} \mathbf{e}_{i}
$$
\begin{qt}
\begin{equation}
    a^{i}=a^{j^{\prime}} \wedge_{j^{\prime}}^{i}
    \label{cotravariant-coord-transform}
\end{equation}
\end{qt}

Similarly, we can make inverse transformation:
$$
a^{k^{\prime}}=\left(\wedge^{-1}\right)_{l}^{k^{\prime}} a^{l}
$$
and
\[
\left(\wedge^{-1}\right)_{l}^{k^{\prime}} \cdot \wedge_{i^{\prime}}^{l}=\delta_{i^{\prime}}^{k^{\prime}} \quad \text { and } \quad \wedge_{i^{\prime}}^{l} \cdot\left(\wedge^{-1}\right)_{k}^{i^{\prime}}=\delta_{k}^{l}
\]
\begin{qt}
Taking the partial derivatives, we arrive at the relations
\[
\wedge_{j^{\prime}}^{i}=\frac{\partial x^{i}}{\partial x^{j^{\prime}}} \quad \text { and } \quad\left(\wedge^{-1}\right)_{l}^{k^{\prime}}=\frac{\partial x^{k^{\prime}}}{\partial x^{l}}
\]
and
$$
\left(\wedge^{-1}\right)_{l}^{k^{\prime}} \cdot \wedge_{k^{\prime}}^{i}=\frac{\partial x^{k^{\prime}}}{\partial x^{l}} \frac{\partial x^{i}}{\partial x^{k^{\prime}}}=\frac{\partial x^{i}}{\partial x^{l}}=\delta_{l}^{i}
$$
is nothing but the chain rule for partial derivatives.
\end{qt}

\section{Scalar, vector, and tensor fields}
\begin{defi}
        Function $\varphi(x)$ is called scalar field or simply scalar if it does not transform under the change of coordinates
        \begin{equation}
\varphi(x)=\varphi^{\prime}\left(x^{\prime}\right)
\end{equation}
\end{defi}
Let us give a clarifying example in 1D. Consider a function
$$
y=x^2
$$
Now, let us change the variables 
$$
x^{\prime}=x+1
$$
The function $y=\left(x^{\prime}\right)^{2},$ obviously, represents another parabola. \bluep{In order to preserve the plot intact, we need to modify the form of the function, that is, to go from $\varphi$ to $\varphi^{\prime}$.} The new function $y^{\prime}=\left(x^{\prime}-1\right)^{2}$ will represent the original parabola,because the change of the variable is completely compensated by the change of the form of the function.
\begin{mybox}
\begin{center}
    Discuss whether the three numbers temperature $T(x)$, pressure $p(x)$ and density $\rho(x)$ form a contravariant vector
\end{center}
\end{mybox}
\begin{mybox2}
We can only form contravariant vector if the numbers transform by following the rule (\ref{cotravariant-coord-transform}). Since these numbers are scalar fields, it transforms like $T(\mathbf{r})=T^{\prime}(\mathbf{r}^{\prime})$. Thus, these three parameters can not form a contravariant vector.
\end{mybox2}
\begin{example}
From the definition above we know $\varphi^{\prime}(x) \neq \varphi(x)$ and $\varphi\left(x^{\prime}\right) \neq \varphi(x)$. Let us calculate these quantities explicitly for the special case of infinitesimal transformation $x^{\prime i}=x^{i}+\xi^{i}$ where $\xi$ are constant coefficients. Now we have
$$
\varphi\left(x^{\prime}\right)=\varphi\left(x+\xi\right)\overset{Taylor}{=}\varphi(x)+\frac{\partial \varphi}{\partial x^{i}} \xi^{i}
$$
and,
$$
\varphi^{\prime}\left(x^{i}\right)=\varphi^{\prime}\left(x^{i}-\xi^{i}\right)=\varphi^{\prime}\left(x^{\prime}\right)-\frac{\partial \varphi^{\prime}}{\partial x^{i^{\prime}}} \cdot \xi^{i}
$$
rewrite the two equations above, we find:
$$
\frac{\partial \varphi^{\prime}}{\partial x^{\prime i}}=\frac{\varphi^{\prime}\left(x^{\prime i}\right)-\varphi^{\prime}\left(x^{i}-\xi^{i}\right)}{\xi^i}
$$
$$
\frac{\partial \varphi}{\partial x^{i}}=\frac{\varphi(x^{\prime i})-\varphi(x^i)}{\xi^i}
$$
At the limit of $\xi\rightarrow0$, we have
$$
\frac{\partial \varphi^{\prime}}{\partial x^{\prime i}}=\frac{\partial \varphi(x)}{\partial x^{i}}+\mathcal{O}(\xi)
$$
As mentioned above, 
$$
\varphi^{\prime}\left(x^{i}\right)=\varphi^{\prime}\left(x^{\prime}\right)-\frac{\partial \varphi^{\prime}}{\partial x^{\prime i}} \cdot \xi^{i}\\
=\varphi^{\prime}\left(x^{\prime}\right)-\frac{\partial \varphi(x)}{\partial x^{i}}\cdot\xi^i
$$
Since $\varphi(x)=\varphi^{\prime}\left(x^{\prime}\right)$, we have
$$
\varphi^{\prime}(x)=\varphi(x)-\xi^{i} \partial_{i} \varphi
$$
\textbf{where we have introduced a useful notation $\partial_{i}=\partial / \partial x^{i}$.}
\end{example}
\textbf{For the vector field, we have the following rule for the coordinate transformation}. The set of three functions $\left\{a^{i}(x)\right\}=\left\{a^{1}(x), a^{2}(x), a^{3}(x)\right\}$ forms contravariant vector field (or simply vector field) if they transform, under the change of coordinates $\left\{x^{i}\right\} \rightarrow\left\{x^{\prime} j\right\},$ as
\begin{qt}
\begin{equation}
a^{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}}}{\partial x^{i}} \cdot a^{i}(x)
\end{equation}
\end{qt}
\bluep{The components of the vector in a given geometrical point of space modify under the coordinate transformation, while the scalar field does not.}The scalar and vector fields can be considered as examples of the more general objects called tensors. Tensors are also defined through their transformation rules.
\begin{qt}
The set of $3^n$ functions $\left\{a^{i_{1} \ldots i_{n}}(x)\right\}$ is called a contravariant tensor of rank $n,$ if these functions transform, under $x^{i} \rightarrow x^{i},$ as
\begin{equation}
a^{i_{1}^{i} \ldots i_{n}^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i_{1}^{\prime}}}{\partial x^{j_{1}}} \ldots \frac{\partial x^{i_{n}^{\prime}}}{\partial x^{j_{n}}} a^{j_{1} \ldots j_{n}}(x)
\end{equation}
\end{qt}

\section{Orthonormal Basis and Cartesian Coordiantes}
The scalar product of two vectors $\mathbf{a}$ and $\mathbf{b}$ in 3D is defined in a ususal way,
\begin{equation}
(\mathbf{a}, \mathbf{b})=\mathbf{a}\cdot\mathbf{b}=|\mathbf{a}| \cdot|\mathbf{b}| \cdot \cos \theta
\end{equation}
Special orthonormal basis $\left\{\hat{\mathbf{n}}_{a}\right\}$ is the one with
\begin{equation}
\left(\hat{\mathbf{n}}_{a}, \hat{\mathbf{n}}_{b}\right)=\delta_{a b}=\left\{\begin{array}{l}
{1 \text { if } a=b} \\
{0 \text { if } a \neq b}
\end{array}\right.
\end{equation}
\begin{example}
Making transformations of the basis vectors, verify that the change of coordinates
$$
x^{\prime}=\frac{x+y}{\sqrt{2}}+3, \quad y^{\prime}=\frac{x-y}{\sqrt{2}}-5
$$
does not modify the type of coordinates $x^{\prime}, y^{\prime},$ which remains Cartesian.
\textbf{Solution}:
$$
x^{\prime}=\frac{x+y}{\sqrt{2}}+3, \quad y^{\prime}=\frac{x-y}{\sqrt{2}}-5
$$
The change of initial point $(0,0) \rightarrow(3,-5)$ does not have relation to the change of basis. Then
$$
\begin{aligned}
x^{\prime} \hat{i}^{\prime}+y^{\prime} \hat{j}^{\prime} &=\left(\frac{x+y}{\sqrt{2}}\right) \hat{i}^{\prime}+\left(\frac{x-y}{\sqrt{2}}\right) \hat{j}^{\prime} \\
&=\frac{x}{\sqrt{2}}\left(\hat{i}^{\prime}+\hat{j}^{\prime}\right)+\frac{y}{\sqrt{2}}\left(\hat{i}^{\prime}-\hat{j}^{\prime}\right)=x \hat{i}+y \hat{j}
\end{aligned}
$$
Thus, $\hat{i}=\frac{1}{\sqrt{2}}\left(\hat{i}^{\prime}+\hat{j}^{\prime}\right), \hat{j}=\frac{1}{\sqrt{2}}\left(\hat{i}^{\prime}-\hat{j}^{\prime}\right) .$ Obviously, $\hat{i}^{2}=\hat{j}^{2}=1$ and $\hat{i} \cdot \hat{j}=0$
\end{example}
Now we can introduce a conjugated covariant basis.
\begin{qt}
Consider basis $\left\{\mathbf{e}_{i}\right\} .$ The conjugated basis is defined as a set of vectors $\left\{\mathbf{e}^{j}\right\}$ which satisfy the relations
\begin{equation}
\mathbf{e}_{i} \cdot \mathbf{e}^{j}=\delta_{i}^{j}
\end{equation}
The special property of the orthonormal basis is that $\hat{\mathbf{n}}^{a}=\hat{\mathbf{n}}_{a}$.

Any vector a can be expanded using the conjugated basis $\mathbf{a}=a_{i} \mathbf{e}^{i} .$ \redp{The coefficients $a_{i}$ are called covariant components of the vector $\mathbf{a}$}.
\end{qt}
In the case of covariant vector components, the transformation is done by means of the matrix inverse to the one for the contravariant components.
\begin{thm}
If we change the basis of the coordinate system from $\mathbf{e}_{i}$ to $\mathbf{e}_{i}^{\prime},$ then the covariant components of the vector a transform as
\begin{equation}
a_{i}^{\prime}=\frac{\partial x^{j}}{\partial x^{i}} a_{j}
\end{equation}
\end{thm}

 The set of three functions $\left\{A_{i}(x)\right\}$ forms a covariant vector field, if they transform from one coordinate system to another one as
 \begin{equation}
A_{i}^{\prime}\left(x^{\prime}\right)=\frac{\partial x^{j}}{\partial x^{i}} A_{j}(x)
\end{equation}
The set of $3^{n}$ functions $\left\{A_{i_{1} i_{2} \ldots i_{n}}(x)\right\}$ form a covariant tensor of rank $n$ if they transform from one coordinate system to another as
\begin{equation}
A_{i_{1} i_{2} \ldots i_{n}}^{\prime}\left(x^{\prime}\right)=\frac{\partial x^{j_{1}}}{\partial x^{i_{1}}} \frac{\partial x^{j_{2}}}{\partial x^{i_{2}}} \cdots \frac{\partial x^{j_{n}}}{\partial x^{n_{n}}} A_{j_{1} j_{2} \ldots j_{n}}(x)
\end{equation}
In general
\begin{qt}
The set of $3^{n+m}$ functions $\left\{B_{i_{1} \ldots i_{n}} j_{1 \ldots j_{n}}(x)\right\}$ forms the tensor of the type $(m, n),$ if these functions transform, under the change of coordinate basis, as
\begin{equation}
B_{i_{1}^{\prime} \ldots i_{n}^{\prime}} ^{j_{1}^{\prime} \ldots j_{m}^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}_1}}{\partial x^{l_{1}}} \cdots \frac{\partial x^{j_{m}^{\prime}}}{\partial x^{l_{m}}} \frac{\partial x^{k_{1}}}{\partial x^{\prime i_{1}}} \cdots \frac{\partial x^{k_{n}}}{\partial x^{\prime i_{n}}} B_{k_{1} \ldots k_{n}}^{l_{1} \ldots l_{m}}(x)
\end{equation}
Other possible names are the mixed tensor of covariant rank $n$ and contravariant rank $m,$ or simply $(m, n)$ -tensor.
\end{qt}
\textbf{Tensors are important due to the fact that they offer the coordinate-independent description of geometrical and physical laws.} The following example shows this observation:
\begin{example}
For an arbitrary $\mathbf{a}(x)=a^{i}(x) \mathbf{e}_{i}$ we have
$$
a^{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}}}{\partial x^{i}} a^{i}(x), \quad \mathbf{e}_{j^{\prime}}\left(x^{\prime}\right)=\mathbf{e}_{k}(x) \frac{\partial x^{k}}{\partial x^{j^{\prime}}}
$$
Then
$$
a^{j^{\prime}}\left(x^{\prime}\right) \mathbf{e}_{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}}}{\partial x^{i}} a^{i}(x) \mathbf{e}_{k}(x) \frac{\partial x^{k}}{\partial x^{j^{\prime}}}=\delta_{l}^{k} a^{i}(x) \mathbf{e}_{k}(x)=a^{i}(x) \mathbf{e}_{i}(x)
$$
\end{example}

If the Kronecker symbol transforms as a mixed $(1,1)$ tensor,
\begin{qt}
\begin{equation}
\delta_{j^{\prime}}^{i^{\prime}}=\frac{\partial x^{i^{\prime}}}{\partial x^{k}} \frac{\partial x^{l}}{\partial x^{j^{\prime}}} \delta_{l}^{k}=\frac{\partial x^{i^{\prime}}}{\partial x^{j^{\prime}}}
\end{equation}
\end{qt}
then in any coordinates $x^i$ it has the same form 
$$
\delta_{j}^{i}=\left\{\begin{array}{l}
{1 \text { if } i=j} \\
{0 \text { if } i \neq j}
\end{array}\right.
$$
\textbf{This property is very important, as it enables us to use the Kronecker symbol in any coordinates}

\begin{example}
Show that the product $A^{i}(x) B_{j}(x)$ of covariant and contravariant vectors transforms as a $(1,1)$ -type mixed tensor.
$$
A^{i^{\prime}}\left(x^{\prime}\right) A_{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{k}} A^{k}(x) \frac{\partial x^{l}}{\partial x^{j^{\prime}}} B_{l}(x)=\frac{\partial x^{i^{\prime}}}{\partial x^{k}} \frac{\partial x^{l}}{\partial x^{j^{\prime}}} A^{k}(x) B_{l}(x)
$$
\end{example}

\section{Orthogonal transformation}
The rotation transformation around $\hat{\mathbf{z}}-$axis is given by the following relation:
\begin{qt}
\begin{equation}
\left(\begin{array}{l}
{x} \\
{y} \\
{z}
\end{array}\right)=\hat{\wedge}_{z}\left(\begin{array}{l}
{x^{\prime}} \\
{y^{\prime}} \\
{z^{\prime}}
\end{array}\right), \quad \text { where } \quad \hat{\wedge}_{z}=\hat{\wedge}_{z}(\alpha)=\left(\begin{array}{ccc}
{\cos \alpha} & {-\sin \alpha} & {0} \\
{\sin \alpha} & {\cos \alpha} & {0} \\
{0} & {0} & {1}
\end{array}\right)
\end{equation}
the matrix above has the following property:
\begin{equation}
\hat{\wedge}_{z}^{T}=\hat{\wedge}_{z}^{-1}
\end{equation}
\end{qt}
\begin{defi}
        The matrix $\hat{\wedge}_{z}$ which satisfies $\hat{\wedge}_{z}^{-1}=\hat{\wedge}_{z}^{T}$ and the corresponding coordinate transformation is called \textbf{orthogonal}.
\end{defi}
Similarly, we can write the rotation matrix around other axis:
\begin{equation}
\hat{\wedge}_{x}(\gamma)=\left(\begin{array}{ccc}
{1} & {0} & {0}\\
{0} & {\cos \gamma} & {-\sin \gamma} \\
{0} & {\sin \gamma} & {\cos \gamma}
\end{array}\right)
\end{equation}
\begin{equation}
\hat{\wedge}_{y}(\beta)=\left(\begin{array}{ccc}
{\cos \beta} & {0} & {-\sin \beta} \\
{0} & {1} & {0}\\
{\sin \beta} & {0} &  {\cos \beta}
\end{array}\right)
\end{equation}
In 3D space, any rotation of the rigid body may be represented as a combination of the rotation around the axes $\hat{\mathbf{z}}, \hat{\mathbf{y}},$ and $\hat{\mathbf{x}}$ to the angles $ \alpha, \beta,$ and $\gamma$:
$$
\hat{\wedge}=\hat{\wedge}_{z}(\alpha) \hat{\wedge}_{y}(\beta) \hat{\wedge}_{x}(\gamma)
$$
Since $(A \cdot B)^{T}=B^{T} \cdot A^{T} \quad \text { and } \quad(A \cdot B)^{-1}=B^{-1} \cdot A^{-1}$, we can easily obtain that \bluep{the general 3D rotation matrix satisfies the orthogonal relation.}
\begin{qt}
For the orthogonal matrix, one can take the determinant and arrive at $det\hat{\wedge}=det\hat{\wedge}^{-1}$. Therefore, 
$$
det\hat{\wedge}=\pm1
$$
As far as any rotation matrix has a determinant equal to one, there must be some other orthogonal matrices with the determinant equal to −1.
\end{qt}
In case where the matrix elements are allowed to be complex, the matrix that satisfies the property $U^{\dagger}=U^{-1}$ is called unitary. The operation $U^{\dagger}$ is called \textbf{Hermitian conjugation} and consists of complex conjugation plus transposition $U^{\dagger}=\left(U^{*}\right)^{T}$.
\begin{example}
Consider the transformation from one orthonormal basis $\hat{\mathbf{n}}_{i}$ to another such basis $\hat{\mathbf{n}}_{k}^{\prime},$ namely, $\hat{\mathbf{n}}_{k}^{\prime}=R_{k}^{i} \hat{\mathbf{n}}_{i} .$ Prove that the matrix $\left\|R_{k}^{i}\right\|$ is orthogonal.

 $\hat{\mathbf{n}}_{k}^{\prime}=R_{k}^{i} \hat{\mathbf{n}}_{i}$ and also $\hat{\mathbf{n}}^{\prime \prime}=\left(R^{-1}\right)_{j}^{l} \hat{\mathbf{n}}^{j} .$ since $\hat{\mathbf{n}}_{k}^{\prime}=\hat{\mathbf{n}}^{k^{\prime}}$ and $\hat{\mathbf{n}}_{i}=\hat{\mathbf{n}}^{i}$
also have $\left(R^{-1}\right)_{j}^{l}=\left(R^{T}\right)_{j}^{l}$
\end{example}

\section{Operations over Tensors, Metric Tensor}
\textbf{Multiplication of a tensor by a number produces a tensor of the same type.} This operation is equivalent to the multiplication of all tensor components to the same number $\alpha$, namely:
\begin{equation}
(\alpha A)_{i_{1} \ldots i_{n}}=\alpha \cdot A_{i_{1} \ldots i_{n}}^{j_{1} \ldots j_{m}}
\end{equation}
\textbf{Multiplication of two tensors is defined for a couple of tensors of any type.} The product of a (m, n)-tensor and a (t, s)-tensor results in the (m + t, n + s)- tensor, e.g.,
\begin{equation}
A_{i_{1} \ldots i_{n}} {}^{j_1 \cdots j_{m}} \cdot C_{l_{1} \ldots l_{s}} {}^{k_1 \ldots k_{i}}=D_{i_{1} \ldots i_{n}} {}^{j_1 \cdots j_{m}} {}_{l_1 \ldots l_{s}}{}^{k_1 \ldots k_{l}}
\end{equation}
\textbf{The order of indices is important here, because $a_{i j}$ may be different from $a_{j i}$.}
\begin{example}
 Prove, by checking the transformation law, that the product of the contravariant vector $a^{i}$ and mixed tensor $b_{i}^{k}$ is a mixed $(2,1)$ -type tensor.
 
$$
a^{i^{\prime}}\left(x^{\prime}\right) b_{j^{\prime}}^{k^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{m}} a^{m}(x) \frac{\partial x^{k^{\prime}}}{\partial x^{l}} \frac{\partial x^{n}}{\partial x^{j^{\prime}}} b_{n}^{l}(x)
$$
\end{example}
\textbf{Contraction reduces the (n,m)-tensor to the (n-1,m-1)-tensor through the summation over two (always upper and lower, of course) indices.} For example,
\begin{equation}
A_{i j k}^{l n} \longrightarrow A_{i j k}^{l k}=\sum_{k=1}^{3} A_{i j k}^{l k}
\end{equation}

The internal product of the two tensors consists in their multiplication with the consequent contraction over some couple of indices. \textbf{Internal product of (m, n) and (r, s)-type tensors results in the (m+r-1, n+s-1)-type tensor.}
\begin{equation}
A_{i j k} \cdot B^{l j}=\sum_{j=1}^{3} A_{i j k} B^{l j}
\end{equation}
\begin{example}
Prove that the internal product $a_{i} \cdot b^{i}$ is a scalar if $a_{i}(x)$ and $b^{i}(x)$ are co- and contravariant vectors.

$$
b^{i^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{l}} b^{l}(x), \quad a_{i^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{k}}{\partial x^{i^{\prime}}} a_{k}(x)
$$
Then
$$
a_{i^{\prime}}\left(x^{\prime}\right) b^{i^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{l}} \frac{\partial x^{k}}{\partial x^{i^{\prime}}} b^{l}(x) a_{k}(x)=\delta_{l}^{k} b^{l}(x) a_{k}(x)=b^{k}(x) a_{k}(x)
$$
\end{example}
\begin{defi}
        Consider a basis $\left\{\mathbf{e}_{i}\right\} .$ The \textbf{scalar product} of the two basis vectors,
        \begin{equation}
g_{i j}=\left(\mathbf{e}_{i}, \mathbf{e}_{j}\right)
\end{equation}
is called \textbf{metric}. Here \textbf{the scalar product is not always inner product.}
\end{defi}
\begin{qt}
\textbf{Properties of metric}:

1. Symmetry of the metric $g_{i j}=g_{j i}$ follows from the symmetry of a scalar product $\left(\mathbf{e}_{i}, \mathbf{e}_{j}\right)=\left(\mathbf{e}_{j}, \mathbf{e}_{i}\right)$

2. For the orthonormal basis $\hat{\mathbf{n}}_{a}$, the metric is nothing but the Kronecker symbol $g_{a b}=\left(\hat{\mathbf{n}}_{a}, \hat{\mathbf{n}}_{b}\right)=\delta_{a b}$

3. Metric is a (0, 2) - tensor, as
$$
\begin{aligned}
g_{i^{\prime} j^{\prime}} &=\left(\mathbf{e}_{i}^{\prime}, \mathbf{e}_{j}^{\prime}\right)=\left(\frac{\partial x^{l}}{\partial x^{i^{\prime}}} \mathbf{e}_{l}, \frac{\partial x^{k}}{\partial x^{\prime j}} \mathbf{e}_{k}\right) \\
&=\frac{\partial x^{l}}{\partial x^{\prime i}} \frac{\partial x^{k}}{\partial x^{\prime j}}\left(\mathbf{e}_{l}, \mathbf{e}_{k}\right)=\frac{\partial x^{l}}{\partial x^{\prime i}} \frac{\partial x^{k}}{\partial x^{\prime j}} \cdot g_{k l}
\end{aligned}
$$

4. The distance between two points: $M_{1}\left(x^{i}\right)$ and $M_{2}\left(y^{i}\right)$ is defined by the inner product:
$$
S_{12}^{2}=g_{i j}\left(x^{i}-y^{i}\right)\left(x^{j}-y^{j}\right)
$$
\end{qt}
\bluep{Since $g_{i j}$ is $(0,2)$ - tensor and $\left(x^{i}-y^{i}\right)$ is $(1,0)$ -tensor (contravariant vector) $-S_{12}^{2}$ is a scalar. Therefore, $S_{12}^{2}$ is the same in any coordinate system.}

\textbf{The conjugated metric} is defined as
\begin{equation}
g^{i j}=\left(\mathbf{e}^{i}, \mathbf{e}^{j}\right) \quad \text { where } \quad\left(\mathbf{e}^{i}, \mathbf{e}_{k}\right)=\delta_{k}^{i}
\end{equation}
\begin{qt}
\begin{equation}
g^{i k} g_{k j}=\delta_{j}^{i}
\end{equation}
\end{qt}

We can\textbf{ Raise and lower indices of a tensor} by taking an appropriate internal product of a given tensor and the corresponding metric tensor.
\begin{equation}
\begin{aligned}
&\text { Lowering the index, } A_{i}(x)=g_{i j} A^{j}(x), \quad B_{i k}(x)=g_{i j} B_{ k}^{j}(x)\\
&\text { Raising the index, } \quad C^{l}(x)=g^{l j} C_{j}(x), \quad D^{i k}(x)=g^{i j} D_{j}^{k}(x)
\end{aligned}
\end{equation}
\begin{equation}
    \mathbf{e}_{i} g^{i j}=\mathbf{e}^{j}, \quad \mathbf{e}^{k} g_{k l}=\mathbf{e}_{l}
\end{equation}
\begin{qt}
Let the metric $g_{ij}$ correspond to the basis ek and to the coordinates $x^{k} .$ The determinants of the metric tensors and of the matrices of the transformations to Cartesian coordinates $X^{a}$ satisfy the following relations:
\begin{equation}
g=\operatorname{det}\left(g_{i j}\right)=\operatorname{det}\left(\frac{\partial X^{a}}{\partial x^{k}}\right)^{2}, \quad g^{-1}=\operatorname{det}\left(g^{k l}\right)=\operatorname{det}\left(\frac{\partial x^{l}}{\partial X^{b}}\right)^{2}
\end{equation}
\end{qt}
The possibility of lowering and raising the indices may help us in contracting two contravariant or two covariant indices of a tensor. 
\begin{example}
Suppose we need to contract the two first indices of the $(3,0)$ -tensor $B^{i j k} .$ After we lower the second index, we arrive at the tensor
$$
B^i{}_{l}{}^{k}=B^{i j k} g_{j l}
$$
And now we can contract the indices $i,j$. But, if we forget to indicate the order of indices, we obtain:$B_{l}^{i k}$, and it is not immediately clear which index was lowered and in which couple of indices one has to perform the contraction.
\end{example}

\section{Symmetric, Skew(Anti) Symmetric Tensors, and Determinants}
\subsection{Definitions and general considerations}
Tensor $A^{i j k \ldots}$ is called \textbf{symmetric} in the indices $i$ and $j,$ if
\begin{equation}
A^{i j k \ldots}=A^{j i k \ldots}
\end{equation}
Tensor $A^{i_{1} i_{2} \ldots i_{n}}$ is called \textbf{completely (or absolutely) symmetric} in the indices $\left(i_{1}, i_{2}, \ldots, i_{n}\right),$ if it is symmetric in any couple of these indices. 

Tensor $A^{i j}$ is called skew-symmetric or antisymmetric, if
\begin{equation}
A^{i j}=-A^{j i}
\end{equation}
\bluep{The advantage of tensors is that their (anti)symmetry holds under the transformation from one basis to another.}
\begin{example}
Consider the basis in 2D,
$$
\mathbf{e}_{1}=\hat{\mathbf{i}}+\hat{\mathbf{j}}, \quad \mathbf{e}_{2}=\hat{\mathbf{i}}-\hat{\mathbf{j}}
$$
(i) Derive all components of the absolutely antisymmetric tensor $\varepsilon^{i j}$ in the new basis, taking into account that $\varepsilon^{a b}=\epsilon^{a b},$ where $\epsilon^{12}=1$ in the orthonormal basis $\hat{\mathbf{n}}_{1}=\hat{\mathbf{i}}, \quad \hat{\mathbf{n}}_{2}=\hat{\mathbf{j}} .$ The calculation should be performed directly and also by using the formula for antisymmetric tensor. Explain the difference between the two results. 

(ii) Repeat the calculation for $\varepsilon_{i j} .$ Calculate the metric components and verify that $\varepsilon_{i j}=g_{i k} g_{j l} \varepsilon^{k l}$ and that $\varepsilon^{i j}=g^{i k} g^{j l} \varepsilon_{k l}$

\end{example}
\textbf{Solution:}
From the basis relation, we have:
$$
x^{\prime}=x+y
$$
$$
y^{\prime}=x-y
$$
Thus, 
$$
\frac{\partial x^{j^{\prime}}}{\partial x^i}=\left(\begin{array}{cc}
{1} & {1}\\
{1} & {-1}
\end{array}\right)
$$
and $\varepsilon^{12}=\frac{\partial x^{\prime 1}}{\partial x^k}\frac{\partial x^{\prime 2}}{\partial x^l}\epsilon^{kl}=\epsilon^{21}-\epsilon^{12}=-2$. Since \textbf{the metric tensor of the orthonormal basis is}
$$
g_{ab}=\left(\begin{array}{cc}
{1} & {0}\\
{0} & {1}
\end{array}\right)
$$
we have $\epsilon_{ab}=\epsilon^{ab}$. Thus, $\varepsilon_{12}=\frac{\partial x^k}{\partial x^{\prime 1}}\frac{\partial x^l}{\partial x^{\prime 2}}\epsilon_{kl}=-1/2$. Also, the metric tensor for our new basis is
$$
g_{ij}=\frac{\partial x^{a}}{\partial x^i}\frac{\partial x^{b}}{\partial x^j}g_{ab}=\left(\begin{array}{cc}
{1/2} & {0}\\
{0} & {1/2}
\end{array}\right)
$$
Using $g^{i k} g_{k j}=\delta_{j}^{i}$, we also have $g^{ij}=\left(\begin{array}{cc}
{2} & {0}\\{0} & {2}\end{array}\right)$. Obviously, we have:
\begin{qt}
$$
\varepsilon_{i j}=g_{i k} g_{j l} \varepsilon^{k l}
$$
$$
\varepsilon^{i j}=g^{i k} g^{j l} \varepsilon_{k l}
$$
\end{qt}

\subsection{Completely Antisymmetric Tensors}
$(n, 0)$ -tensor $A^{i_{1} \ldots i_{n}}$ is called completely (or absolutely) antisymmetric, if it is antisymmetric in any couple of its indices
\[
\forall(k, l), \quad A^{i_{1} \ldots i_{l} \ldots i_{k} \ldots i_{n}}=-A^{i_{1} \ldots i_{k} \ldots i_{l} \ldots i_{n}}
\]
In the case of an absolutely antisymmetric tensor, the sign changes when we perform permutation of any two indices.
\begin{qt}
\begin{itemize}
    \item In a D-dimensional space, all completely antisymmetric tensors of rank $n>D$ are equal to zero.

    \item Theorem 2 An absolutely antisymmetric tensor $A^{i_{1} \ldots i_{D}}$ in a D-dimensional space has only one independent component.
    
    \item \bluep{Prove that for symmetric $a^{i j}$ and antisymmetric $b_{i j}$ tensors the scalar internal product is zero $a^{i j} \cdot b_{i j}=0$}
\end{itemize}
\end{qt}
For the last point, we can use the following argument to prove it:
$$
a^{j i} b_{j i}=\left(+a^{i j}\right)\left(-b_{i j}\right)\substack{i\rightarrow j,j\rightarrow i\\=}-a^{i j} b_{i j}
$$
The quantity that equals itself with an opposite sign is zero.
\begin{example}
(i) Prove that if $b^{i j}(x)$ is a tensor and $a_{l}(x)$ and $c_{k}(x)$ are covariant vector fields, then $f(x)=b^{i j}(x) \cdot a_{i}(x) \cdot c_{j}(x)$ is a scalar field.

(ii) In case $a_{i}(x) \equiv c_{i}(x),$ formulate the sufficient condition for $b^{i j}(x),$ such that the product $f(x) \equiv 0$
\end{example}
\textbf{Solution:}

(i) $b^{i j}(x) \cdot a_{i}(x) \cdot c_{j}(x)=d^{ij}{}_{ij}(x)$, which is a scalar.

(ii) when $b$ is anti-symmetric: $b^{i j}=b^{[i j]}$

\subsubsection{Determinants}
Consider first $2 D$ and special coordinates $X^{1}$ and $X^{2}$ corresponding to the orthonormal basis $\hat{\mathbf{n}}_{1}$ and $\hat{\mathbf{n}}_{2}$. We first define a special maximal absolutely antisymmetric object in 2D using the relations:
$$
E_{a b}=-E_{b a} \quad \text { and } \quad E_{12}=1
$$
\begin{qt}
For and matrix $\left\|C_{b}^{a}\right\|$ we can write its determinant as
\begin{equation}
\operatorname{det}\left\|C_{b}^{a}\right\|=E_{a b} C_{1}^{a} C_{2}^{b}
\end{equation}
and
\begin{equation}
\operatorname{det}\left(C_{b}^{a}\right)=\frac{1}{2} E_{a b} E^{d e} C_{d}^{a} C_{e}^{b}
\end{equation}
The difference between these two equations is that the latter admits two expressions $C_{1}^{a} C_{2}^{b}$ and $C_{2}^{a} C_{1}^{b}$. Thus, we have the $1/2$ factor in the second equation.
\end{qt}
Because $E_{ab}$ is maximal absolutely anti-symmetric, we also have:
\begin{equation}
E_{a b} C_{e}^{a} C_{d}^{b}=-E_{a b} C_{e}^{b} C_{d}^{a}
\end{equation}
\textit{Proof}
$$
E_{a b} C_{e}^{a} C_{d}^{b}=E_{b a} C_{e}^{b} C_{d}^{a}=-E_{a b} C_{d}^{a} C_{e}^{b}
$$
Here in the first equality, we have exchanged the names of the umbral(dummy) indices $a \leftrightarrow b$ and in the second one used antisymmetry $E_{a b}=-E_{b a}$. \bluep{Now we consider an arbitrary dimension of space D.}
\begin{qt}
Consider $a, b=1,2,3, \ldots, D . \quad \forall$ matrix $\left\|C_{b}^{a}\right\|,$ the determinant is
\begin{equation}
\operatorname{det}\left\|C_{b}^{a}\right\|=E_{a_{1} a_{2} \dots a_{D}} \cdot C_{1}^{a_{1}} C_{2}^{a_{2}} \cdots C_{D}^{a_{D}}
\end{equation}
where $E_{a_1a_2\dots a_D}$ is absolutely antisymmetric and $E_{123\dots D}=1$. \redp{The determinant in the equation above is a sum of D! terms, each of which is a product of elements from different lines and columns, taken with the positive sign for even and with the negative sign for odd parity of permutations.}
\end{qt}
In general, the repression
\begin{equation}
\mathcal{A}_{a_{1} \ldots a_{D}}=E_{b_{1} \ldots b_{D}} A_{a_{1}}^{b_{1}} A_{a_{2}}^{b_{2}} \ldots A_{a_{D}}^{b_{D}}
\end{equation}
\bluep{is absolutely antisymmetric in the indices $\left\{a_{1}, \ldots, a_{D}\right\}$ and therefore is proportional to $E_{a_{1} \ldots a_{D}}$}.
\begin{qt}
In an arbitrary dimension D
\begin{equation}
\operatorname{det}\left\|C_{b}^{a}\right\|=\frac{1}{D !} E_{a_{1} a_{2} \ldots a_{D}} E^{b_{1} b_{2} \ldots b_{D}} \cdot C_{b_{1}}^{a_{1}} C_{b_{2}}^{a_{2}} \ldots C_{b_{D}}^{a_{D}}
\end{equation}
and
\begin{equation}
E^{a_{1} a_{2} \ldots a_{D}} \cdot E_{b_{1} b_{2} \ldots b_{D}}=\left|\begin{array}{cccc}
{\delta_{b_{1}}^{a_{1}}} & {\delta_{b_{2}}^{a_{1}}} & {\ldots}&{ \delta_{b_{D}}^{a_{1}}} \\
{\delta_{b_{1}}^{a_{2}}} & {\delta_{b_{2}}^{a_{2}}} & {\ldots} & {\delta_{b_{D}}^{a_{2}}} \\
{\ldots} & {\cdots} & {\cdots} & {\ldots} \\
{\delta_{b_{1}}^{a_{D}}} & {\ldots} & {\cdots} & {\delta_{b_{D}}^{a_{D}}}
\end{array}\right|
\end{equation}
\end{qt}
Following the theorem above, we have for the special dimension 3D:
$$
E^{a b c} E_{d e f}=\left|\begin{array}{ccc}
{\delta_{d}^{a}} & {\delta_{e}^{a}} & {\delta_{f}^{a}} \\
{\delta_{d}^{b}} & {\delta_{e}^{b}} & {\delta_{f}^{b}} \\
{\delta_{d}^{c}} & {\delta_{e}^{c}} & {\delta_{f}^{c}}
\end{array}\right|
$$
Now, let's contract the indices $c,f$:
\begin{qt}
\begin{equation}
E^{a b c} E_{d e c}=\left|\begin{array}{ccc}
{\delta_{d}^{a}} & {\delta_{e}^{a}} & {\delta_{c}^{a}} \\
{\delta_{d}^{b}} & {\delta_{e}^{b}} & {\delta_{c}^{b}} \\
{\delta_{d}^{c}} & {\delta_{e}^{c}} & {3}
\end{array}\right|=\delta_{d}^{a} \delta_{e}^{b}-\delta_{e}^{a} \delta_{d}^{b}
\end{equation}
\end{qt}
The last formula is sometimes called magic, as it is an extremely useful tool for the calculations in analytic geometry and vector calculus. Let us proceed and contract indices b and e. We obtain
$$
E^{a b c} E_{d b c}=3 \delta_{d}^{a}-\delta_{d}^{a}=2 \delta_{d}^{a}
$$
Finally, contracting the last remaining couple of indices, we arrive at
$$
E^{a b c} E_{a b c}=6
$$
In general
\begin{qt}
\begin{equation}
E^{a_{1} a_{2} \ldots a_{D}} E_{a_{1} a_{2} \ldots a_{D}}=D !
\end{equation}
Because for the first index, we have $D$ choices, for the second $D-1$ choices, for the third $D-2$ choices, etc. 
\end{qt}
\begin{example}
Using definition of the maximal antisymmetric symbol $E^{a_{1} a_{2} \ldots a_{D}}$ prove the rule for the product of matrix determinants:
$$
\operatorname{det}(A \cdot B)=\operatorname{det} A \cdot \operatorname{det} B
$$
Here both $A=\left\|a_{k}^{i}\right\|$ and $B=\left\|b_{k}^{i}\right\|$ are $n \times n$ matrices.
\end{example}
\textbf{Solution:}
$$
\operatorname{det}(A \cdot B)=E_{i_{1} i_{2} \ldots i_{D}}(A \cdot B)_{1}^{i_{1}}(A \cdot B)_{2}^{i_{2}} \ldots(A \cdot B)_{D}^{i_{D}}
$$
expand, we have
$$
\operatorname{det}(A \cdot B)=E_{i_{1} i_{2} \ldots i_{D}} a_{k_{1}}^{i_{1}} a_{k_{2}}^{i_{2}} \ldots a_{k_{D}}^{i_{D}} b_{1}^{k_{1}} b_{2}^{k_{2}} \ldots b_{D}^{k_{D}}
$$
According to (5.11.8), we have
$$
E_{i_{1} i_{2} \ldots i_{D}} a_{k_{1}}^{i_{1}} a_{k_{2}}^{i_{2}} \ldots a_{k_{D}}^{i_{D}}=E_{k_{1} k_{2} \ldots k_{D}} \cdot \operatorname{det} A
$$
Therefore,
$$
\operatorname{det}(A \cdot B)=\operatorname{det} A E_{k_{1} k_{2} \ldots k_{D}} b_{1}^{k_{1}} b_{2}^{k_{2}} \ldots b_{D}^{k_{D}}
$$
Finally, let's consider two more statements
\begin{lemma}
For any nondegenerate $D \times D$ matrix $M_{b}^{a}$ with the determinant $M,$ the elements of the inverse matrix $\left(M^{-1}\right)_{c}^{b}$ are given by the expressions
\begin{equation}
\left(M^{-1}\right)_{a}^{b}=\frac{1}{M(D-1) !} E_{a a_{2} \ldots a_{D}} E^{b b_{2} \ldots b_{D}} M_{b_{2}}^{a_{2}} M_{b_{3}}^{a_{3}} \ldots M_{b_{D}}^{a_{D}}
\end{equation}
\end{lemma}
Consider the nondegenerate $D \times D$ matrix $\left\|M_{b}^{a}\right\|$ with the elements $M_{b}^{a}=M_{b}^{a}(\kappa)$ being functions of some parameter $\kappa .$In general, the determinant of this matrix $M=\operatorname{det}\left\|M_{b}^{a}\right\|$ also depends on $\kappa .$ Suppose all functions $M_{b}^{a}(\kappa)$ are differentiable. Then the derivative of the determinant equals
\begin{qt}
\begin{equation}
\dot{M}=\frac{d M}{d \kappa}=M\left(M^{-1}\right)_{a}^{b} \frac{d M_{b}^{a}}{d \kappa}
\end{equation}
\end{qt}
where $\left(M^{-1}\right)_{a}^{b}$ are the elements of the matrix imverse to $\left(M_{b}^{a}\right)$ and the dot over $a$ function indicates its derivative with respect to $\kappa$.

\subsection{Applications to Vector Algebra}
Let's come back to the following definition for a \textbf{special object}:
$$
E^{a b c} E_{d e c}=\left|\begin{array}{ccc}
{\delta_{d}^{a}} & {\delta_{e}^{a}} & {\delta_{c}^{a}} \\
{\delta_{d}^{b}} & {\delta_{e}^{b}} & {\delta_{c}^{b}} \\
{\delta_{d}^{c}} & {\delta_{e}^{c}} & {3}
\end{array}\right|=\delta_{d}^{a} \delta_{e}^{b}-\delta_{e}^{a} \delta_{d}^{b}
$$
We first working on a special orthonormal basis $\hat{\mathbf{n}}_{a}=(\hat{\mathbf{i}}, \hat{\mathbf{j}}, \hat{\mathbf{k}})$, corresponding to the Cartesian coordinates $X^a$. In a vector product notations, we have
$$
\begin{aligned}
&\left[\hat{\mathbf{n}}_{1}, \hat{\mathbf{n}}_{2}\right]=\hat{\mathbf{n}}_{1} \times \hat{\mathbf{n}}_{2}=\hat{\mathbf{n}}_{3}\\
&\left[\hat{\mathbf{n}}_{2}, \hat{\mathbf{n}}_{3}\right]=\hat{\mathbf{n}}_{2} \times \hat{\mathbf{n}}_{3}=\hat{\mathbf{n}}_{1}\\
&\left[\hat{\mathbf{n}}_{3}, \hat{\mathbf{n}}_{1}\right]=\hat{\mathbf{n}}_{3} \times \hat{\mathbf{n}}_{1}=\hat{\mathbf{n}}_{2}
\end{aligned}
$$
Using the previously defined $E^{abc}$, we can write this in a more compact way:
\begin{equation}
\left[\hat{\mathbf{n}}_{a}, \hat{\mathbf{n}}_{b}\right]=E_{a b c} \cdot \hat{\mathbf{n}}^{c}
\end{equation}
For orthonormal basis, $\hat{\mathbf{n}}_{a}=\hat{\mathbf{n}}^{a}$ and $E^{a b c}=E_{a b c}$. In general, if we consider two vectors $\mathbf{V}=V^{a} \hat{\mathbf{n}}_{a}$ and $\mathbf{W}=W^{a} \hat{\mathbf{n}}_{a}$, then
\redp{$$
[\mathbf{V}, \mathbf{W}]=E_{a b c} \cdot V^{a} \cdot W^{b} \cdot \hat{\mathbf{n}}^{c}
$$}
Using this object, we can solve many problems of vector algebra:
\begin{example}
Consider the mixed product of the three vectors
$$
(\mathbf{U}, \mathbf{V}, \mathbf{W})=(\mathbf{U},[\mathbf{V}, \mathbf{W}])=U^{a} \cdot[\mathbf{V}, \mathbf{W}]_{a}
$$
$$
=U^{a} \cdot E_{a b c} V^{b} W^{c}=E_{a b c} U^{a} V^{b} W^{c}=\left|\begin{array}{ccc}
{U^{1}} & {U^{2}} & {U^{3}} \\
{V^{1}} & {V^{2}} & {V^{3}} \\
{W^{1}} & {W^{2}} & {W^{3}}
\end{array}\right|
$$
\end{example}
The similar arguments can be made to prove the following properties of the \textbf{mixed product}:
\begin{qt}
(i) Cyclic identity: $(\mathbf{U}, \mathbf{V}, \mathbf{W})=(\mathbf{W}, \mathbf{U}, \mathbf{V})=(\mathbf{V}, \mathbf{W}, \mathbf{U})$

(ii) Antisymmetry: $(\mathbf{U}, \mathbf{V}, \mathbf{W})=-(\mathbf{V}, \mathbf{U}, \mathbf{W})=-(\mathbf{U}, \mathbf{W}, \mathbf{V})$
\end{qt}
\begin{example}
Prove $[\mathbf{U}, \mathbf{V}] \times[\mathbf{W}, \mathbf{Y}]=\mathbf{V} \cdot(\mathbf{U}, \mathbf{W}, \mathbf{Y})-\mathbf{U} \cdot(\mathbf{V}, \mathbf{W}, \mathbf{Y})$
$$
([\mathbf{U}, \mathbf{V}] \times[\mathbf{W}, \mathbf{Y}])_{a}=E_{a b c}[\mathbf{U}, \mathbf{V}]^{b}[\mathbf{W}, \mathbf{Y}]^{c}=E_{a b c} E^{b d e} U_{d} V_{e} E^{c f g} W_{f} Y_{g}
$$
$$
=-E_{b a c} E^{b d e} E^{c f g} U_{d} V_{e} W_{f} Y_{g}=-\left(\delta_{a}^{d} \delta_{c}^{e}-\delta_{a}^{e} \delta_{c}^{d}\right) E^{c f g} U_{d} V_{e} W_{f} Y_{g}
$$
$$
=E^{c f g}\left(U_{c} V_{a} W_{f} Y_{g}-U_{a} V_{c} W_{f} Y_{g}\right)=V_{a}(\mathbf{U}, \mathbf{W}, \mathbf{Y})-U_{a}(\mathbf{V}, \mathbf{W}, \mathbf{Y})
$$
\end{example}
Some more identities
\begin{qt}
$$
[[\mathbf{U}, \mathbf{V}],[\mathbf{W}, \mathbf{Y}]]=\mathbf{W} \cdot(\mathbf{Y}, \mathbf{U}, \mathbf{V})-\mathbf{Y} \cdot(\mathbf{W}, \mathbf{U}, \mathbf{V})
$$
$$
\mathbf{V}(\mathbf{W}, \mathbf{Y}, \mathbf{U})-\mathbf{U}(\mathbf{W}, \mathbf{Y}, \mathbf{V})=\mathbf{W}(\mathbf{U}, \mathbf{V}, \mathbf{Y})-\mathbf{Y}(\mathbf{U}, \mathbf{V}, \mathbf{W})
$$
$$
[\mathbf{U} \times \mathbf{V}] \cdot[\mathbf{W} \times \mathbf{Y}]=(\mathbf{U} \cdot \mathbf{W})(\mathbf{V} \cdot \mathbf{Y})-(\mathbf{U} \cdot \mathbf{Y})(\mathbf{V} \cdot \mathbf{W})
$$
$$
[\mathbf{A},[\mathbf{B}, \mathbf{C}]]=\mathbf{B}(\mathbf{A}, \mathbf{C})-\mathbf{C}(\mathbf{A}, \mathbf{B})
$$
$$
[\mathbf{U},[\mathbf{V}, \mathbf{W}]]+[\mathbf{W},[\mathbf{U}, \mathbf{V}]]+[\mathbf{V},[\mathbf{W}, \mathbf{U}]]=0
$$
\end{qt}
Now, let's construct a tensor version of the object $E_{abc}$. To do so, one has to use the \textbf{transformation rule for the covariant tensor of the third rank,starting from the special Cartesian coordinates $X^a$}:
\begin{equation}
\varepsilon_{i j k}=\frac{\partial X^{a}}{\partial x^{i}} \frac{\partial X^{b}}{\partial x^{j}} \frac{\partial X^{c}}{\partial x^{k}} E_{a b c}
\end{equation}
\begin{qt}
The component $\varepsilon_{123}$ is a square root of the metric determinant,
\begin{equation}
    \varepsilon_{123}=g^{1 / 2}, \quad \text { where } g=\operatorname{det}\left\|g_{i j}\right\|
    \label{eps123-!}
\end{equation}
or
\begin{equation}
    \varepsilon_{123}=\operatorname{det}\left\|\frac{\partial X^{a}}{\partial x^{i}}\right\|
    \label{eps123}
\end{equation}
while we can use (\ref{eps123}) without much thought, we need to be careful using (\ref{eps123-!}). The reason is that \redp{the coordinate transformations that break parity can change the sign in the r.h.s. in Eq.(\ref{eps123-!})}
\end{qt}
Despite $E_{i j k}$ is not being a tensor, we can easily control its transformation from one coordinate system to another:
$$
E_{i j k}=\frac{1}{\sqrt{g}} \varepsilon_{i j k}
$$
$E_{i j k}$ is a particular example of objects which are called \textbf{tensor densities}.
\begin{defi}
        The quantity $A_{i_{1} \ldots i_{n}}{}^{j_{1} \ldots j_{n}}$ is a tensor density of the $(m, n)$ -type with the weight $r,$ if the quantity
        $$
g^{-r / 2} \cdot A_{i_{1} \ldots i_{n}}{}^{j_{1} \ldots j_{m}}
$$
is a tensor.
\end{defi}
In general,
\begin{equation}
\varepsilon_{i_{1} i_{2} \cdots i_{D}}=\frac{\partial x^{a_{1}}}{\partial x^{i_{1}}} \frac{\partial x^{a_{2}}}{\partial x^{i_{2}}} \cdots \frac{\partial x^{a_{D}}}{\partial x^{i_{D}}} E_{a_{1} a_{2} \cdots a_{D}}
\end{equation}
Since
\begin{equation}
g_{i j}=\frac{\partial x^{a}}{\partial x^{i}} \frac{\partial x^{b}}{\partial x^{j}} \delta_{b}^{a}
\end{equation}
assuming that the orientation of axes is such that $\operatorname{det}\left(\frac{\partial x^{a}}{\partial x^{i}}\right)>0,$ we get
\begin{equation}
\operatorname{det}\left\|\frac{\partial x^{a}}{\partial x^{i}}\right\|=\sqrt{g}
\end{equation}
Then
\begin{qt}
\begin{equation}
\varepsilon_{123 \dots D}=\frac{\partial x^{a_{1}}}{\partial x^{1}} \frac{\partial x^{a_{2}}}{\partial x^{2}} \cdots \frac{\partial x^{a_{D}}}{\partial x^{D}} E_{a_{1} a_{2} \cdots a_{D}}=\operatorname{det}\left(\frac{\partial x^{a}}{\partial x^{i}}\right)=\sqrt{g}
\end{equation}
Similarly, $\varepsilon^{12 \cdots D}=\frac{1}{\sqrt{g}}$.
\end{qt}

\section{Curvilinear Coordinates, Local Coordinate Transformations}
So far, we have learned the following transformation rule for scalar field, vector, and tensor:
$$
\varphi^{\prime}\left(x^{\prime}\right)=\varphi(x)
$$
$$
a^{\prime i}\left(x^{\prime}\right)=\frac{\partial x^{i}}{\partial x^{j}} a^{j}(x)
$$
$$
b_{l}^{\prime}\left(x^{\prime}\right)=\frac{\partial x^{k}}{\partial x^{\prime l}} b_{k}(x)
$$
$$
A_{j^{\prime}}^{i^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{k}} \frac{\partial x^{l}}{\partial x^{\prime j}} A_{l}^{k}(x)
$$
and for metric tensor:
$$
g_{i j}(x)=\frac{\partial X^{a}}{\partial x^{i}} \frac{\partial X^{b}}{\partial x^{j}} g_{a b}
$$
$$
g^{i j}=\frac{\partial x^{i}}{\partial X^{a}} \frac{\partial x^{j}}{\partial X^{b}} \delta^{a b}
$$
where $g_{a b}=\delta_{a b}$ is a metric in Cartesian coordinates.

It is so easy to generalize the notion of tensor and algebraic operations over tensors, \textbf{because all these operations are defined in the same point of the space.} Thus, the main difference between general coordinate transformation $x^{\prime \alpha}=x^{\prime \alpha}(x)$ and the special one $x^{\prime \alpha}=\wedge_{\beta}^{\alpha^{\prime}} x^{\beta}+B^{\alpha^{\prime}}$ with $\wedge_{\beta}^{\alpha^{\prime}}=$ const and $B^{\alpha^{\prime}}=$ const is that \textbf{in the general case, the transition coefficients $\partial x^{i} / \partial x^{\prime j}$ are not necessary constants.}

One of important consequences is that \bluep{the metric tensor $g_{i j}$ also depends on the point. Additionally, the antisymmetric tensor $\varepsilon^{i j k}$ also depends on the coordinates}.