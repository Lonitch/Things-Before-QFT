\chapter{Tensor Calculus}

\section{Total and Partial Derivatives}
To understand the different kinds of derivatives, let's say we have a function $\rho(t, x(t), p(t))$ which, in general, depends on the location $x(t)$ and momentum $p(t)$ plus the time $t.$ A key observation is that the location $x(t)$ and momentum $p(t)$ are functions of $t$ too. Therefore, we need to be extremely careful what we mean when we calculate the derivative with respect to the time $t .$
$$
\frac{d \rho}{d t}=\lim _{\Delta t \rightarrow 0} \frac{\rho(t+\Delta t, x(t+\Delta t), p(t+\Delta t))-\rho(t, x(t), p(t))}{\Delta t}
$$
\textbf{The result is the total rate of change of $\rho$}.
$$
\frac{\partial \rho}{\partial t}=\lim _{\Delta t \rightarrow 0} \frac{\rho(t+\Delta t, x(t), p(t))-\rho(t, x(t), p(t))}{\Delta t}
$$
The key difference is that we only vary $t$ if it appears explicitly
in $\rho$ but not if it only appears implicitly because $x(t)$ and $p(t)$
also depend on $t .$ Thus
$$
\frac{d \rho}{d t}=\frac{\partial \rho}{\partial x} \frac{d x}{d t}+\frac{\partial \rho}{\partial p} \frac{d p}{d t}+\frac{\partial \rho}{\partial t}
$$
\section{Taylor Expansion}
In general, we want to estimate the value of some function $f(x)$ at some value of $x$ by using our knowledge of the function's value at some fixed point $a .$ The Taylor series then reads
\begin{equation}
\begin{aligned}
f(x)=& \sum_{n=0}^{\infty} \frac{f^{(n)}(a)(x-a)^{n}}{n !} \\
=& \frac{f^{(0)}(a)(x-a)^{0}}{0 !}+\frac{f^{(1)}(a)(x-a)^{1}}{1 !}+\frac{f^{(2)}(a)(x-a)^{2}}{2 !} \\
&+\frac{f^{(3)}(a)(x-a)^{3}}{3 !}+\ldots
\end{aligned}
\end{equation}
or
\begin{equation}
f(x+a)=f(x)+(a \cdot \partial) f(x)+\frac{1}{2}(a \cdot \partial)^{2} f(x)+\cdots
\end{equation}
Taylor expansion of a scalar field (function $f$ that maps $\mathbb{R}^{n}$ to $\mathbb{R}$).Now, identify $\partial f / \partial t$ as $\hat{\boldsymbol{n}} \cdot \nabla f .$ In addition, see that $t \hat{\boldsymbol{n}}=\boldsymbol{x}-\boldsymbol{x}_{0} .$ Some clever recombining of terms gives
\begin{equation}
f(x)=f\left(x_{0}\right)+\left.\left(x-x_{0}\right) \cdot \nabla f\right|_{x_{0}}+\left.\frac{1}{2}\left(\left[x-x_{0}\right] \cdot \nabla\right)^{2} f\right|_{x_{0}}+\ldots
\end{equation}
and
\begin{equation}
\hat{\boldsymbol{n}} \cdot \nabla=\partial_{t}
\end{equation}

\section{Vector Identities}
\begin{equation}
\begin{aligned}
&\vec{\nabla} \cdot(\vec{\nabla} \times \vec{A}) \equiv \operatorname{div}(\operatorname{rot} \vec{A})=(\vec{\nabla} \times \vec{\nabla}) \cdot \vec{A} \equiv 0\\
&\vec{\nabla} \times(\vec{\nabla} \varphi) \equiv \operatorname{rot} \operatorname{grad} \varphi=(\vec{\nabla} \times \vec{\nabla}) \varphi \equiv 0
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
&\vec{\nabla} \cdot(\vec{A} \varphi)=\varphi \vec{\nabla} \cdot \vec{A}+\vec{A} \cdot \vec{\nabla} \varphi \quad \Longleftrightarrow \quad \operatorname{div}(\vec{A} \varphi)=\varphi \operatorname{div} \vec{A}+\vec{A} \cdot \operatorname{grad} \varphi\\
&\vec{\nabla} \times(\vec{A} \varphi)=\varphi \vec{\nabla} \times \vec{A}-\vec{A} \times \vec{\nabla} \varphi \quad \Longleftrightarrow \quad \operatorname{rot}(\vec{A} \varphi)=\varphi \operatorname{rot} \vec{A}-\vec{A} \times \operatorname{grad} \varphi\\
&\vec{\nabla} \cdot(\vec{A} \times \vec{B})=\vec{B} \cdot(\vec{\nabla} \times \vec{A})-\vec{A} \cdot(\vec{\nabla} \times \vec{B}) \quad \Longleftrightarrow \quad \operatorname{div}(\vec{A} \times \vec{B})=\vec{B} \cdot \operatorname{rot} \vec{A}-\vec{A} \cdot \operatorname{rot} \vec{B}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\vec{\nabla} \times(\vec{A} \times \vec{B}) &=(\vec{B} \cdot \vec{\nabla}) \vec{A}-(\vec{A} \cdot \vec{\nabla}) \vec{B}+\vec{A}(\vec{\nabla} \cdot \vec{B})-\vec{B}(\vec{\nabla} \cdot \vec{A}) \\
& \Longleftrightarrow \operatorname{rot}(\vec{A} \times \vec{B})=(\vec{B} \operatorname{grad}) \vec{A}-(\vec{A} \operatorname{grad}) \vec{B}+\vec{A}(\operatorname{div} \vec{B})-\vec{B}(\operatorname{div} \vec{A})
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\vec{\nabla}(\vec{A} \cdot \vec{B})=&(\vec{B} \cdot \vec{\nabla}) \vec{A}+(\vec{A} \cdot \vec{\nabla}) \vec{B}+\vec{A} \times(\vec{\nabla} \times \vec{B})+\vec{B} \times(\vec{\nabla} \times \vec{A}) \\
& \Longleftrightarrow \operatorname{grad}(\vec{A} \cdot \vec{B})=(\vec{B} \cdot \operatorname{grad}) \vec{A}+(\vec{A} \cdot \operatorname{grad}) \vec{B}+\vec{A} \times \operatorname{rot} \vec{B}+\vec{B} \times \operatorname{rot} \vec{A}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
&\vec{\nabla} \cdot(\vec{\nabla} \varphi) \equiv \operatorname{div}(\operatorname{grad} \varphi) \equiv \Delta \varphi=\frac{\partial^{2} \varphi}{\partial x^{2}}+\frac{\partial^{2} \varphi}{\partial y^{2}}+\frac{\partial^{2} \varphi}{\partial z^{2}}, \quad \Delta=\text { Laplace Operator }\\
&\vec{\nabla} \times(\vec{\nabla} \times \vec{A}) \equiv \operatorname{rot}(\operatorname{rot} \vec{A})=\vec{\nabla}(\vec{\nabla} \cdot \vec{A})-(\vec{\nabla} \cdot \vec{\nabla}) \vec{A} \equiv \operatorname{grad} \operatorname{div} \vec{A}-\Delta \vec{A}
\end{aligned}
\end{equation}

\section{Linear Spaces, Vectors, and Tensors}
The linear space, say $L$, consists of the elements (vectors) that \textbf{permit linear operations with the properties described below}:
\begin{itemize}
    \item Summing up the vectors
    \item Multiplication by a number
\end{itemize}
\begin{qt}
    Consider the set of vectors, elements of the linear space L
    $$
\left\{\mathbf{a}_{1}, \mathbf{a}_{2}, \ldots, \mathbf{a}_{n}\right\}=\left\{\mathbf{a}_{i} | i=1, \ldots, n\right\}
$$
and the set of real numbers $k_{1}, k_{2}, \ldots k_{n} .$ Vector
$$
\mathbf{k}=\sum_{i=1}^{n} k^{i} \mathbf{a}_{i}=\mathbf{k}^{i} \mathbf{a}_{i}=0\Longrightarrow \sum_{i=1}^{n}\left(k^{i}\right)^{2}=0
$$
then vectors $\left\{\mathbf{a}_{\mathbf{i}}, i=1, \ldots, n\right\}$ are called \textbf{linearly independent}. The last condition means that no one of the coefficients $k_{i}$ can be different from zero.
\end{qt}
For example, in 2D, two vectors are linearly dependent if and only if they are parallel; in 3D, three vectors are linearly dependent if and only if they belong to the same plane, etc.
\begin{defi}
        The maximal number of linearly independent elements of the linear space $L$ is called its dimension. It proves useful to denote $L_{D}$ a linear space of dimension $D .$
\end{defi}
\begin{thm}
Consider a $D$-dimensional linear space $L_{D}$ and the set of linearly independent vectors $\mathbf{e}_{i}=\left(\mathbf{e}_{1}, \mathbf{e}_{2}, \ldots, \mathbf{e}_{D}\right) .$ Then, for any vector a one can write
\begin{equation}
    \mathbf{a}=\sum_{i=1}^{D} a^{i} \mathbf{e}_{i}=a^{i} \mathbf{e}_{i}
    \label{vector-basis}
\end{equation}
where the coefficients $a^{i}$ are defined in a unique way.
\end{thm}
\bluep{The coefficients $a^{i}$ are called components or \textbf{contravariant components of the vector $\mathbf{a}$}}.The word “contravariant” here means that the components $a^i$ have upper indices.

\section{Direct product of the two linear spaces}
Let's consider the example of phase space in the classic mechanics. The coordinate system in the linear space $L_{D}$ consists of the initial point $O$ and the basis $\mathbf{e}_{i} .$ The position of a point $P$ can be characterized by \textbf{its position vector or radius vector $\mathbf{r}=\overrightarrow{O P}=x^{i} \mathbf{e}_{i}$}.

If the phase space is describing one particle, the space is composed of the radius vectors $\mathbf{r}$ and velocities $\mathbf{v}$ of the particle:
$$
\mathbf{r}=x^{i} \mathbf{e}_{i}, \quad \mathbf{v}=v^{j} \mathbf{f}_{j}
$$
The two set of bases can be related and independent. This example is a particular case of the linear space which is called \redp{a direct product
of the two linear spaces}. The element of the direct product of the two linear spaces $L_{D_{1}}$ and $L_{D_{2}}$ (\textbf{they can have different dimensions}) is the ordered set of the elements of each of the two spaces $L_{D_{1}}$ and $L_{D_{2}} .$
\begin{qt}
    The notation for the basis in the case of configuration space is $\mathbf{e}_{i} \otimes \mathbf{f}_{j}$. Hence, the state of the point-like particle in the phase space is characterized by the element of this linear space, which can be presented as:
    $$
x^{i} v^{j} \mathbf{e}_{i} \otimes \mathbf{f}_{j}
$$
In general, one can define the space that is a direct product of several linear spaces with different individual basis sets $\mathbf{e}_{i}$ each. In this case, we will have $\mathbf{e}_{i}^{(1)}, \mathbf{e}_{i}^{(2)}, \ldots$ $\mathbf{e}_{i}^{(N)} .$ The basis in the direct product space will be
$$
\mathbf{e}_{i_{1}}^{(1)} \otimes \mathbf{e}_{i_{2}}^{(2)} \otimes \ldots \otimes \mathbf{e}_{i_{N}}^{(N)}
$$
and the element becomes
$$
T^{i_{1} i_{2} \ldots i_{N}} \mathbf{e}_{i_{1}}^{(1)} \otimes \mathbf{e}_{i_{2}}^{(2)} \otimes \ldots \otimes \mathbf{e}_{i_{N}}^{(N)}
$$
\end{qt}
\section{Vector basis and its transformation}
Let us start from the components of the vector, which were defined in (\ref{vector-basis}). Consider, along with the original basis $\mathbf{e}_{i},$ another basis $\mathbf{e}_{i}^{\prime} .$ since each vector of the new basis belongs to the same space, it can be expanded using the original basis as
\begin{equation}
\mathbf{e}_{i}^{\prime}=\wedge_{i^{\prime}}^{j} \mathbf{e}_{j}
\label{basis-transform}
\end{equation}
and
$$
\mathbf{a}=a^{i} \mathbf{e}_{i}=a^{j^{\prime}} \mathbf{e}_{j^{\prime}}=a^{j^{\prime}} \wedge_{j^{\prime}}^{i} \mathbf{e}_{i}
$$
\begin{qt}
\begin{equation}
    a^{i}=a^{j^{\prime}} \wedge_{j^{\prime}}^{i}
    \label{cotravariant-coord-transform}
\end{equation}
\end{qt}

Similarly, we can make inverse transformation:
$$
a^{k^{\prime}}=\left(\wedge^{-1}\right)_{l}^{k^{\prime}} a^{l}
$$
and
\[
\left(\wedge^{-1}\right)_{l}^{k^{\prime}} \cdot \wedge_{i^{\prime}}^{l}=\delta_{i^{\prime}}^{k^{\prime}} \quad \text { and } \quad \wedge_{i^{\prime}}^{l} \cdot\left(\wedge^{-1}\right)_{k}^{i^{\prime}}=\delta_{k}^{l}
\]
\begin{qt}
Taking the partial derivatives, we arrive at the relations
\[
\wedge_{j^{\prime}}^{i}=\frac{\partial x^{i}}{\partial x^{j^{\prime}}} \quad \text { and } \quad\left(\wedge^{-1}\right)_{l}^{k^{\prime}}=\frac{\partial x^{k^{\prime}}}{\partial x^{l}}
\]
and
$$
\left(\wedge^{-1}\right)_{l}^{k^{\prime}} \cdot \wedge_{k^{\prime}}^{i}=\frac{\partial x^{k^{\prime}}}{\partial x^{l}} \frac{\partial x^{i}}{\partial x^{k^{\prime}}}=\frac{\partial x^{i}}{\partial x^{l}}=\delta_{l}^{i}
$$
is nothing but the chain rule for partial derivatives.
\end{qt}

\section{Scalar, vector, and tensor fields}
\begin{defi}
        Function $\varphi(x)$ is called scalar field or simply scalar if it does not transform under the change of coordinates
        \begin{equation}
\varphi(x)=\varphi^{\prime}\left(x^{\prime}\right)
\end{equation}
\end{defi}
Let us give a clarifying example in 1D. Consider a function
$$
y=x^2
$$
Now, let us change the variables 
$$
x^{\prime}=x+1
$$
The function $y=\left(x^{\prime}\right)^{2},$ obviously, represents another parabola. \bluep{In order to preserve the plot intact, we need to modify the form of the function, that is, to go from $\varphi$ to $\varphi^{\prime}$.} The new function $y^{\prime}=\left(x^{\prime}-1\right)^{2}$ will represent the original parabola,because the change of the variable is completely compensated by the change of the form of the function.
\begin{mybox}
\begin{center}
    Discuss whether the three numbers temperature $T(x)$, pressure $p(x)$ and density $\rho(x)$ form a contravariant vector
\end{center}
\end{mybox}
\begin{mybox2}
We can only form contravariant vector if the numbers transform by following the rule (\ref{cotravariant-coord-transform}). Since these numbers are scalar fields, it transforms like $T(\mathbf{r})=T^{\prime}(\mathbf{r}^{\prime})$. Thus, these three parameters can not form a contravariant vector.
\end{mybox2}
\begin{example}
From the definition above we know $\varphi^{\prime}(x) \neq \varphi(x)$ and $\varphi\left(x^{\prime}\right) \neq \varphi(x)$. Let us calculate these quantities explicitly for the special case of infinitesimal transformation $x^{\prime i}=x^{i}+\xi^{i}$ where $\xi$ are constant coefficients. Now we have
$$
\varphi\left(x^{\prime}\right)=\varphi\left(x+\xi\right)\overset{Taylor}{=}\varphi(x)+\frac{\partial \varphi}{\partial x^{i}} \xi^{i}
$$
and,
$$
\varphi^{\prime}\left(x^{i}\right)=\varphi^{\prime}\left(x^{i}-\xi^{i}\right)=\varphi^{\prime}\left(x^{\prime}\right)-\frac{\partial \varphi^{\prime}}{\partial x^{i^{\prime}}} \cdot \xi^{i}
$$
rewrite the two equations above, we find:
$$
\frac{\partial \varphi^{\prime}}{\partial x^{\prime i}}=\frac{\varphi^{\prime}\left(x^{\prime i}\right)-\varphi^{\prime}\left(x^{i}-\xi^{i}\right)}{\xi^i}
$$
$$
\frac{\partial \varphi}{\partial x^{i}}=\frac{\varphi(x^{\prime i})-\varphi(x^i)}{\xi^i}
$$
At the limit of $\xi\rightarrow0$, we have
$$
\frac{\partial \varphi^{\prime}}{\partial x^{\prime i}}=\frac{\partial \varphi(x)}{\partial x^{i}}+\mathcal{O}(\xi)
$$
As mentioned above, 
$$
\varphi^{\prime}\left(x^{i}\right)=\varphi^{\prime}\left(x^{\prime}\right)-\frac{\partial \varphi^{\prime}}{\partial x^{\prime i}} \cdot \xi^{i}\\
=\varphi^{\prime}\left(x^{\prime}\right)-\frac{\partial \varphi(x)}{\partial x^{i}}\cdot\xi^i
$$
Since $\varphi(x)=\varphi^{\prime}\left(x^{\prime}\right)$, we have
$$
\varphi^{\prime}(x)=\varphi(x)-\xi^{i} \partial_{i} \varphi
$$
\textbf{where we have introduced a useful notation $\partial_{i}=\partial / \partial x^{i}$.}
\end{example}
\textbf{For the vector field, we have the following rule for the coordinate transformation}. The set of three functions $\left\{a^{i}(x)\right\}=\left\{a^{1}(x), a^{2}(x), a^{3}(x)\right\}$ forms contravariant vector field (or simply vector field) if they transform, under the change of coordinates $\left\{x^{i}\right\} \rightarrow\left\{x^{\prime} j\right\},$ as
\begin{qt}
\begin{equation}
a^{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}}}{\partial x^{i}} \cdot a^{i}(x)
\end{equation}
\end{qt}
\bluep{The components of the vector in a given geometrical point of space modify under the coordinate transformation, while the scalar field does not.}The scalar and vector fields can be considered as examples of the more general objects called tensors. Tensors are also defined through their transformation rules.
\begin{qt}
The set of $3^n$ functions $\left\{a^{i_{1} \ldots i_{n}}(x)\right\}$ is called a contravariant tensor of rank $n,$ if these functions transform, under $x^{i} \rightarrow x^{i},$ as
\begin{equation}
a^{i_{1}^{i} \ldots i_{n}^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i_{1}^{\prime}}}{\partial x^{j_{1}}} \ldots \frac{\partial x^{i_{n}^{\prime}}}{\partial x^{j_{n}}} a^{j_{1} \ldots j_{n}}(x)
\end{equation}
\end{qt}

\section{Orthonormal Basis and Cartesian Coordiantes}
The scalar product of two vectors $\mathbf{a}$ and $\mathbf{b}$ in 3D is defined in a ususal way,
\begin{equation}
(\mathbf{a}, \mathbf{b})=\mathbf{a}\cdot\mathbf{b}=|\mathbf{a}| \cdot|\mathbf{b}| \cdot \cos \theta
\end{equation}
Special orthonormal basis $\left\{\hat{\mathbf{n}}_{a}\right\}$ is the one with
\begin{equation}
\left(\hat{\mathbf{n}}_{a}, \hat{\mathbf{n}}_{b}\right)=\delta_{a b}=\left\{\begin{array}{l}
{1 \text { if } a=b} \\
{0 \text { if } a \neq b}
\end{array}\right.
\end{equation}
\begin{example}
Making transformations of the basis vectors, verify that the change of coordinates
$$
x^{\prime}=\frac{x+y}{\sqrt{2}}+3, \quad y^{\prime}=\frac{x-y}{\sqrt{2}}-5
$$
does not modify the type of coordinates $x^{\prime}, y^{\prime},$ which remains Cartesian.
\textbf{Solution}:
$$
x^{\prime}=\frac{x+y}{\sqrt{2}}+3, \quad y^{\prime}=\frac{x-y}{\sqrt{2}}-5
$$
The change of initial point $(0,0) \rightarrow(3,-5)$ does not have relation to the change of basis. Then
$$
\begin{aligned}
x^{\prime} \hat{i}^{\prime}+y^{\prime} \hat{j}^{\prime} &=\left(\frac{x+y}{\sqrt{2}}\right) \hat{i}^{\prime}+\left(\frac{x-y}{\sqrt{2}}\right) \hat{j}^{\prime} \\
&=\frac{x}{\sqrt{2}}\left(\hat{i}^{\prime}+\hat{j}^{\prime}\right)+\frac{y}{\sqrt{2}}\left(\hat{i}^{\prime}-\hat{j}^{\prime}\right)=x \hat{i}+y \hat{j}
\end{aligned}
$$
Thus, $\hat{i}=\frac{1}{\sqrt{2}}\left(\hat{i}^{\prime}+\hat{j}^{\prime}\right), \hat{j}=\frac{1}{\sqrt{2}}\left(\hat{i}^{\prime}-\hat{j}^{\prime}\right) .$ Obviously, $\hat{i}^{2}=\hat{j}^{2}=1$ and $\hat{i} \cdot \hat{j}=0$
\end{example}
Now we can introduce a conjugated covariant basis.
\begin{qt}
Consider basis $\left\{\mathbf{e}_{i}\right\} .$ The conjugated basis is defined as a set of vectors $\left\{\mathbf{e}^{j}\right\}$ which satisfy the relations
\begin{equation}
\mathbf{e}_{i} \cdot \mathbf{e}^{j}=\delta_{i}^{j}
\end{equation}
The special property of the orthonormal basis is that $\hat{\mathbf{n}}^{a}=\hat{\mathbf{n}}_{a}$.

Any vector a can be expanded using the conjugated basis $\mathbf{a}=a_{i} \mathbf{e}^{i} .$ \redp{The coefficients $a_{i}$ are called covariant components of the vector $\mathbf{a}$}.
\end{qt}
In the case of covariant vector components, the transformation is done by means of the matrix inverse to the one for the contravariant components.
\begin{thm}
If we change the basis of the coordinate system from $\mathbf{e}_{i}$ to $\mathbf{e}_{i}^{\prime},$ then the covariant components of the vector a transform as
\begin{equation}
a_{i}^{\prime}=\frac{\partial x^{j}}{\partial x^{i}} a_{j}
\end{equation}
\end{thm}

 The set of three functions $\left\{A_{i}(x)\right\}$ forms a covariant vector field, if they transform from one coordinate system to another one as
 \begin{equation}
A_{i}^{\prime}\left(x^{\prime}\right)=\frac{\partial x^{j}}{\partial x^{i}} A_{j}(x)
\end{equation}
The set of $3^{n}$ functions $\left\{A_{i_{1} i_{2} \ldots i_{n}}(x)\right\}$ form a covariant tensor of rank $n$ if they transform from one coordinate system to another as
\begin{equation}
A_{i_{1} i_{2} \ldots i_{n}}^{\prime}\left(x^{\prime}\right)=\frac{\partial x^{j_{1}}}{\partial x^{i_{1}}} \frac{\partial x^{j_{2}}}{\partial x^{i_{2}}} \cdots \frac{\partial x^{j_{n}}}{\partial x^{n_{n}}} A_{j_{1} j_{2} \ldots j_{n}}(x)
\end{equation}
In general
\begin{qt}
The set of $3^{n+m}$ functions $\left\{B_{i_{1} \ldots i_{n}} j_{1 \ldots j_{n}}(x)\right\}$ forms the tensor of the type $(m, n),$ if these functions transform, under the change of coordinate basis, as
\begin{equation}
B_{i_{1}^{\prime} \ldots i_{n}^{\prime}} ^{j_{1}^{\prime} \ldots j_{m}^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}_1}}{\partial x^{l_{1}}} \cdots \frac{\partial x^{j_{m}^{\prime}}}{\partial x^{l_{m}}} \frac{\partial x^{k_{1}}}{\partial x^{\prime i_{1}}} \cdots \frac{\partial x^{k_{n}}}{\partial x^{\prime i_{n}}} B_{k_{1} \ldots k_{n}}^{l_{1} \ldots l_{m}}(x)
\end{equation}
Other possible names are the mixed tensor of covariant rank $n$ and contravariant rank $m,$ or simply $(m, n)$ -tensor.
\end{qt}
\textbf{Tensors are important due to the fact that they offer the coordinate-independent description of geometrical and physical laws.} The following example shows this observation:
\begin{example}
For an arbitrary $\mathbf{a}(x)=a^{i}(x) \mathbf{e}_{i}$ we have
$$
a^{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}}}{\partial x^{i}} a^{i}(x), \quad \mathbf{e}_{j^{\prime}}\left(x^{\prime}\right)=\mathbf{e}_{k}(x) \frac{\partial x^{k}}{\partial x^{j^{\prime}}}
$$
Then
$$
a^{j^{\prime}}\left(x^{\prime}\right) \mathbf{e}_{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}}}{\partial x^{i}} a^{i}(x) \mathbf{e}_{k}(x) \frac{\partial x^{k}}{\partial x^{j^{\prime}}}=\delta_{l}^{k} a^{i}(x) \mathbf{e}_{k}(x)=a^{i}(x) \mathbf{e}_{i}(x)
$$
\end{example}

If the Kronecker symbol transforms as a mixed $(1,1)$ tensor,
\begin{qt}
\begin{equation}
\delta_{j^{\prime}}^{i^{\prime}}=\frac{\partial x^{i^{\prime}}}{\partial x^{k}} \frac{\partial x^{l}}{\partial x^{j^{\prime}}} \delta_{l}^{k}=\frac{\partial x^{i^{\prime}}}{\partial x^{j^{\prime}}}
\end{equation}
\end{qt}
then in any coordinates $x^i$ it has the same form 
$$
\delta_{j}^{i}=\left\{\begin{array}{l}
{1 \text { if } i=j} \\
{0 \text { if } i \neq j}
\end{array}\right.
$$
\textbf{This property is very important, as it enables us to use the Kronecker symbol in any coordinates}

\begin{example}
Show that the product $A^{i}(x) B_{j}(x)$ of covariant and contravariant vectors transforms as a $(1,1)$ -type mixed tensor.
$$
A^{i^{\prime}}\left(x^{\prime}\right) A_{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{k}} A^{k}(x) \frac{\partial x^{l}}{\partial x^{j^{\prime}}} B_{l}(x)=\frac{\partial x^{i^{\prime}}}{\partial x^{k}} \frac{\partial x^{l}}{\partial x^{j^{\prime}}} A^{k}(x) B_{l}(x)
$$
\end{example}

\section{Orthogonal transformation}
The rotation transformation around $\hat{\mathbf{z}}-$axis is given by the following relation:
\begin{qt}
\begin{equation}
\left(\begin{array}{l}
{x} \\
{y} \\
{z}
\end{array}\right)=\hat{\wedge}_{z}\left(\begin{array}{l}
{x^{\prime}} \\
{y^{\prime}} \\
{z^{\prime}}
\end{array}\right), \quad \text { where } \quad \hat{\wedge}_{z}=\hat{\wedge}_{z}(\alpha)=\left(\begin{array}{ccc}
{\cos \alpha} & {-\sin \alpha} & {0} \\
{\sin \alpha} & {\cos \alpha} & {0} \\
{0} & {0} & {1}
\end{array}\right)
\end{equation}
the matrix above has the following property:
\begin{equation}
\hat{\wedge}_{z}^{T}=\hat{\wedge}_{z}^{-1}
\end{equation}
\end{qt}
\begin{defi}
        The matrix $\hat{\wedge}_{z}$ which satisfies $\hat{\wedge}_{z}^{-1}=\hat{\wedge}_{z}^{T}$ and the corresponding coordinate transformation is called \textbf{orthogonal}.
\end{defi}
Similarly, we can write the rotation matrix around other axis:
\begin{equation}
\hat{\wedge}_{x}(\gamma)=\left(\begin{array}{ccc}
{1} & {0} & {0}\\
{0} & {\cos \gamma} & {-\sin \gamma} \\
{0} & {\sin \gamma} & {\cos \gamma}
\end{array}\right)
\end{equation}
\begin{equation}
\hat{\wedge}_{y}(\beta)=\left(\begin{array}{ccc}
{\cos \beta} & {0} & {-\sin \beta} \\
{0} & {1} & {0}\\
{\sin \beta} & {0} &  {\cos \beta}
\end{array}\right)
\end{equation}
In 3D space, any rotation of the rigid body may be represented as a combination of the rotation around the axes $\hat{\mathbf{z}}, \hat{\mathbf{y}},$ and $\hat{\mathbf{x}}$ to the angles $ \alpha, \beta,$ and $\gamma$:
$$
\hat{\wedge}=\hat{\wedge}_{z}(\alpha) \hat{\wedge}_{y}(\beta) \hat{\wedge}_{x}(\gamma)
$$
Since $(A \cdot B)^{T}=B^{T} \cdot A^{T} \quad \text { and } \quad(A \cdot B)^{-1}=B^{-1} \cdot A^{-1}$, we can easily obtain that \bluep{the general 3D rotation matrix satisfies the orthogonal relation.}
\begin{qt}
For the orthogonal matrix, one can take the determinant and arrive at $det\hat{\wedge}=det\hat{\wedge}^{-1}$. Therefore, 
$$
det\hat{\wedge}=\pm1
$$
As far as any rotation matrix has a determinant equal to one, there must be some other orthogonal matrices with the determinant equal to −1.
\end{qt}
In case where the matrix elements are allowed to be complex, the matrix that satisfies the property $U^{\dagger}=U^{-1}$ is called unitary. The operation $U^{\dagger}$ is called \textbf{Hermitian conjugation} and consists of complex conjugation plus transposition $U^{\dagger}=\left(U^{*}\right)^{T}$.
\begin{example}
Consider the transformation from one orthonormal basis $\hat{\mathbf{n}}_{i}$ to another such basis $\hat{\mathbf{n}}_{k}^{\prime},$ namely, $\hat{\mathbf{n}}_{k}^{\prime}=R_{k}^{i} \hat{\mathbf{n}}_{i} .$ Prove that the matrix $\left\|R_{k}^{i}\right\|$ is orthogonal.

 $\hat{\mathbf{n}}_{k}^{\prime}=R_{k}^{i} \hat{\mathbf{n}}_{i}$ and also $\hat{\mathbf{n}}^{\prime \prime}=\left(R^{-1}\right)_{j}^{l} \hat{\mathbf{n}}^{j} .$ since $\hat{\mathbf{n}}_{k}^{\prime}=\hat{\mathbf{n}}^{k^{\prime}}$ and $\hat{\mathbf{n}}_{i}=\hat{\mathbf{n}}^{i}$
also have $\left(R^{-1}\right)_{j}^{l}=\left(R^{T}\right)_{j}^{l}$
\end{example}

\section{Operations over Tensors, Metric Tensor}
\textbf{Multiplication of a tensor by a number produces a tensor of the same type.} This operation is equivalent to the multiplication of all tensor components to the same number $\alpha$, namely:
\begin{equation}
(\alpha A)_{i_{1} \ldots i_{n}}=\alpha \cdot A_{i_{1} \ldots i_{n}}^{j_{1} \ldots j_{m}}
\end{equation}
\textbf{Multiplication of two tensors is defined for a couple of tensors of any type.} The product of a (m, n)-tensor and a (t, s)-tensor results in the (m + t, n + s)- tensor, e.g.,
\begin{equation}
A_{i_{1} \ldots i_{n}} {}^{j_1 \cdots j_{m}} \cdot C_{l_{1} \ldots j_{s}} {}^{k_1 \ldots l_{i}}=D_{i_{1} \ldots i_{n}} {}^{j_1 \cdots j_{m}} {}_{l_1 \ldots l_{s}}{}^{k_1 \ldots k_{l}}
\end{equation}
\textbf{The order of indices is important here, because $a_{i j}$ may be different from $a_{j i}$.}
\begin{example}
 Prove, by checking the transformation law, that the product of the contravariant vector $a^{i}$ and mixed tensor $b_{i}^{k}$ is a mixed $(2,1)$ -type tensor.
 
$$
a^{i^{\prime}}\left(x^{\prime}\right) b_{j^{\prime}}^{k^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{m}} a^{m}(x) \frac{\partial x^{k^{\prime}}}{\partial x^{l}} \frac{\partial x^{n}}{\partial x^{j^{\prime}}} b_{n}^{l}(x)
$$
\end{example}
\textbf{Contraction reduces the (n,m)-tensor to the (n-1,m-1)-tensor through the summation over two (always upper and lower, of course) indices.} For example,
\begin{equation}
A_{i j k}^{l n} \longrightarrow A_{i j k}^{l k}=\sum_{k=1}^{3} A_{i j k}^{l k}
\end{equation}

The internal product of the two tensors consists in their multiplication with the consequent contraction over some couple of indices. \textbf{Internal product of (m, n) and (r, s)-type tensors results in the (m+r-1, n+s-1)-type tensor.}
\begin{equation}
A_{i j k} \cdot B^{l j}=\sum_{j=1}^{3} A_{i j k} B^{l j}
\end{equation}
\begin{example}
Prove that the internal product $a_{i} \cdot b^{i}$ is a scalar if $a_{i}(x)$ and $b^{i}(x)$ are co- and contravariant vectors.

$$
b^{i^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{l}} b^{l}(x), \quad a_{i^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{k}}{\partial x^{i^{\prime}}} a_{k}(x)
$$
Then
$$
a_{i^{\prime}}\left(x^{\prime}\right) b^{i^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{l}} \frac{\partial x^{k}}{\partial x^{i^{\prime}}} b^{l}(x) a_{k}(x)=\delta_{l}^{k} b^{l}(x) a_{k}(x)=b^{k}(x) a_{k}(x)
$$
\end{example}
\begin{defi}
        Consider a basis $\left\{\mathbf{e}_{i}\right\} .$ The \textbf{scalar product} of the two basis vectors,
        \begin{equation}
g_{i j}=\left(\mathbf{e}_{i}, \mathbf{e}_{j}\right)
\end{equation}
is called \textbf{metric}. Here \textbf{the scalar product is not always inner product.}
\end{defi}
\begin{qt}
\textbf{Properties of metric}:

1. Symmetry of the metric $g_{i j}=g_{j i}$ follows from the symmetry of a scalar product $\left(\mathbf{e}_{i}, \mathbf{e}_{j}\right)=\left(\mathbf{e}_{j}, \mathbf{e}_{i}\right)$

2. For the orthonormal basis $\hat{\mathbf{n}}_{a}$, the metric is nothing but the Kronecker symbol $g_{a b}=\left(\hat{\mathbf{n}}_{a}, \hat{\mathbf{n}}_{b}\right)=\delta_{a b}$

3. Metric is a (0, 2) - tensor, as
$$
\begin{aligned}
g_{i^{\prime} j^{\prime}} &=\left(\mathbf{e}_{i}^{\prime}, \mathbf{e}_{j}^{\prime}\right)=\left(\frac{\partial x^{l}}{\partial x^{i^{\prime}}} \mathbf{e}_{l}, \frac{\partial x^{k}}{\partial x^{\prime j}} \mathbf{e}_{k}\right) \\
&=\frac{\partial x^{l}}{\partial x^{\prime i}} \frac{\partial x^{k}}{\partial x^{\prime j}}\left(\mathbf{e}_{l}, \mathbf{e}_{k}\right)=\frac{\partial x^{l}}{\partial x^{\prime i}} \frac{\partial x^{k}}{\partial x^{\prime j}} \cdot g_{k l}
\end{aligned}
$$

4. The distance between two points: $M_{1}\left(x^{i}\right)$ and $M_{2}\left(y^{i}\right)$ is defined by the inner product:
$$
S_{12}^{2}=g_{i j}\left(x^{i}-y^{i}\right)\left(x^{j}-y^{j}\right)
$$
\end{qt}
\bluep{Since $g_{i j}$ is $(0,2)$ - tensor and $\left(x^{i}-y^{i}\right)$ is $(1,0)$ -tensor (contravariant vector) $-S_{12}^{2}$ is a scalar. Therefore, $S_{12}^{2}$ is the same in any coordinate system.}

\textbf{The conjugated metric} is defined as
\begin{equation}
g^{i j}=\left(\mathbf{e}^{i}, \mathbf{e}^{j}\right) \quad \text { where } \quad\left(\mathbf{e}^{i}, \mathbf{e}_{k}\right)=\delta_{k}^{i}
\end{equation}
\begin{qt}
\begin{equation}
g^{i k} g_{k j}=\delta_{j}^{i}
\end{equation}
\end{qt}

We can\textbf{ Raise and lower indices of a tensor} by taking an appropriate internal product of a given tensor and the corresponding metric tensor.
\begin{equation}
\begin{aligned}
&\text { Lowering the index, } A_{i}(x)=g_{i j} A^{j}(x), \quad B_{i k}(x)=g_{i j} B_{ k}^{j}(x)\\
&\text { Raising the index, } \quad C^{l}(x)=g^{l j} C_{j}(x), \quad D^{i k}(x)=g^{i j} D_{j}^{k}(x)
\end{aligned}
\end{equation}
\begin{equation}
    \mathbf{e}_{i} g^{i j}=\mathbf{e}^{j}, \quad \mathbf{e}^{k} g_{k l}=\mathbf{e}_{l}
\end{equation}
\begin{qt}
Let the metric $g_{ij}$ correspond to the basis ek and to the coordinates $x^{k} .$ The determinants of the metric tensors and of the matrices of the transformations to Cartesian coordinates $X^{a}$ satisfy the following relations:
\begin{equation}
g=\operatorname{det}\left(g_{i j}\right)=\operatorname{det}\left(\frac{\partial X^{a}}{\partial x^{k}}\right)^{2}, \quad g^{-1}=\operatorname{det}\left(g^{k l}\right)=\operatorname{det}\left(\frac{\partial x^{l}}{\partial X^{b}}\right)^{2}
\end{equation}
\end{qt}
The possibility of lowering and raising the indices may help us in contracting two contravariant or two covariant indices of a tensor. 
\begin{example}
Suppose we need to contract the two first indices of the $(3,0)$ -tensor $B^{i j k} .$ After we lower the second index, we arrive at the tensor
$$
B^i{}_{l}{}^{k}=B^{i j k} g_{j l}
$$
And now we can contract the indices $i,j$. But, if we forget to indicate the order of indices, we obtain:$B_{l}^{i k}$, and it is not immediately clear which index was lowered and in which couple of indices one has to perform the contraction.
\end{example}

\section{Symmetric, Skew(Anti) Symmetric Tensors, and Determinants}
\subsection{Definitions and general considerations}
Tensor $A^{i j k \ldots}$ is called \textbf{symmetric} in the indices $i$ and $j,$ if
\begin{equation}
A^{i j k \ldots}=A^{j i k \ldots}
\end{equation}
Tensor $A^{i_{1} i_{2} \ldots i_{n}}$ is called \textbf{completely (or absolutely) symmetric} in the indices $\left(i_{1}, i_{2}, \ldots, i_{n}\right),$ if it is symmetric in any couple of these indices. 

Tensor $A^{i j}$ is called skew-symmetric or antisymmetric, if
\begin{equation}
A^{i j}=-A^{j i}
\end{equation}
\bluep{The advantage of tensors is that their (anti)symmetry holds under the transformation from one basis to another.}
\begin{example}
Consider the basis in 2D,
$$
\mathbf{e}_{1}=\hat{\mathbf{i}}+\hat{\mathbf{j}}, \quad \mathbf{e}_{2}=\hat{\mathbf{i}}-\hat{\mathbf{j}}
$$
(i) Derive all components of the absolutely antisymmetric tensor $\varepsilon^{i j}$ in the new basis, taking into account that $\varepsilon^{a b}=\epsilon^{a b},$ where $\epsilon^{12}=1$ in the orthonormal basis $\hat{\mathbf{n}}_{1}=\hat{\mathbf{i}}, \quad \hat{\mathbf{n}}_{2}=\hat{\mathbf{j}} .$ The calculation should be performed directly and also by using the formula for antisymmetric tensor. Explain the difference between the two results. 

(ii) Repeat the calculation for $\varepsilon_{i j} .$ Calculate the metric components and verify that $\varepsilon_{i j}=g_{i k} g_{j l} \varepsilon^{k l}$ and that $\varepsilon^{i j}=g^{i k} g^{j l} \varepsilon_{k l}$

\end{example}
\textbf{Solution:}
From the basis relation, we have:
$$
x^{\prime}=x+y
$$
$$
y^{\prime}=x-y
$$
Thus, 
$$
\frac{\partial x^{j^{\prime}}}{\partial x^i}=\left(\begin{array}{cc}
{1} & {1}\\
{1} & {-1}
\end{array}\right)
$$
and $\varepsilon^{12}=\frac{\partial x^{\prime 1}}{\partial x^k}\frac{\partial x^{\prime 2}}{\partial x^l}\epsilon^{kl}=\epsilon^{21}-\epsilon^{12}=-2$. Since \textbf{the metric tensor of the orthonormal basis is}
$$
g_{ab}=\left(\begin{array}{cc}
{1} & {0}\\
{0} & {1}
\end{array}\right)
$$
we have $\epsilon_{ab}=\epsilon^{ab}$. Thus, $\varepsilon_{12}=\frac{\partial x^k}{\partial x^{\prime 1}}\frac{\partial x^l}{\partial x^{\prime 2}}\epsilon_{kl}=-1/2$. Also, the metric tensor for our new basis is
$$
g_{ij}=\frac{\partial x^{a}}{\partial x^i}\frac{\partial x^{b}}{\partial x^j}g_{ab}=\left(\begin{array}{cc}
{1/2} & {0}\\
{0} & {1/2}
\end{array}\right)
$$
Using $g^{i k} g_{k j}=\delta_{j}^{i}$, we also have $g^{ij}=\left(\begin{array}{cc}
{2} & {0}\\{0} & {2}\end{array}\right)$. Obviously, we have:
\begin{qt}
$$
\varepsilon_{i j}=g_{i k} g_{j l} \varepsilon^{k l}
$$
$$
\varepsilon^{i j}=g^{i k} g^{j l} \varepsilon_{k l}
$$
\end{qt}

\subsection{Completely Antisymmetric Tensors}