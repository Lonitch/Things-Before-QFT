\chapter{Tensor Calculus}

\section{Total and Partial Derivatives}
To understand the different kinds of derivatives, let's say we have a function $\rho(t, x(t), p(t))$ which, in general, depends on the location $x(t)$ and momentum $p(t)$ plus the time $t.$ A key observation is that the location $x(t)$ and momentum $p(t)$ are functions of $t$ too. Therefore, we need to be extremely careful what we mean when we calculate the derivative with respect to the time $t .$
$$
\frac{d \rho}{d t}=\lim _{\Delta t \rightarrow 0} \frac{\rho(t+\Delta t, x(t+\Delta t), p(t+\Delta t))-\rho(t, x(t), p(t))}{\Delta t}
$$
\textbf{The result is the total rate of change of $\rho$}.
$$
\frac{\partial \rho}{\partial t}=\lim _{\Delta t \rightarrow 0} \frac{\rho(t+\Delta t, x(t), p(t))-\rho(t, x(t), p(t))}{\Delta t}
$$
The key difference is that we only vary $t$ if it appears explicitly
in $\rho$ but not if it only appears implicitly because $x(t)$ and $p(t)$
also depend on $t .$ Thus
$$
\frac{d \rho}{d t}=\frac{\partial \rho}{\partial x} \frac{d x}{d t}+\frac{\partial \rho}{\partial p} \frac{d p}{d t}+\frac{\partial \rho}{\partial t}
$$
\section{Taylor Expansion}
In general, we want to estimate the value of some function $f(x)$ at some value of $x$ by using our knowledge of the function's value at some fixed point $a .$ The Taylor series then reads
\begin{equation}
\begin{aligned}
f(x)=& \sum_{n=0}^{\infty} \frac{f^{(n)}(a)(x-a)^{n}}{n !} \\
=& \frac{f^{(0)}(a)(x-a)^{0}}{0 !}+\frac{f^{(1)}(a)(x-a)^{1}}{1 !}+\frac{f^{(2)}(a)(x-a)^{2}}{2 !} \\
&+\frac{f^{(3)}(a)(x-a)^{3}}{3 !}+\ldots
\end{aligned}
\end{equation}
or
\begin{equation}
f(x+a)=f(x)+(a \cdot \partial) f(x)+\frac{1}{2}(a \cdot \partial)^{2} f(x)+\cdots
\end{equation}
Taylor expansion of a scalar field (function $f$ that maps $\mathbb{R}^{n}$ to $\mathbb{R}$).Now, identify $\partial f / \partial t$ as $\hat{\boldsymbol{n}} \cdot \nabla f .$ In addition, see that $t \hat{\boldsymbol{n}}=\boldsymbol{x}-\boldsymbol{x}_{0} .$ Some clever recombining of terms gives
\begin{equation}
f(x)=f\left(x_{0}\right)+\left.\left(x-x_{0}\right) \cdot \nabla f\right|_{x_{0}}+\left.\frac{1}{2}\left(\left[x-x_{0}\right] \cdot \nabla\right)^{2} f\right|_{x_{0}}+\ldots
\end{equation}
and
\begin{equation}
\hat{\boldsymbol{n}} \cdot \nabla=\partial_{t}
\end{equation}

\section{Vector Identities}
\begin{equation}
\begin{aligned}
&\vec{\nabla} \cdot(\vec{\nabla} \times \vec{A}) \equiv \operatorname{div}(\operatorname{rot} \vec{A})=(\vec{\nabla} \times \vec{\nabla}) \cdot \vec{A} \equiv 0\\
&\vec{\nabla} \times(\vec{\nabla} \varphi) \equiv \operatorname{rot} \operatorname{grad} \varphi=(\vec{\nabla} \times \vec{\nabla}) \varphi \equiv 0
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
&\vec{\nabla} \cdot(\vec{A} \varphi)=\varphi \vec{\nabla} \cdot \vec{A}+\vec{A} \cdot \vec{\nabla} \varphi \quad \Longleftrightarrow \quad \operatorname{div}(\vec{A} \varphi)=\varphi \operatorname{div} \vec{A}+\vec{A} \cdot \operatorname{grad} \varphi\\
&\vec{\nabla} \times(\vec{A} \varphi)=\varphi \vec{\nabla} \times \vec{A}-\vec{A} \times \vec{\nabla} \varphi \quad \Longleftrightarrow \quad \operatorname{rot}(\vec{A} \varphi)=\varphi \operatorname{rot} \vec{A}-\vec{A} \times \operatorname{grad} \varphi\\
&\vec{\nabla} \cdot(\vec{A} \times \vec{B})=\vec{B} \cdot(\vec{\nabla} \times \vec{A})-\vec{A} \cdot(\vec{\nabla} \times \vec{B}) \quad \Longleftrightarrow \quad \operatorname{div}(\vec{A} \times \vec{B})=\vec{B} \cdot \operatorname{rot} \vec{A}-\vec{A} \cdot \operatorname{rot} \vec{B}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\vec{\nabla} \times(\vec{A} \times \vec{B}) &=(\vec{B} \cdot \vec{\nabla}) \vec{A}-(\vec{A} \cdot \vec{\nabla}) \vec{B}+\vec{A}(\vec{\nabla} \cdot \vec{B})-\vec{B}(\vec{\nabla} \cdot \vec{A}) \\
& \Longleftrightarrow \operatorname{rot}(\vec{A} \times \vec{B})=(\vec{B} \operatorname{grad}) \vec{A}-(\vec{A} \operatorname{grad}) \vec{B}+\vec{A}(\operatorname{div} \vec{B})-\vec{B}(\operatorname{div} \vec{A})
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\vec{\nabla}(\vec{A} \cdot \vec{B})=&(\vec{B} \cdot \vec{\nabla}) \vec{A}+(\vec{A} \cdot \vec{\nabla}) \vec{B}+\vec{A} \times(\vec{\nabla} \times \vec{B})+\vec{B} \times(\vec{\nabla} \times \vec{A}) \\
& \Longleftrightarrow \operatorname{grad}(\vec{A} \cdot \vec{B})=(\vec{B} \cdot \operatorname{grad}) \vec{A}+(\vec{A} \cdot \operatorname{grad}) \vec{B}+\vec{A} \times \operatorname{rot} \vec{B}+\vec{B} \times \operatorname{rot} \vec{A}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
&\vec{\nabla} \cdot(\vec{\nabla} \varphi) \equiv \operatorname{div}(\operatorname{grad} \varphi) \equiv \Delta \varphi=\frac{\partial^{2} \varphi}{\partial x^{2}}+\frac{\partial^{2} \varphi}{\partial y^{2}}+\frac{\partial^{2} \varphi}{\partial z^{2}}, \quad \Delta=\text { Laplace Operator }\\
&\vec{\nabla} \times(\vec{\nabla} \times \vec{A}) \equiv \operatorname{rot}(\operatorname{rot} \vec{A})=\vec{\nabla}(\vec{\nabla} \cdot \vec{A})-(\vec{\nabla} \cdot \vec{\nabla}) \vec{A} \equiv \operatorname{grad} \operatorname{div} \vec{A}-\Delta \vec{A}
\end{aligned}
\end{equation}

\section{Linear Spaces, Vectors, and Tensors}
The linear space, say $L$, consists of the elements (vectors) that \textbf{permit linear operations with the properties described below}:
\begin{itemize}
    \item Summing up the vectors
    \item Multiplication by a number
\end{itemize}
\begin{qt}
    Consider the set of vectors, elements of the linear space L
    $$
\left\{\mathbf{a}_{1}, \mathbf{a}_{2}, \ldots, \mathbf{a}_{n}\right\}=\left\{\mathbf{a}_{i} | i=1, \ldots, n\right\}
$$
and the set of real numbers $k_{1}, k_{2}, \ldots k_{n} .$ Vector
$$
\mathbf{k}=\sum_{i=1}^{n} k^{i} \mathbf{a}_{i}=\mathbf{k}^{i} \mathbf{a}_{i}=0\Longrightarrow \sum_{i=1}^{n}\left(k^{i}\right)^{2}=0
$$
then vectors $\left\{\mathbf{a}_{\mathbf{i}}, i=1, \ldots, n\right\}$ are called \textbf{linearly independent}. The last condition means that no one of the coefficients $k_{i}$ can be different from zero.
\end{qt}
For example, in 2D, two vectors are linearly dependent if and only if they are parallel; in 3D, three vectors are linearly dependent if and only if they belong to the same plane, etc.
\begin{defi}
        The maximal number of linearly independent elements of the linear space $L$ is called its dimension. It proves useful to denote $L_{D}$ a linear space of dimension $D .$
\end{defi}
\begin{thm}
Consider a $D$-dimensional linear space $L_{D}$ and the set of linearly independent vectors $\mathbf{e}_{i}=\left(\mathbf{e}_{1}, \mathbf{e}_{2}, \ldots, \mathbf{e}_{D}\right) .$ Then, for any vector a one can write
\begin{equation}
    \mathbf{a}=\sum_{i=1}^{D} a^{i} \mathbf{e}_{i}=a^{i} \mathbf{e}_{i}
    \label{vector-basis}
\end{equation}
where the coefficients $a^{i}$ are defined in a unique way.
\end{thm}
\bluep{The coefficients $a^{i}$ are called components or \textbf{contravariant components of the vector $\mathbf{a}$}}.The word “contravariant” here means that the components $a^i$ have upper indices.

\section{Direct product of the two linear spaces}
Let's consider the example of phase space in the classic mechanics. The coordinate system in the linear space $L_{D}$ consists of the initial point $O$ and the basis $\mathbf{e}_{i} .$ The position of a point $P$ can be characterized by \textbf{its position vector or radius vector $\mathbf{r}=\overrightarrow{O P}=x^{i} \mathbf{e}_{i}$}.

If the phase space is describing one particle, the space is composed of the radius vectors $\mathbf{r}$ and velocities $\mathbf{v}$ of the particle:
$$
\mathbf{r}=x^{i} \mathbf{e}_{i}, \quad \mathbf{v}=v^{j} \mathbf{f}_{j}
$$
The two set of bases can be related and independent. This example is a particular case of the linear space which is called \redp{a direct product
of the two linear spaces}. The element of the direct product of the two linear spaces $L_{D_{1}}$ and $L_{D_{2}}$ (\textbf{they can have different dimensions}) is the ordered set of the elements of each of the two spaces $L_{D_{1}}$ and $L_{D_{2}} .$
\begin{qt}
    The notation for the basis in the case of configuration space is $\mathbf{e}_{i} \otimes \mathbf{f}_{j}$. Hence, the state of the point-like particle in the phase space is characterized by the element of this linear space, which can be presented as:
    $$
x^{i} v^{j} \mathbf{e}_{i} \otimes \mathbf{f}_{j}
$$
In general, one can define the space that is a direct product of several linear spaces with different individual basis sets $\mathbf{e}_{i}$ each. In this case, we will have $\mathbf{e}_{i}^{(1)}, \mathbf{e}_{i}^{(2)}, \ldots$ $\mathbf{e}_{i}^{(N)} .$ The basis in the direct product space will be
$$
\mathbf{e}_{i_{1}}^{(1)} \otimes \mathbf{e}_{i_{2}}^{(2)} \otimes \ldots \otimes \mathbf{e}_{i_{N}}^{(N)}
$$
and the element becomes
$$
T^{i_{1} i_{2} \ldots i_{N}} \mathbf{e}_{i_{1}}^{(1)} \otimes \mathbf{e}_{i_{2}}^{(2)} \otimes \ldots \otimes \mathbf{e}_{i_{N}}^{(N)}
$$
\end{qt}
\section{Vector basis and its transformation}
Let us start from the components of the vector, which were defined in (\ref{vector-basis}). Consider, along with the original basis $\mathbf{e}_{i},$ another basis $\mathbf{e}_{i}^{\prime} .$ since each vector of the new basis belongs to the same space, it can be expanded using the original basis as
\begin{equation}
\mathbf{e}_{i}^{\prime}=\wedge_{i^{\prime}}^{j} \mathbf{e}_{j}
\label{basis-transform}
\end{equation}
and
$$
\mathbf{a}=a^{i} \mathbf{e}_{i}=a^{j^{\prime}} \mathbf{e}_{j^{\prime}}=a^{j^{\prime}} \wedge_{j^{\prime}}^{i} \mathbf{e}_{i}
$$
\begin{qt}
\begin{equation}
    a^{i}=a^{j^{\prime}} \wedge_{j^{\prime}}^{i}
    \label{cotravariant-coord-transform}
\end{equation}
\end{qt}

Similarly, we can make inverse transformation:
$$
a^{k^{\prime}}=\left(\wedge^{-1}\right)_{l}^{k^{\prime}} a^{l}
$$
and
\[
\left(\wedge^{-1}\right)_{l}^{k^{\prime}} \cdot \wedge_{i^{\prime}}^{l}=\delta_{i^{\prime}}^{k^{\prime}} \quad \text { and } \quad \wedge_{i^{\prime}}^{l} \cdot\left(\wedge^{-1}\right)_{k}^{i^{\prime}}=\delta_{k}^{l}
\]
\begin{qt}
Taking the partial derivatives, we arrive at the relations
\[
\wedge_{j^{\prime}}^{i}=\frac{\partial x^{i}}{\partial x^{j^{\prime}}} \quad \text { and } \quad\left(\wedge^{-1}\right)_{l}^{k^{\prime}}=\frac{\partial x^{k^{\prime}}}{\partial x^{l}}
\]
and
$$
\left(\wedge^{-1}\right)_{l}^{k^{\prime}} \cdot \wedge_{k^{\prime}}^{i}=\frac{\partial x^{k^{\prime}}}{\partial x^{l}} \frac{\partial x^{i}}{\partial x^{k^{\prime}}}=\frac{\partial x^{i}}{\partial x^{l}}=\delta_{l}^{i}
$$
is nothing but the chain rule for partial derivatives.
\end{qt}

\section{Scalar, vector, and tensor fields}
\begin{defi}
        Function $\varphi(x)$ is called scalar field or simply scalar if it does not transform under the change of coordinates
        \begin{equation}
\varphi(x)=\varphi^{\prime}\left(x^{\prime}\right)
\end{equation}
\end{defi}
Let us give a clarifying example in 1D. Consider a function
$$
y=x^2
$$
Now, let us change the variables 
$$
x^{\prime}=x+1
$$
The function $y=\left(x^{\prime}\right)^{2},$ obviously, represents another parabola. \bluep{In order to preserve the plot intact, we need to modify the form of the function, that is, to go from $\varphi$ to $\varphi^{\prime}$.} The new function $y^{\prime}=\left(x^{\prime}-1\right)^{2}$ will represent the original parabola,because the change of the variable is completely compensated by the change of the form of the function.
\begin{mybox}
\begin{center}
    Discuss whether the three numbers temperature $T(x)$, pressure $p(x)$ and density $\rho(x)$ form a contravariant vector
\end{center}
\end{mybox}
\begin{mybox2}
We can only form contravariant vector if the numbers transform by following the rule (\ref{cotravariant-coord-transform}). Since these numbers are scalar fields, it transforms like $T(\mathbf{r})=T^{\prime}(\mathbf{r}^{\prime})$. Thus, these three parameters can not form a contravariant vector.
\end{mybox2}
\begin{example}
From the definition above we know $\varphi^{\prime}(x) \neq \varphi(x)$ and $\varphi\left(x^{\prime}\right) \neq \varphi(x)$. Let us calculate these quantities explicitly for the special case of infinitesimal transformation $x^{\prime i}=x^{i}+\xi^{i}$ where $\xi$ are constant coefficients. Now we have
$$
\varphi\left(x^{\prime}\right)=\varphi\left(x+\xi\right)\overset{Taylor}{=}\varphi(x)+\frac{\partial \varphi}{\partial x^{i}} \xi^{i}
$$
and,
$$
\varphi^{\prime}\left(x^{i}\right)=\varphi^{\prime}\left(x^{i}-\xi^{i}\right)=\varphi^{\prime}\left(x^{\prime}\right)-\frac{\partial \varphi^{\prime}}{\partial x^{i^{\prime}}} \cdot \xi^{i}
$$
rewrite the two equations above, we find:
$$
\frac{\partial \varphi^{\prime}}{\partial x^{\prime i}}=\frac{\varphi^{\prime}\left(x^{\prime i}\right)-\varphi^{\prime}\left(x^{i}-\xi^{i}\right)}{\xi^i}
$$
$$
\frac{\partial \varphi}{\partial x^{i}}=\frac{\varphi(x^{\prime i})-\varphi(x^i)}{\xi^i}
$$
At the limit of $\xi\rightarrow0$, we have
$$
\frac{\partial \varphi^{\prime}}{\partial x^{\prime i}}=\frac{\partial \varphi(x)}{\partial x^{i}}+\mathcal{O}(\xi)
$$
As mentioned above, 
$$
\varphi^{\prime}\left(x^{i}\right)=\varphi^{\prime}\left(x^{\prime}\right)-\frac{\partial \varphi^{\prime}}{\partial x^{\prime i}} \cdot \xi^{i}\\
=\varphi^{\prime}\left(x^{\prime}\right)-\frac{\partial \varphi(x)}{\partial x^{i}}\cdot\xi^i
$$
Since $\varphi(x)=\varphi^{\prime}\left(x^{\prime}\right)$, we have
$$
\varphi^{\prime}(x)=\varphi(x)-\xi^{i} \partial_{i} \varphi
$$
\textbf{where we have introduced a useful notation $\partial_{i}=\partial / \partial x^{i}$.}
\end{example}
\textbf{For the vector field, we have the following rule for the coordinate transformation}. The set of three functions $\left\{a^{i}(x)\right\}=\left\{a^{1}(x), a^{2}(x), a^{3}(x)\right\}$ forms contravariant vector field (or simply vector field) if they transform, under the change of coordinates $\left\{x^{i}\right\} \rightarrow\left\{x^{\prime} j\right\},$ as
\begin{qt}
\begin{equation}
a^{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}}}{\partial x^{i}} \cdot a^{i}(x)
\end{equation}
\end{qt}
\bluep{The components of the vector in a given geometrical point of space modify under the coordinate transformation, while the scalar field does not.}The scalar and vector fields can be considered as examples of the more general objects called tensors. Tensors are also defined through their transformation rules.
\begin{qt}
The set of $3^n$ functions $\left\{a^{i_{1} \ldots i_{n}}(x)\right\}$ is called a contravariant tensor of rank $n,$ if these functions transform, under $x^{i} \rightarrow x^{i},$ as
\begin{equation}
a^{i_{1}^{i} \ldots i_{n}^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i_{1}^{\prime}}}{\partial x^{j_{1}}} \ldots \frac{\partial x^{i_{n}^{\prime}}}{\partial x^{j_{n}}} a^{j_{1} \ldots j_{n}}(x)
\end{equation}
\end{qt}

\section{Orthonormal Basis and Cartesian Coordiantes}
The scalar product of two vectors $\mathbf{a}$ and $\mathbf{b}$ in 3D is defined in a ususal way,
\begin{equation}
(\mathbf{a}, \mathbf{b})=\mathbf{a}\cdot\mathbf{b}=|\mathbf{a}| \cdot|\mathbf{b}| \cdot \cos \theta
\end{equation}
Special orthonormal basis $\left\{\hat{\mathbf{n}}_{a}\right\}$ is the one with
\begin{equation}
\left(\hat{\mathbf{n}}_{a}, \hat{\mathbf{n}}_{b}\right)=\delta_{a b}=\left\{\begin{array}{l}
{1 \text { if } a=b} \\
{0 \text { if } a \neq b}
\end{array}\right.
\end{equation}
\begin{example}
Making transformations of the basis vectors, verify that the change of coordinates
$$
x^{\prime}=\frac{x+y}{\sqrt{2}}+3, \quad y^{\prime}=\frac{x-y}{\sqrt{2}}-5
$$
does not modify the type of coordinates $x^{\prime}, y^{\prime},$ which remains Cartesian.
\textbf{Solution}:
$$
x^{\prime}=\frac{x+y}{\sqrt{2}}+3, \quad y^{\prime}=\frac{x-y}{\sqrt{2}}-5
$$
The change of initial point $(0,0) \rightarrow(3,-5)$ does not have relation to the change of basis. Then
$$
\begin{aligned}
x^{\prime} \hat{i}^{\prime}+y^{\prime} \hat{j}^{\prime} &=\left(\frac{x+y}{\sqrt{2}}\right) \hat{i}^{\prime}+\left(\frac{x-y}{\sqrt{2}}\right) \hat{j}^{\prime} \\
&=\frac{x}{\sqrt{2}}\left(\hat{i}^{\prime}+\hat{j}^{\prime}\right)+\frac{y}{\sqrt{2}}\left(\hat{i}^{\prime}-\hat{j}^{\prime}\right)=x \hat{i}+y \hat{j}
\end{aligned}
$$
Thus, $\hat{i}=\frac{1}{\sqrt{2}}\left(\hat{i}^{\prime}+\hat{j}^{\prime}\right), \hat{j}=\frac{1}{\sqrt{2}}\left(\hat{i}^{\prime}-\hat{j}^{\prime}\right) .$ Obviously, $\hat{i}^{2}=\hat{j}^{2}=1$ and $\hat{i} \cdot \hat{j}=0$
\end{example}
Now we can introduce a conjugated covariant basis.
\begin{qt}
Consider basis $\left\{\mathbf{e}_{i}\right\} .$ The conjugated basis is defined as a set of vectors $\left\{\mathbf{e}^{j}\right\}$ which satisfy the relations
\begin{equation}
\mathbf{e}_{i} \cdot \mathbf{e}^{j}=\delta_{i}^{j}
\end{equation}
The special property of the orthonormal basis is that $\hat{\mathbf{n}}^{a}=\hat{\mathbf{n}}_{a}$.

Any vector a can be expanded using the conjugated basis $\mathbf{a}=a_{i} \mathbf{e}^{i} .$ \redp{The coefficients $a_{i}$ are called covariant components of the vector $\mathbf{a}$}.
\end{qt}
In the case of covariant vector components, the transformation is done by means of the matrix inverse to the one for the contravariant components.
\begin{thm}
If we change the basis of the coordinate system from $\mathbf{e}_{i}$ to $\mathbf{e}_{i}^{\prime},$ then the covariant components of the vector a transform as
\begin{equation}
a_{i}^{\prime}=\frac{\partial x^{j}}{\partial x^{i}} a_{j}
\end{equation}
\end{thm}

 The set of three functions $\left\{A_{i}(x)\right\}$ forms a covariant vector field, if they transform from one coordinate system to another one as
 \begin{equation}
A_{i}^{\prime}\left(x^{\prime}\right)=\frac{\partial x^{j}}{\partial x^{i}} A_{j}(x)
\end{equation}
The set of $3^{n}$ functions $\left\{A_{i_{1} i_{2} \ldots i_{n}}(x)\right\}$ form a covariant tensor of rank $n$ if they transform from one coordinate system to another as
\begin{equation}
A_{i_{1} i_{2} \ldots i_{n}}^{\prime}\left(x^{\prime}\right)=\frac{\partial x^{j_{1}}}{\partial x^{i_{1}}} \frac{\partial x^{j_{2}}}{\partial x^{i_{2}}} \cdots \frac{\partial x^{j_{n}}}{\partial x^{n_{n}}} A_{j_{1} j_{2} \ldots j_{n}}(x)
\end{equation}
In general
\begin{qt}
The set of $3^{n+m}$ functions $\left\{B_{i_{1} \ldots i_{n}} j_{1 \ldots j_{n}}(x)\right\}$ forms the tensor of the type $(m, n),$ if these functions transform, under the change of coordinate basis, as
\begin{equation}
B_{i_{1}^{\prime} \ldots i_{n}^{\prime}} ^{j_{1}^{\prime} \ldots j_{m}^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}_1}}{\partial x^{l_{1}}} \cdots \frac{\partial x^{j_{m}^{\prime}}}{\partial x^{l_{m}}} \frac{\partial x^{k_{1}}}{\partial x^{\prime i_{1}}} \cdots \frac{\partial x^{k_{n}}}{\partial x^{\prime i_{n}}} B_{k_{1} \ldots k_{n}}^{l_{1} \ldots l_{m}}(x)
\end{equation}
Other possible names are the mixed tensor of covariant rank $n$ and contravariant rank $m,$ or simply $(m, n)$ -tensor.
\end{qt}
\textbf{Tensors are important due to the fact that they offer the coordinate-independent description of geometrical and physical laws.} The following example shows this observation:
\begin{example}
For an arbitrary $\mathbf{a}(x)=a^{i}(x) \mathbf{e}_{i}$ we have
$$
a^{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}}}{\partial x^{i}} a^{i}(x), \quad \mathbf{e}_{j^{\prime}}\left(x^{\prime}\right)=\mathbf{e}_{k}(x) \frac{\partial x^{k}}{\partial x^{j^{\prime}}}
$$
Then
$$
a^{j^{\prime}}\left(x^{\prime}\right) \mathbf{e}_{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{j^{\prime}}}{\partial x^{i}} a^{i}(x) \mathbf{e}_{k}(x) \frac{\partial x^{k}}{\partial x^{j^{\prime}}}=\delta_{l}^{k} a^{i}(x) \mathbf{e}_{k}(x)=a^{i}(x) \mathbf{e}_{i}(x)
$$
\end{example}

If the Kronecker symbol transforms as a mixed $(1,1)$ tensor,
\begin{qt}
\begin{equation}
\delta_{j^{\prime}}^{i^{\prime}}=\frac{\partial x^{i^{\prime}}}{\partial x^{k}} \frac{\partial x^{l}}{\partial x^{j^{\prime}}} \delta_{l}^{k}=\frac{\partial x^{i^{\prime}}}{\partial x^{j^{\prime}}}
\end{equation}
\end{qt}
then in any coordinates $x^i$ it has the same form 
$$
\delta_{j}^{i}=\left\{\begin{array}{l}
{1 \text { if } i=j} \\
{0 \text { if } i \neq j}
\end{array}\right.
$$
\textbf{This property is very important, as it enables us to use the Kronecker symbol in any coordinates}

\begin{example}
Show that the product $A^{i}(x) B_{j}(x)$ of covariant and contravariant vectors transforms as a $(1,1)$ -type mixed tensor.
$$
A^{i^{\prime}}\left(x^{\prime}\right) A_{j^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{k}} A^{k}(x) \frac{\partial x^{l}}{\partial x^{j^{\prime}}} B_{l}(x)=\frac{\partial x^{i^{\prime}}}{\partial x^{k}} \frac{\partial x^{l}}{\partial x^{j^{\prime}}} A^{k}(x) B_{l}(x)
$$
\end{example}

\section{Orthogonal transformation}
The rotation transformation around $\hat{\mathbf{z}}-$axis is given by the following relation:
\begin{qt}
\begin{equation}
\left(\begin{array}{l}
{x} \\
{y} \\
{z}
\end{array}\right)=\hat{\wedge}_{z}\left(\begin{array}{l}
{x^{\prime}} \\
{y^{\prime}} \\
{z^{\prime}}
\end{array}\right), \quad \text { where } \quad \hat{\wedge}_{z}=\hat{\wedge}_{z}(\alpha)=\left(\begin{array}{ccc}
{\cos \alpha} & {-\sin \alpha} & {0} \\
{\sin \alpha} & {\cos \alpha} & {0} \\
{0} & {0} & {1}
\end{array}\right)
\end{equation}
the matrix above has the following property:
\begin{equation}
\hat{\wedge}_{z}^{T}=\hat{\wedge}_{z}^{-1}
\end{equation}
\end{qt}
\begin{defi}
        The matrix $\hat{\wedge}_{z}$ which satisfies $\hat{\wedge}_{z}^{-1}=\hat{\wedge}_{z}^{T}$ and the corresponding coordinate transformation is called \textbf{orthogonal}.
\end{defi}
Similarly, we can write the rotation matrix around other axis:
\begin{equation}
\hat{\wedge}_{x}(\gamma)=\left(\begin{array}{ccc}
{1} & {0} & {0}\\
{0} & {\cos \gamma} & {-\sin \gamma} \\
{0} & {\sin \gamma} & {\cos \gamma}
\end{array}\right)
\end{equation}
\begin{equation}
\hat{\wedge}_{y}(\beta)=\left(\begin{array}{ccc}
{\cos \beta} & {0} & {-\sin \beta} \\
{0} & {1} & {0}\\
{\sin \beta} & {0} &  {\cos \beta}
\end{array}\right)
\end{equation}
In 3D space, any rotation of the rigid body may be represented as a combination of the rotation around the axes $\hat{\mathbf{z}}, \hat{\mathbf{y}},$ and $\hat{\mathbf{x}}$ to the angles $ \alpha, \beta,$ and $\gamma$:
$$
\hat{\wedge}=\hat{\wedge}_{z}(\alpha) \hat{\wedge}_{y}(\beta) \hat{\wedge}_{x}(\gamma)
$$
Since $(A \cdot B)^{T}=B^{T} \cdot A^{T} \quad \text { and } \quad(A \cdot B)^{-1}=B^{-1} \cdot A^{-1}$, we can easily obtain that \bluep{the general 3D rotation matrix satisfies the orthogonal relation.}
\begin{qt}
For the orthogonal matrix, one can take the determinant and arrive at $det\hat{\wedge}=det\hat{\wedge}^{-1}$. Therefore, 
$$
det\hat{\wedge}=\pm1
$$
As far as any rotation matrix has a determinant equal to one, there must be some other orthogonal matrices with the determinant equal to −1.
\end{qt}
In case where the matrix elements are allowed to be complex, the matrix that satisfies the property $U^{\dagger}=U^{-1}$ is called unitary. The operation $U^{\dagger}$ is called \textbf{Hermitian conjugation} and consists of complex conjugation plus transposition $U^{\dagger}=\left(U^{*}\right)^{T}$.
\begin{example}
Consider the transformation from one orthonormal basis $\hat{\mathbf{n}}_{i}$ to another such basis $\hat{\mathbf{n}}_{k}^{\prime},$ namely, $\hat{\mathbf{n}}_{k}^{\prime}=R_{k}^{i} \hat{\mathbf{n}}_{i} .$ Prove that the matrix $\left\|R_{k}^{i}\right\|$ is orthogonal.

 $\hat{\mathbf{n}}_{k}^{\prime}=R_{k}^{i} \hat{\mathbf{n}}_{i}$ and also $\hat{\mathbf{n}}^{\prime \prime}=\left(R^{-1}\right)_{j}^{l} \hat{\mathbf{n}}^{j} .$ since $\hat{\mathbf{n}}_{k}^{\prime}=\hat{\mathbf{n}}^{k^{\prime}}$ and $\hat{\mathbf{n}}_{i}=\hat{\mathbf{n}}^{i}$
also have $\left(R^{-1}\right)_{j}^{l}=\left(R^{T}\right)_{j}^{l}$
\end{example}

\section{Operations over Tensors, Metric Tensor}
\textbf{Multiplication of a tensor by a number produces a tensor of the same type.} This operation is equivalent to the multiplication of all tensor components to the same number $\alpha$, namely:
\begin{equation}
(\alpha A)_{i_{1} \ldots i_{n}}=\alpha \cdot A_{i_{1} \ldots i_{n}}^{j_{1} \ldots j_{m}}
\end{equation}
\textbf{Multiplication of two tensors is defined for a couple of tensors of any type.} The product of a (m, n)-tensor and a (t, s)-tensor results in the (m + t, n + s)- tensor, e.g.,
\begin{equation}
A_{i_{1} \ldots i_{n}} {}^{j_1 \cdots j_{m}} \cdot C_{l_{1} \ldots l_{s}} {}^{k_1 \ldots k_{i}}=D_{i_{1} \ldots i_{n}} {}^{j_1 \cdots j_{m}} {}_{l_1 \ldots l_{s}}{}^{k_1 \ldots k_{l}}
\end{equation}
\textbf{The order of indices is important here, because $a_{i j}$ may be different from $a_{j i}$.}
\begin{example}
 Prove, by checking the transformation law, that the product of the contravariant vector $a^{i}$ and mixed tensor $b_{i}^{k}$ is a mixed $(2,1)$ -type tensor.
 
$$
a^{i^{\prime}}\left(x^{\prime}\right) b_{j^{\prime}}^{k^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{m}} a^{m}(x) \frac{\partial x^{k^{\prime}}}{\partial x^{l}} \frac{\partial x^{n}}{\partial x^{j^{\prime}}} b_{n}^{l}(x)
$$
\end{example}
\textbf{Contraction reduces the (n,m)-tensor to the (n-1,m-1)-tensor through the summation over two (always upper and lower, of course) indices.} For example,
\begin{equation}
A_{i j k}^{l n} \longrightarrow A_{i j k}^{l k}=\sum_{k=1}^{3} A_{i j k}^{l k}
\end{equation}

The internal product of the two tensors consists in their multiplication with the consequent contraction over some couple of indices. \textbf{Internal product of (m, n) and (r, s)-type tensors results in the (m+r-1, n+s-1)-type tensor.}
\begin{equation}
A_{i j k} \cdot B^{l j}=\sum_{j=1}^{3} A_{i j k} B^{l j}
\end{equation}
\begin{example}
Prove that the internal product $a_{i} \cdot b^{i}$ is a scalar if $a_{i}(x)$ and $b^{i}(x)$ are co- and contravariant vectors.

$$
b^{i^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{l}} b^{l}(x), \quad a_{i^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{k}}{\partial x^{i^{\prime}}} a_{k}(x)
$$
Then
$$
a_{i^{\prime}}\left(x^{\prime}\right) b^{i^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{l}} \frac{\partial x^{k}}{\partial x^{i^{\prime}}} b^{l}(x) a_{k}(x)=\delta_{l}^{k} b^{l}(x) a_{k}(x)=b^{k}(x) a_{k}(x)
$$
\end{example}
\begin{defi}
        Consider a basis $\left\{\mathbf{e}_{i}\right\} .$ The \textbf{scalar product} of the two basis vectors,
        \begin{equation}
g_{i j}=\left(\mathbf{e}_{i}, \mathbf{e}_{j}\right)
\end{equation}
is called \textbf{metric}. Here \textbf{the scalar product is not always inner product.}
\end{defi}
\begin{qt}
\textbf{Properties of metric}:

1. Symmetry of the metric $g_{i j}=g_{j i}$ follows from the symmetry of a scalar product $\left(\mathbf{e}_{i}, \mathbf{e}_{j}\right)=\left(\mathbf{e}_{j}, \mathbf{e}_{i}\right)$

2. For the orthonormal basis $\hat{\mathbf{n}}_{a}$, the metric is nothing but the Kronecker symbol $g_{a b}=\left(\hat{\mathbf{n}}_{a}, \hat{\mathbf{n}}_{b}\right)=\delta_{a b}$

3. Metric is a (0, 2) - tensor, as
$$
\begin{aligned}
g_{i^{\prime} j^{\prime}} &=\left(\mathbf{e}_{i}^{\prime}, \mathbf{e}_{j}^{\prime}\right)=\left(\frac{\partial x^{l}}{\partial x^{i^{\prime}}} \mathbf{e}_{l}, \frac{\partial x^{k}}{\partial x^{\prime j}} \mathbf{e}_{k}\right) \\
&=\frac{\partial x^{l}}{\partial x^{\prime i}} \frac{\partial x^{k}}{\partial x^{\prime j}}\left(\mathbf{e}_{l}, \mathbf{e}_{k}\right)=\frac{\partial x^{l}}{\partial x^{\prime i}} \frac{\partial x^{k}}{\partial x^{\prime j}} \cdot g_{k l}
\end{aligned}
$$

4. The distance between two points: $M_{1}\left(x^{i}\right)$ and $M_{2}\left(y^{i}\right)$ is defined by the inner product:
$$
S_{12}^{2}=g_{i j}\left(x^{i}-y^{i}\right)\left(x^{j}-y^{j}\right)
$$
\end{qt}
\bluep{Since $g_{i j}$ is $(0,2)$ - tensor and $\left(x^{i}-y^{i}\right)$ is $(1,0)$ -tensor (contravariant vector) $-S_{12}^{2}$ is a scalar. Therefore, $S_{12}^{2}$ is the same in any coordinate system.}

\textbf{The conjugated metric} is defined as
\begin{equation}
g^{i j}=\left(\mathbf{e}^{i}, \mathbf{e}^{j}\right) \quad \text { where } \quad\left(\mathbf{e}^{i}, \mathbf{e}_{k}\right)=\delta_{k}^{i}
\end{equation}
\begin{qt}
\begin{equation}
g^{i k} g_{k j}=\delta_{j}^{i}
\end{equation}
\end{qt}

We can\textbf{ Raise and lower indices of a tensor} by taking an appropriate internal product of a given tensor and the corresponding metric tensor.
\begin{equation}
\begin{aligned}
&\text { Lowering the index, } A_{i}(x)=g_{i j} A^{j}(x), \quad B_{i k}(x)=g_{i j} B_{ k}^{j}(x)\\
&\text { Raising the index, } \quad C^{l}(x)=g^{l j} C_{j}(x), \quad D^{i k}(x)=g^{i j} D_{j}^{k}(x)
\end{aligned}
\end{equation}
\begin{equation}
    \mathbf{e}_{i} g^{i j}=\mathbf{e}^{j}, \quad \mathbf{e}^{k} g_{k l}=\mathbf{e}_{l}
\end{equation}
\begin{qt}
Let the metric $g_{ij}$ correspond to the basis ek and to the coordinates $x^{k} .$ The determinants of the metric tensors and of the matrices of the transformations to Cartesian coordinates $X^{a}$ satisfy the following relations:
\begin{equation}
g=\operatorname{det}\left(g_{i j}\right)=\operatorname{det}\left(\frac{\partial X^{a}}{\partial x^{k}}\right)^{2}, \quad g^{-1}=\operatorname{det}\left(g^{k l}\right)=\operatorname{det}\left(\frac{\partial x^{l}}{\partial X^{b}}\right)^{2}
\end{equation}
\end{qt}
The possibility of lowering and raising the indices may help us in contracting two contravariant or two covariant indices of a tensor. 
\begin{example}
Suppose we need to contract the two first indices of the $(3,0)$ -tensor $B^{i j k} .$ After we lower the second index, we arrive at the tensor
$$
B^i{}_{l}{}^{k}=B^{i j k} g_{j l}
$$
And now we can contract the indices $i,j$. But, if we forget to indicate the order of indices, we obtain:$B_{l}^{i k}$, and it is not immediately clear which index was lowered and in which couple of indices one has to perform the contraction.
\end{example}

\section{Symmetric, Skew(Anti) Symmetric Tensors, and Determinants}
\subsection{Definitions and general considerations}
Tensor $A^{i j k \ldots}$ is called \textbf{symmetric} in the indices $i$ and $j,$ if
\begin{equation}
A^{i j k \ldots}=A^{j i k \ldots}
\end{equation}
Tensor $A^{i_{1} i_{2} \ldots i_{n}}$ is called \textbf{completely (or absolutely) symmetric} in the indices $\left(i_{1}, i_{2}, \ldots, i_{n}\right),$ if it is symmetric in any couple of these indices. 

Tensor $A^{i j}$ is called skew-symmetric or antisymmetric, if
\begin{equation}
A^{i j}=-A^{j i}
\end{equation}
\bluep{The advantage of tensors is that their (anti)symmetry holds under the transformation from one basis to another.}
\begin{example}
Consider the basis in 2D,
$$
\mathbf{e}_{1}=\hat{\mathbf{i}}+\hat{\mathbf{j}}, \quad \mathbf{e}_{2}=\hat{\mathbf{i}}-\hat{\mathbf{j}}
$$
(i) Derive all components of the absolutely antisymmetric tensor $\varepsilon^{i j}$ in the new basis, taking into account that $\varepsilon^{a b}=\epsilon^{a b},$ where $\epsilon^{12}=1$ in the orthonormal basis $\hat{\mathbf{n}}_{1}=\hat{\mathbf{i}}, \quad \hat{\mathbf{n}}_{2}=\hat{\mathbf{j}} .$ The calculation should be performed directly and also by using the formula for antisymmetric tensor. Explain the difference between the two results. 

(ii) Repeat the calculation for $\varepsilon_{i j} .$ Calculate the metric components and verify that $\varepsilon_{i j}=g_{i k} g_{j l} \varepsilon^{k l}$ and that $\varepsilon^{i j}=g^{i k} g^{j l} \varepsilon_{k l}$

\end{example}
\textbf{Solution:}
From the basis relation, we have:
$$
x^{\prime}=x+y
$$
$$
y^{\prime}=x-y
$$
Thus, 
$$
\frac{\partial x^{j^{\prime}}}{\partial x^i}=\left(\begin{array}{cc}
{1} & {1}\\
{1} & {-1}
\end{array}\right)
$$
and $\varepsilon^{12}=\frac{\partial x^{\prime 1}}{\partial x^k}\frac{\partial x^{\prime 2}}{\partial x^l}\epsilon^{kl}=\epsilon^{21}-\epsilon^{12}=-2$. Since \textbf{the metric tensor of the orthonormal basis is}
$$
g_{ab}=\left(\begin{array}{cc}
{1} & {0}\\
{0} & {1}
\end{array}\right)
$$
we have $\epsilon_{ab}=\epsilon^{ab}$. Thus, $\varepsilon_{12}=\frac{\partial x^k}{\partial x^{\prime 1}}\frac{\partial x^l}{\partial x^{\prime 2}}\epsilon_{kl}=-1/2$. Also, the metric tensor for our new basis (\textbf{w.r.t. the orthonormalized vector basis}) is
$$
g_{ij}=\frac{\partial x^{a}}{\partial x^i}\frac{\partial x^{b}}{\partial x^j}g_{ab}=\left(\begin{array}{cc}
{1/2} & {0}\\
{0} & {1/2}
\end{array}\right)
$$
Using $g^{i k} g_{k j}=\delta_{j}^{i}$, we also have $g^{ij}=\left(\begin{array}{cc}
{2} & {0}\\{0} & {2}\end{array}\right)$. Obviously, we have:
\begin{qt}
$$
\varepsilon_{i j}=g_{i k} g_{j l} \varepsilon^{k l}
$$
$$
\varepsilon^{i j}=g^{i k} g^{j l} \varepsilon_{k l}
$$
\end{qt}

\subsection{Completely Antisymmetric Tensors}
$(n, 0)$ -tensor $A^{i_{1} \ldots i_{n}}$ is called completely (or absolutely) antisymmetric, if it is antisymmetric in any couple of its indices
\[
\forall(k, l), \quad A^{i_{1} \ldots i_{l} \ldots i_{k} \ldots i_{n}}=-A^{i_{1} \ldots i_{k} \ldots i_{l} \ldots i_{n}}
\]
In the case of an absolutely antisymmetric tensor, the sign changes when we perform permutation of any two indices.
\begin{qt}
\begin{itemize}
    \item In a D-dimensional space, all completely antisymmetric tensors of rank $n>D$ are equal to zero.

    \item Theorem 2 An absolutely antisymmetric tensor $A^{i_{1} \ldots i_{D}}$ in a D-dimensional space has only one independent component.
    
    \item \bluep{Prove that for symmetric $a^{i j}$ and antisymmetric $b_{i j}$ tensors the scalar internal product is zero $a^{i j} \cdot b_{i j}=0$}
\end{itemize}
\end{qt}
For the last point, we can use the following argument to prove it:
$$
a^{j i} b_{j i}=\left(+a^{i j}\right)\left(-b_{i j}\right)\substack{i\rightarrow j,j\rightarrow i\\=}-a^{i j} b_{i j}
$$
The quantity that equals itself with an opposite sign is zero.
\begin{example}
(i) Prove that if $b^{i j}(x)$ is a tensor and $a_{l}(x)$ and $c_{k}(x)$ are covariant vector fields, then $f(x)=b^{i j}(x) \cdot a_{i}(x) \cdot c_{j}(x)$ is a scalar field.

(ii) In case $a_{i}(x) \equiv c_{i}(x),$ formulate the sufficient condition for $b^{i j}(x),$ such that the product $f(x) \equiv 0$
\end{example}
\textbf{Solution:}

(i) $b^{i j}(x) \cdot a_{i}(x) \cdot c_{j}(x)=d^{ij}{}_{ij}(x)$, which is a scalar.

(ii) when $b$ is anti-symmetric: $b^{i j}=b^{[i j]}$

\subsubsection{Determinants}
Consider first $2 D$ and special coordinates $X^{1}$ and $X^{2}$ corresponding to the orthonormal basis $\hat{\mathbf{n}}_{1}$ and $\hat{\mathbf{n}}_{2}$. We first define a special maximal absolutely antisymmetric object in 2D using the relations:
$$
E_{a b}=-E_{b a} \quad \text { and } \quad E_{12}=1
$$
\begin{qt}
For and matrix $\left\|C_{b}^{a}\right\|$ we can write its determinant as
\begin{equation}
\operatorname{det}\left\|C_{b}^{a}\right\|=E_{a b} C_{1}^{a} C_{2}^{b}
\end{equation}
and
\begin{equation}
\operatorname{det}\left(C_{b}^{a}\right)=\frac{1}{2} E_{a b} E^{d e} C_{d}^{a} C_{e}^{b}
\end{equation}
The difference between these two equations is that the latter admits two expressions $C_{1}^{a} C_{2}^{b}$ and $C_{2}^{a} C_{1}^{b}$. Thus, we have the $1/2$ factor in the second equation.
\end{qt}
Because $E_{ab}$ is maximal absolutely anti-symmetric, we also have:
\begin{equation}
E_{a b} C_{e}^{a} C_{d}^{b}=-E_{a b} C_{e}^{b} C_{d}^{a}
\end{equation}
\textit{Proof}
$$
E_{a b} C_{e}^{a} C_{d}^{b}=E_{b a} C_{e}^{b} C_{d}^{a}=-E_{a b} C_{d}^{a} C_{e}^{b}
$$
Here in the first equality, we have exchanged the names of the umbral(dummy) indices $a \leftrightarrow b$ and in the second one used antisymmetry $E_{a b}=-E_{b a}$. \bluep{Now we consider an arbitrary dimension of space D.}
\begin{qt}
Consider $a, b=1,2,3, \ldots, D . \quad \forall$ matrix $\left\|C_{b}^{a}\right\|,$ the determinant is
\begin{equation}
\operatorname{det}\left\|C_{b}^{a}\right\|=E_{a_{1} a_{2} \dots a_{D}} \cdot C_{1}^{a_{1}} C_{2}^{a_{2}} \cdots C_{D}^{a_{D}}
\end{equation}
where $E_{a_1a_2\dots a_D}$ is absolutely antisymmetric and $E_{123\dots D}=1$. \redp{The determinant in the equation above is a sum of D! terms, each of which is a product of elements from different lines and columns, taken with the positive sign for even and with the negative sign for odd parity of permutations.}
\end{qt}
In general, the repression
\begin{equation}
\mathcal{A}_{a_{1} \ldots a_{D}}=E_{b_{1} \ldots b_{D}} A_{a_{1}}^{b_{1}} A_{a_{2}}^{b_{2}} \ldots A_{a_{D}}^{b_{D}}
\end{equation}
\bluep{is absolutely antisymmetric in the indices $\left\{a_{1}, \ldots, a_{D}\right\}$ and therefore is proportional to $E_{a_{1} \ldots a_{D}}$}.
\begin{qt}
In an arbitrary dimension D
\begin{equation}
\operatorname{det}\left\|C_{b}^{a}\right\|=\frac{1}{D !} E_{a_{1} a_{2} \ldots a_{D}} E^{b_{1} b_{2} \ldots b_{D}} \cdot C_{b_{1}}^{a_{1}} C_{b_{2}}^{a_{2}} \ldots C_{b_{D}}^{a_{D}}
\end{equation}
and
\begin{equation}
E^{a_{1} a_{2} \ldots a_{D}} \cdot E_{b_{1} b_{2} \ldots b_{D}}=\left|\begin{array}{cccc}
{\delta_{b_{1}}^{a_{1}}} & {\delta_{b_{2}}^{a_{1}}} & {\ldots}&{ \delta_{b_{D}}^{a_{1}}} \\
{\delta_{b_{1}}^{a_{2}}} & {\delta_{b_{2}}^{a_{2}}} & {\ldots} & {\delta_{b_{D}}^{a_{2}}} \\
{\ldots} & {\cdots} & {\cdots} & {\ldots} \\
{\delta_{b_{1}}^{a_{D}}} & {\ldots} & {\cdots} & {\delta_{b_{D}}^{a_{D}}}
\end{array}\right|
\end{equation}
\end{qt}
Following the theorem above, we have for the special dimension 3D:
$$
E^{a b c} E_{d e f}=\left|\begin{array}{ccc}
{\delta_{d}^{a}} & {\delta_{e}^{a}} & {\delta_{f}^{a}} \\
{\delta_{d}^{b}} & {\delta_{e}^{b}} & {\delta_{f}^{b}} \\
{\delta_{d}^{c}} & {\delta_{e}^{c}} & {\delta_{f}^{c}}
\end{array}\right|
$$
Now, let's contract the indices $c,f$:
\begin{qt}
\begin{equation}
E^{a b c} E_{d e c}=\left|\begin{array}{ccc}
{\delta_{d}^{a}} & {\delta_{e}^{a}} & {\delta_{c}^{a}} \\
{\delta_{d}^{b}} & {\delta_{e}^{b}} & {\delta_{c}^{b}} \\
{\delta_{d}^{c}} & {\delta_{e}^{c}} & {3}
\end{array}\right|=\delta_{d}^{a} \delta_{e}^{b}-\delta_{e}^{a} \delta_{d}^{b}
\end{equation}
\end{qt}
The last formula is sometimes called magic, as it is an extremely useful tool for the calculations in analytic geometry and vector calculus. Let us proceed and contract indices b and e. We obtain
$$
E^{a b c} E_{d b c}=3 \delta_{d}^{a}-\delta_{d}^{a}=2 \delta_{d}^{a}
$$
Finally, contracting the last remaining couple of indices, we arrive at
$$
E^{a b c} E_{a b c}=6
$$
In general
\begin{qt}
\begin{equation}
E^{a_{1} a_{2} \ldots a_{D}} E_{a_{1} a_{2} \ldots a_{D}}=D !
\end{equation}
Because for the first index, we have $D$ choices, for the second $D-1$ choices, for the third $D-2$ choices, etc. 
\end{qt}
\begin{example}
Using definition of the maximal antisymmetric symbol $E^{a_{1} a_{2} \ldots a_{D}}$ prove the rule for the product of matrix determinants:
$$
\operatorname{det}(A \cdot B)=\operatorname{det} A \cdot \operatorname{det} B
$$
Here both $A=\left\|a_{k}^{i}\right\|$ and $B=\left\|b_{k}^{i}\right\|$ are $n \times n$ matrices.
\end{example}
\textbf{Solution:}
$$
\operatorname{det}(A \cdot B)=E_{i_{1} i_{2} \ldots i_{D}}(A \cdot B)_{1}^{i_{1}}(A \cdot B)_{2}^{i_{2}} \ldots(A \cdot B)_{D}^{i_{D}}
$$
expand, we have
$$
\operatorname{det}(A \cdot B)=E_{i_{1} i_{2} \ldots i_{D}} a_{k_{1}}^{i_{1}} a_{k_{2}}^{i_{2}} \ldots a_{k_{D}}^{i_{D}} b_{1}^{k_{1}} b_{2}^{k_{2}} \ldots b_{D}^{k_{D}}
$$
According to (5.11.8), we have
$$
E_{i_{1} i_{2} \ldots i_{D}} a_{k_{1}}^{i_{1}} a_{k_{2}}^{i_{2}} \ldots a_{k_{D}}^{i_{D}}=E_{k_{1} k_{2} \ldots k_{D}} \cdot \operatorname{det} A
$$
Therefore,
$$
\operatorname{det}(A \cdot B)=\operatorname{det} A E_{k_{1} k_{2} \ldots k_{D}} b_{1}^{k_{1}} b_{2}^{k_{2}} \ldots b_{D}^{k_{D}}
$$
Finally, let's consider two more statements
\begin{lemma}
For any nondegenerate $D \times D$ matrix $M_{b}^{a}$ with the determinant $M,$ the elements of the inverse matrix $\left(M^{-1}\right)_{c}^{b}$ are given by the expressions
\begin{equation}
\left(M^{-1}\right)_{a}^{b}=\frac{1}{M(D-1) !} E_{a a_{2} \ldots a_{D}} E^{b b_{2} \ldots b_{D}} M_{b_{2}}^{a_{2}} M_{b_{3}}^{a_{3}} \ldots M_{b_{D}}^{a_{D}}
\end{equation}
\end{lemma}
Consider the nondegenerate $D \times D$ matrix $\left\|M_{b}^{a}\right\|$ with the elements $M_{b}^{a}=M_{b}^{a}(\kappa)$ being functions of some parameter $\kappa .$In general, the determinant of this matrix $M=\operatorname{det}\left\|M_{b}^{a}\right\|$ also depends on $\kappa .$ Suppose all functions $M_{b}^{a}(\kappa)$ are differentiable. Then the derivative of the determinant equals
\begin{qt}
\begin{equation}
\dot{M}=\frac{d M}{d \kappa}=M\left(M^{-1}\right)_{a}^{b} \frac{d M_{b}^{a}}{d \kappa}
\end{equation}
\end{qt}
where $\left(M^{-1}\right)_{a}^{b}$ are the elements of the matrix imverse to $\left(M_{b}^{a}\right)$ and the dot over $a$ function indicates its derivative with respect to $\kappa$.

\subsection{Applications to Vector Algebra}
Let's come back to the following definition for a \textbf{special object}:
$$
E^{a b c} E_{d e c}=\left|\begin{array}{ccc}
{\delta_{d}^{a}} & {\delta_{e}^{a}} & {\delta_{c}^{a}} \\
{\delta_{d}^{b}} & {\delta_{e}^{b}} & {\delta_{c}^{b}} \\
{\delta_{d}^{c}} & {\delta_{e}^{c}} & {3}
\end{array}\right|=\delta_{d}^{a} \delta_{e}^{b}-\delta_{e}^{a} \delta_{d}^{b}
$$
We first working on a special orthonormal basis $\hat{\mathbf{n}}_{a}=(\hat{\mathbf{i}}, \hat{\mathbf{j}}, \hat{\mathbf{k}})$, corresponding to the Cartesian coordinates $X^a$. In a vector product notations, we have
$$
\begin{aligned}
&\left[\hat{\mathbf{n}}_{1}, \hat{\mathbf{n}}_{2}\right]=\hat{\mathbf{n}}_{1} \times \hat{\mathbf{n}}_{2}=\hat{\mathbf{n}}_{3}\\
&\left[\hat{\mathbf{n}}_{2}, \hat{\mathbf{n}}_{3}\right]=\hat{\mathbf{n}}_{2} \times \hat{\mathbf{n}}_{3}=\hat{\mathbf{n}}_{1}\\
&\left[\hat{\mathbf{n}}_{3}, \hat{\mathbf{n}}_{1}\right]=\hat{\mathbf{n}}_{3} \times \hat{\mathbf{n}}_{1}=\hat{\mathbf{n}}_{2}
\end{aligned}
$$
Using the previously defined $E^{abc}$, we can write this in a more compact way:
\begin{equation}
\left[\hat{\mathbf{n}}_{a}, \hat{\mathbf{n}}_{b}\right]=E_{a b c} \cdot \hat{\mathbf{n}}^{c}
\end{equation}
For orthonormal basis, $\hat{\mathbf{n}}_{a}=\hat{\mathbf{n}}^{a}$ and $E^{a b c}=E_{a b c}$. In general, if we consider two vectors $\mathbf{V}=V^{a} \hat{\mathbf{n}}_{a}$ and $\mathbf{W}=W^{a} \hat{\mathbf{n}}_{a}$, then
\redp{$$
[\mathbf{V}, \mathbf{W}]=E_{a b c} \cdot V^{a} \cdot W^{b} \cdot \hat{\mathbf{n}}^{c}
$$}
Using this object, we can solve many problems of vector algebra:
\begin{example}
Consider the mixed product of the three vectors
$$
(\mathbf{U}, \mathbf{V}, \mathbf{W})=(\mathbf{U},[\mathbf{V}, \mathbf{W}])=U^{a} \cdot[\mathbf{V}, \mathbf{W}]_{a}
$$
$$
=U^{a} \cdot E_{a b c} V^{b} W^{c}=E_{a b c} U^{a} V^{b} W^{c}=\left|\begin{array}{ccc}
{U^{1}} & {U^{2}} & {U^{3}} \\
{V^{1}} & {V^{2}} & {V^{3}} \\
{W^{1}} & {W^{2}} & {W^{3}}
\end{array}\right|
$$
\end{example}
The similar arguments can be made to prove the following properties of the \textbf{mixed product}:
\begin{qt}
(i) Cyclic identity: $(\mathbf{U}, \mathbf{V}, \mathbf{W})=(\mathbf{W}, \mathbf{U}, \mathbf{V})=(\mathbf{V}, \mathbf{W}, \mathbf{U})$

(ii) Antisymmetry: $(\mathbf{U}, \mathbf{V}, \mathbf{W})=-(\mathbf{V}, \mathbf{U}, \mathbf{W})=-(\mathbf{U}, \mathbf{W}, \mathbf{V})$
\end{qt}
\begin{example}
Prove $[\mathbf{U}, \mathbf{V}] \times[\mathbf{W}, \mathbf{Y}]=\mathbf{V} \cdot(\mathbf{U}, \mathbf{W}, \mathbf{Y})-\mathbf{U} \cdot(\mathbf{V}, \mathbf{W}, \mathbf{Y})$
$$
([\mathbf{U}, \mathbf{V}] \times[\mathbf{W}, \mathbf{Y}])_{a}=E_{a b c}[\mathbf{U}, \mathbf{V}]^{b}[\mathbf{W}, \mathbf{Y}]^{c}=E_{a b c} E^{b d e} U_{d} V_{e} E^{c f g} W_{f} Y_{g}
$$
$$
=-E_{b a c} E^{b d e} E^{c f g} U_{d} V_{e} W_{f} Y_{g}=-\left(\delta_{a}^{d} \delta_{c}^{e}-\delta_{a}^{e} \delta_{c}^{d}\right) E^{c f g} U_{d} V_{e} W_{f} Y_{g}
$$
$$
=E^{c f g}\left(U_{c} V_{a} W_{f} Y_{g}-U_{a} V_{c} W_{f} Y_{g}\right)=V_{a}(\mathbf{U}, \mathbf{W}, \mathbf{Y})-U_{a}(\mathbf{V}, \mathbf{W}, \mathbf{Y})
$$
\end{example}
Some more identities
\begin{qt}
$$
[[\mathbf{U}, \mathbf{V}],[\mathbf{W}, \mathbf{Y}]]=\mathbf{W} \cdot(\mathbf{Y}, \mathbf{U}, \mathbf{V})-\mathbf{Y} \cdot(\mathbf{W}, \mathbf{U}, \mathbf{V})
$$
$$
\mathbf{V}(\mathbf{W}, \mathbf{Y}, \mathbf{U})-\mathbf{U}(\mathbf{W}, \mathbf{Y}, \mathbf{V})=\mathbf{W}(\mathbf{U}, \mathbf{V}, \mathbf{Y})-\mathbf{Y}(\mathbf{U}, \mathbf{V}, \mathbf{W})
$$
$$
[\mathbf{U} \times \mathbf{V}] \cdot[\mathbf{W} \times \mathbf{Y}]=(\mathbf{U} \cdot \mathbf{W})(\mathbf{V} \cdot \mathbf{Y})-(\mathbf{U} \cdot \mathbf{Y})(\mathbf{V} \cdot \mathbf{W})
$$
$$
[\mathbf{A},[\mathbf{B}, \mathbf{C}]]=\mathbf{B}(\mathbf{A}, \mathbf{C})-\mathbf{C}(\mathbf{A}, \mathbf{B})
$$
$$
[\mathbf{U},[\mathbf{V}, \mathbf{W}]]+[\mathbf{W},[\mathbf{U}, \mathbf{V}]]+[\mathbf{V},[\mathbf{W}, \mathbf{U}]]=0
$$
\end{qt}
Now, let's construct a tensor version of the object $E_{abc}$. To do so, one has to use the \textbf{transformation rule for the covariant tensor of the third rank,starting from the special Cartesian coordinates $X^a$}:
\begin{equation}
\varepsilon_{i j k}=\frac{\partial X^{a}}{\partial x^{i}} \frac{\partial X^{b}}{\partial x^{j}} \frac{\partial X^{c}}{\partial x^{k}} E_{a b c}
\end{equation}
\begin{qt}
The component $\varepsilon_{123}$ is a square root of the metric determinant,
\begin{equation}
    \varepsilon_{123}=g^{1 / 2}, \quad \text { where } g=\operatorname{det}\left\|g_{i j}\right\|
    \label{eps123-!}
\end{equation}
or
\begin{equation}
    \varepsilon_{123}=\operatorname{det}\left\|\frac{\partial X^{a}}{\partial x^{i}}\right\|
    \label{eps123}
\end{equation}
while we can use (\ref{eps123}) without much thought, we need to be careful using (\ref{eps123-!}). The reason is that \redp{the coordinate transformations that break parity can change the sign in the r.h.s. in Eq.(\ref{eps123-!})}
\end{qt}
Despite $E_{i j k}$ is not being a tensor, we can easily control its transformation from one coordinate system to another:
$$
E_{i j k}=\frac{1}{\sqrt{g}} \varepsilon_{i j k}
$$
$E_{i j k}$ is a particular example of objects which are called \textbf{tensor densities}.
\begin{defi}
        The quantity $A_{i_{1} \ldots i_{n}}{}^{j_{1} \ldots j_{n}}$ is a tensor density of the $(m, n)$ -type with the weight $r,$ if the quantity
        $$
g^{-r / 2} \cdot A_{i_{1} \ldots i_{n}}{}^{j_{1} \ldots j_{m}}
$$
is a tensor.
\end{defi}
In general,
\begin{equation}
\varepsilon_{i_{1} i_{2} \cdots i_{D}}=\frac{\partial x^{a_{1}}}{\partial x^{i_{1}}} \frac{\partial x^{a_{2}}}{\partial x^{i_{2}}} \cdots \frac{\partial x^{a_{D}}}{\partial x^{i_{D}}} E_{a_{1} a_{2} \cdots a_{D}}
\end{equation}
Since
\begin{equation}
g_{i j}=\frac{\partial x^{a}}{\partial x^{i}} \frac{\partial x^{b}}{\partial x^{j}} \delta_{b}^{a}
\end{equation}
assuming that the orientation of axes is such that $\operatorname{det}\left(\frac{\partial x^{a}}{\partial x^{i}}\right)>0,$ we get
\begin{equation}
\operatorname{det}\left\|\frac{\partial x^{a}}{\partial x^{i}}\right\|=\sqrt{g}
\end{equation}
Then
\begin{qt}
\begin{equation}
\varepsilon_{123 \dots D}=\frac{\partial x^{a_{1}}}{\partial x^{1}} \frac{\partial x^{a_{2}}}{\partial x^{2}} \cdots \frac{\partial x^{a_{D}}}{\partial x^{D}} E_{a_{1} a_{2} \cdots a_{D}}=\operatorname{det}\left(\frac{\partial x^{a}}{\partial x^{i}}\right)=\sqrt{g}
\end{equation}
Similarly, $\varepsilon^{12 \cdots D}=\frac{1}{\sqrt{g}}$.
\end{qt}

\section{Curvilinear Coordinates, Local Coordinate Transformations}
So far, we have learned the following transformation rule for scalar field, vector, and tensor:
$$
\varphi^{\prime}\left(x^{\prime}\right)=\varphi(x)
$$
$$
a^{\prime i}\left(x^{\prime}\right)=\frac{\partial x^{i}}{\partial x^{j}} a^{j}(x)
$$
$$
b_{l}^{\prime}\left(x^{\prime}\right)=\frac{\partial x^{k}}{\partial x^{\prime l}} b_{k}(x)
$$
$$
A_{j^{\prime}}^{i^{\prime}}\left(x^{\prime}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{k}} \frac{\partial x^{l}}{\partial x^{\prime j}} A_{l}^{k}(x)
$$
and for metric tensor:
\begin{equation}
    g_{i j}(x)=\frac{\partial X^{a}}{\partial x^{i}} \frac{\partial X^{b}}{\partial x^{j}} g_{a b}
    \label{metric-orthonormal}
\end{equation}
\begin{equation}
    g^{i j}=\frac{\partial x^{i}}{\partial X^{a}} \frac{\partial x^{j}}{\partial X^{b}} \delta^{a b}
\end{equation}


where $g_{a b}=\delta_{a b}$ is a metric in Cartesian coordinates.

It is so easy to generalize the notion of tensor and algebraic operations over tensors, \textbf{because all these operations are defined in the same point of the space.} Thus, the main difference between general coordinate transformation $x^{\prime \alpha}=x^{\prime \alpha}(x)$ and the special one $x^{\prime \alpha}=\wedge_{\beta}^{\alpha^{\prime}} x^{\beta}+B^{\alpha^{\prime}}$ with $\wedge_{\beta}^{\alpha^{\prime}}=$ const and $B^{\alpha^{\prime}}=$ const is that \textbf{in the general case, the transition coefficients $\partial x^{i} / \partial x^{\prime j}$ are not necessary constants.}

One of important consequences is that \bluep{the metric tensor $g_{i j}$ also depends on the point. Additionally, the antisymmetric tensor $\varepsilon^{i j k}$ also depends on the coordinates}.

\subsection{Polar coordinates on the plane}
Our purpose in this section is to learn how to transform an arbitrary tensor to polar coordinates. From $(x,y)$ to $(r,\varphi)$, we have
$$
\begin{array}{l}
{\mathbf{e}_{r}=\frac{\partial x}{\partial r} \hat{\mathbf{n}}_{x}+\frac{\partial y}{\partial r} \hat{\mathbf{n}}_{y}=\hat{\mathbf{i}} \cos \varphi+\hat{\mathbf{j}} \sin \varphi} \\
{\mathbf{e}_{\varphi}=\frac{\partial x}{\partial \varphi} \hat{\mathbf{n}}_{x}+\frac{\partial y}{\partial \varphi} \hat{\mathbf{n}}_{y}=r(-\hat{\mathbf{i}} \sin \varphi+\hat{\mathbf{j}} \cos \varphi)}
\end{array}
$$
\textbf{Because these basis vectors are orthogonal,} the metric in polar coordinates is diagonal
\begin{equation}
g_{i j}=\left(\begin{array}{ll}
{g_{r r}} & {g_{r \varphi}} \\
{g_{\varphi r}} & {g_{\varphi \varphi}}
\end{array}\right)=\left(\begin{array}{cc}
{\mathbf{e}_{r} \cdot \mathbf{e}_{r}} & {\mathbf{e}_{r} \cdot \mathbf{e}_{\varphi}} \\
{\mathbf{e}_{\varphi} \cdot \mathbf{e}_{r}} & {\mathbf{e}_{\varphi} \cdot \mathbf{e}_{\varphi}}
\end{array}\right)=\left(\begin{array}{cc}
{1} & {0} \\
{0} & {r^{2}}
\end{array}\right)
\end{equation}
The first observation is that the basic vectors $\mathbf{e}_{r}$ and $\mathbf{e}_{\varphi}$ are orthogonal $\mathbf{e}_{r} \cdot \mathbf{e}_{\varphi}=0 .$ As a result, the metric in polar coordinates is diagonal. Again, \redp{this metric tensor is w.r.t. the coordinate basis, not w.r.t. orthonormalized vector basis}. To get metrix tensor w.r.t. orthonormalized vector, we can simply use Eq.(\ref{metric-orthonormal}).

In what follows we shall mark the components of the vector in this basis by tilde. The simpler notations without tilde are always reserved for the components of the vector in the normalized basis. Generally, \bluep{using the normalized basis means that all components of the vector have the same dimension.}

\begin{example}
For the polar coordinates on the 2D plane, find the metric by performing transformation of the metric in Cartesian coordinates
\end{example}
\textbf{Solution}:
$$
g_{a b}=\delta_{a b}, \quad \text { that is } \quad g_{x x}=1=g_{y y}, \quad g_{x y}=g_{y x}=0
$$
$$
\begin{aligned}
g_{\varphi \varphi} &=\frac{\partial x}{\partial \varphi} \frac{\partial x}{\partial \varphi} g_{x x}+\frac{\partial x}{\partial \varphi} \frac{\partial y}{\partial \varphi} g_{x y}+\frac{\partial y}{\partial \varphi} \frac{\partial x}{\partial \varphi} g_{y x}+\frac{\partial y}{\partial \varphi} \frac{\partial y}{\partial \varphi} g_{y y}=\\
&=\left(\frac{\partial x}{\partial \varphi}\right)^{2}+\left(\frac{\partial y}{\partial \varphi}\right)^{2}=r^{2} \sin ^{2} \varphi+r^{2} \cos ^{2} \varphi=r^{2}
\end{aligned}
$$
and
$$
g_{r r}=\frac{\partial x}{\partial r} \frac{\partial x}{\partial r} g_{x x}+\frac{\partial y}{\partial r} \frac{\partial y}{\partial r} g_{y y}=\left(\frac{\partial x}{\partial r}\right)^{2}+\left(\frac{\partial y}{\partial r}\right)^{2}=\cos ^{2} \varphi+\sin ^{2} \varphi=1
$$
Thus, the metric is
$$
\left(\begin{array}{l}
{g_{r r} g_{r \varphi}} \\
{g_{\varphi r} g_{\varphi \varphi}}
\end{array}\right)=\left(\begin{array}{ll}
{1} & {0} \\
{0} & {r^{2}}
\end{array}\right)
$$
\bluep{This derivation of the metric has a simple geometric interpretation. Consider two points that have infinitesimally close $\varphi \text { and } r.$ The distance between a these two points $d s$ is defined by the relation}
$$
\begin{aligned}
d s^{2} &=d x^{2}+d y^{2}=g_{a b} d X^{a} d X^{b}=g_{i j} d x^{i} d x^{j}=d r^{2}+r^{2} d \varphi^{2} \\
&=g_{r r} d r d r+g_{\varphi \varphi} d \varphi d \varphi+2 g_{\varphi r} d \varphi d r
\end{aligned}
$$
\begin{qt}
\redp{The tensor form of the transformation of the metric corresponds to the
coordinate-independent distance between two infinitesimally close points}
\end{qt}

\subsection{Cylindrical and Spherical Coordinates}
 Cylindrical coordinates in 3D are defined by the relations:
 $$
\begin{aligned}
x=r \cos \varphi, & y=r \sin \varphi, \quad z=z \\
\text { where } \quad 0 \leq r<\infty, & 0 \leq \varphi<2 \pi \quad \text { and } \quad-\infty<z<\infty
\end{aligned}
$$
The basic vectors are
$$
\mathbf{e}_{r}=\hat{\mathbf{i}} \cos \varphi+\hat{\mathbf{j}} \sin \varphi, \quad \mathbf{e}_{\varphi}=-\hat{\mathbf{i}} r \sin \varphi+\hat{\mathbf{j}} r \cos \varphi, \quad \mathbf{e}_{z}=\hat{\mathbf{k}}
$$
and the metric tensor is
\begin{equation}
g_{i j}=\left(\begin{array}{l}
{g_{r r} g_{r \varphi} g_{r z}} \\
{g_{\varphi r} g_{\varphi \varphi} g_{\varphi z}} \\
{g_{z r} g_{z \varphi} g_{z z}}
\end{array}\right)=\left(\begin{array}{ccc}
{1} & {0} & {0} \\
{0} & {r^{2}} & {0} \\
{0} & {0} & {1}
\end{array}\right)
\end{equation}

For spherical coordinates, we have
the following relation with Cartesian coordiantes
$$
\begin{aligned}
x=r \cos \varphi \sin \chi, & y=r \sin \varphi \sin \chi, \quad z=r \cos \chi \\
\text { where } \quad 0 \leq r<\infty, & 0 \leq \varphi<2 \pi \quad \text { and } \quad 0 \leq \chi \leq \pi
\end{aligned}
$$
The basic vector is then
$$
\begin{aligned}
&\mathbf{e}_{r}=\hat{\mathbf{i}} \cos \varphi \sin \chi+\hat{\mathbf{j}} \sin \varphi \sin \chi+\hat{\mathbf{k}} \cos \chi\\
&\mathbf{e}_{x}=\hat{\mathbf{i}} r \cos \varphi \cos \chi+\hat{\mathbf{j}} r \sin \varphi \cos \chi-\hat{\mathbf{k}} r \sin \chi\\
&\mathbf{e}_{\varphi}=-\hat{\mathbf{i}} r \sin \varphi \sin \chi+\hat{\mathbf{j}} r \cos \varphi \sin \chi
\end{aligned}
$$
with the metric tensor as
$$
g_{i j}=\left(\begin{array}{lll}
{g_{r r}} & {g_{r x}} & {g_{r \varphi}} \\
{g_{\chi r}} & {g_{\chi x}} & {g_{x \varphi}} \\
{g_{\varphi r}} & {g_{\varphi x}} & {g_{\varphi \varphi}}
\end{array}\right)=\left(\begin{array}{ccc}
{1} & {0} & {0} \\
{0} & {r^{2}} & {0} \\
{0} & {0} & {r^{2} \sin ^{2} x}
\end{array}\right)
$$
And the normalized basis is
\begin{equation}
    \begin{aligned}
&\hat{\mathbf{n}}_{r}=\hat{\mathbf{i}} \cos \varphi \sin \chi+\hat{\mathbf{j}} \sin \varphi \sin \chi+\hat{\mathbf{k}} \cos \chi\\
&\hat{\mathbf{n}}_{\varphi}=-\hat{\mathbf{i}} \sin \varphi+\hat{\mathbf{j}} \cos \varphi\\
&\hat{\mathbf{n}}_{\chi}=\hat{\mathbf{i}} \cos \varphi \cos \chi+\hat{\mathbf{j}} \sin \varphi \cos \chi-\hat{\mathbf{k}} \sin \chi
\end{aligned}
\label{unitvec-spherical}
\end{equation}
Now let us derive the expression for the velocity and acceleration in 3D for the case of the spherical coordinates. The starting point is Eq.(\ref{unitvec-spherical}). A simple calculus gives the following result for the first derivatives:
$$
\begin{aligned}
\dot{\mathbf{n}}_{r} &=\dot{\varphi} \sin \chi(-\hat{\mathbf{i}} \sin \varphi+\hat{\mathbf{j}} \cos \varphi) \\
&+\dot{\chi}(\hat{\mathbf{i}} \cos \varphi \cos \chi+\hat{\mathbf{j}} \sin \varphi \cos \chi-\hat{\mathbf{k}} \sin \chi) \\
\dot{\mathbf{n}}_{\varphi} &=-\dot{\varphi}(\hat{\mathbf{i}} \cos \varphi+\hat{\mathbf{j}} \sin \varphi) \\
\dot{\mathbf{n}}_{\chi} &=\dot{\varphi} \cos \chi(-\hat{\mathbf{i}} \sin \varphi+\hat{\mathbf{j}} \cos \varphi) \\
&-\dot{\chi}(\hat{\mathbf{i}} \sin \chi \cos \varphi+\hat{\mathbf{j}} \sin \chi \sin \varphi+\hat{\mathbf{k}} \cos \chi)
\end{aligned}
$$
Next, we need to perform an inverse transformation from the Cartesian basis to the new basis:
$$
\begin{aligned}
&\hat{\mathbf{i}}=\frac{\partial r}{\partial x} \mathbf{e}_{r}+\frac{\partial \varphi}{\partial x} \mathbf{e}_{\varphi}+\frac{\partial \chi}{\partial x} \mathbf{e}_{x}\\
&\hat{\mathbf{j}}=\frac{\partial r}{\partial y} \mathbf{e}_{r}+\frac{\partial \varphi}{\partial y} \mathbf{e}_{\varphi}+\frac{\partial \chi}{\partial y} \mathbf{e}_{\chi}\\
&\mathbf{k}=\frac{\partial r}{\partial z} \mathbf{e}_{r}+\frac{\partial \varphi}{\partial z} \mathbf{e}_{\varphi}+\frac{\partial \chi}{\partial z} \mathbf{e}_{\chi}
\end{aligned}
$$
Using these relations, one can derive the first derivatives of the vector $\hat{\mathbf{n}}_{r}, \hat{\mathbf{n}}_{\varphi}, \hat{\mathbf{n}}_{\chi}$ in the final form:
$$
\begin{aligned}
&\dot{\mathbf{n}}_{r}=\dot{\varphi} \sin \chi \hat{\mathbf{n}}_{\varphi}+\dot{\chi} \hat{\mathbf{n}}_{\chi}\\
&\dot{\mathbf{n}}_{\varphi}=-\dot{\varphi}\left(\sin \chi \hat{\mathbf{n}}_{r}+\cos \chi \hat{\mathbf{n}}_{\chi}\right)\\
&\dot{\mathbf{n}}_{\chi}=\dot{\varphi} \cos \chi \hat{\mathbf{n}}_{\varphi}-\dot{\chi} \hat{\mathbf{n}}_{r}
\end{aligned}
$$
Now the particle's velocity and acceleration are
\begin{equation}
\mathbf{v}=\dot{\mathbf{r}}=\frac{d}{d t}\left(r \hat{\mathbf{n}}_{r}\right)=\dot{r} \hat{\mathbf{n}}_{r}+r \dot{\chi} \hat{\mathbf{n}}_{\chi}+r \dot{\varphi} \sin \chi \hat{\mathbf{n}}_{\varphi}
\end{equation}
\begin{equation}
\begin{aligned}
\mathbf{a} &=\ddot{\mathbf{r}}=\left(\ddot{r}-r \dot{\chi}^{2}-r \dot{\varphi}^{2} \sin ^{2} \chi\right) \hat{\mathbf{n}}_{r}+(2 r \dot{\varphi} \dot{\chi} \cos \chi+2 \dot{\varphi} \dot{r} \sin \chi+r \ddot{\varphi} \sin \chi) \hat{\mathbf{n}}_{\varphi} \\
&+\left(2 \dot{r} \dot{\chi}+r \ddot{\chi}-r \dot{\varphi}^{2} \sin \chi \cos \chi\right) \hat{\mathbf{n}}_{\chi}
\end{aligned}
\end{equation}

One can also find the components of Levi-Civita tensor $\epsilon_{123}$ and $\epsilon^{123}$ for cylindric and spherical coordinates using the following relations:
\begin{equation}
g=\operatorname{det}\left\|g_{i j}\right\| \quad \text { and } \quad \operatorname{det}\left\|g^{i j}\right\|=\frac{1}{g}
\end{equation}
$$
\varepsilon^{123}=\frac{1}{\sqrt{g}}
$$
$$
\varepsilon_{123}=g^{1 / 2}
$$
For the \bluep{hyperbolic coordinates,} we have
$$
x=r \cdot \cosh \varphi, \quad y=r \cdot \sinh \varphi
$$
Since
$$
\cosh ^{2} \varphi-\sinh ^{2} \varphi=1 ; \quad \cosh ^{\prime} \varphi=\sinh \varphi ; \quad \sinh ^{\prime} \varphi=\cosh \varphi
$$
we find
$$
g_{r r}=\cosh 2 \varphi, \quad g_{\varphi \varphi}=r^{2} \cosh 2 \varphi, \quad g_{r \varphi}=r \sinh 2 \varphi
$$
\textbf{in this case the basis is not orthogonal.}
\section{Deivatives of Tensors,Covariant Derivatives}
Let us start from a scalar field $\varphi$. Consider its partial derivative
\begin{equation}
\partial_{i} \varphi=\varphi_{, i}=\frac{\partial \varphi}{\partial x^{i}}
\end{equation}
\redp{In the transformed coordinates $x^{i}=x^{i i}\left(x^{j}\right)$ we obtain, using the chain rule and $\varphi^{\prime}\left(x^{\prime}\right)=\varphi(x)$}
$$
\partial_{i^{\prime}} \varphi^{\prime}=\frac{\partial \varphi^{\prime}}{\partial x^{i^{\prime}}}=\frac{\partial x^{j}}{\partial x^{\prime i}} \frac{\partial \varphi}{\partial x^{j}}=\frac{\partial x^{j}}{\partial x^{\prime i}} \partial_{j} \varphi
$$
\textbf{The last formula shows that the partial derivative of a scalar field $\varphi_{, i}=\partial_{i} \varphi$ transforms as a covariant vector field. Certainly, there is no need to modify a partial derivative in this case.}

Now we introduce \redp{vector differential operator $\nabla$}. 
\begin{qt}
When acting on a scalar, $\nabla$ produces a covariant vector which is called gradient.
\end{qt}

\textbf{The next step is to consider a partial derivative of a covariant vector $b_i(x)$}.Let us make a corresponding transformation
\begin{qt}
\begin{equation}
\partial_{j^{\prime}} b_{i^{\prime}}=\frac{\partial}{\partial x^{\prime j}} b_{i^{\prime}}=\frac{\partial}{\partial x^{\prime j}}\left(\frac{\partial x^{k}}{\partial x^{\prime i}} b_{k}\right)=\frac{\partial x^{k}}{\partial x^{i^{\prime}}} \frac{\partial x^{l}}{\partial x^{\prime j}} \partial_{l} b_{k}+\frac{\partial^{2} x^{k}}{\partial x^{\prime j} \partial x^{\prime i}} b_{k}
\label{vector-grad}
\end{equation}
\end{qt}
The parenthesis contains a product of the two expressions that are functions of different coordinates $x^{\prime j}$ and $x^j$. \bluep{When the partial derivative $\partial / \partial x^{\prime j}$  acts on the function of the coordinates  $x^{i}$, it must be applied following the chain rule}
\begin{equation}
\frac{\partial}{\partial x^{\prime j}}=\frac{\partial x^{l}}{\partial x^{\prime j}} \frac{\partial}{\partial x^{l}}
\label{chain-rule}
\end{equation}
The last term in Eq.(\ref{vector-grad}) can be zero when $\partial x^{k} / \partial x^{\prime i}$ is constant. Then its derivatives are zeros and the gradient of a covariant vector is a tensor. But, \textbf{for a general case of curvilinear coordinates (e.g., polar in 2D or spherical
in 3D), the formula (\ref{vector-grad}) shows the non-tensor nature of the transformation.}

For contravariant vector $a^i(x)$ we have similar relations to (\ref{vector-grad}):
\begin{qt}
\begin{equation}
\partial_{k^{\prime}} a^{i^{\prime}}=\frac{\partial}{\partial x^{k^{\prime}}}\left(\frac{\partial x^{i^{\prime}}}{\partial x^{j}} a^{j}\right)=\frac{\partial x^{i^{\prime}}}{\partial x^{j}} \frac{\partial x^{l}}{\partial x^{k^{\prime}}} \partial_{l} a^{j}+\frac{\partial^{2} x^{i^{\prime}}}{\partial x^{j} \partial x^{l}} \frac{\partial x^{l}}{\partial x^{k^{\prime}}} a^{j}
\end{equation}
\textbf{Remember, $\frac{\partial x^{i^{\prime}}}{\partial x^{j}}$ is a function of $x^j$ and chain rule (\ref{chain-rule}) must apply here.}
\end{qt}
Similarly, for mixed tensor $T_i^j(x)$
\begin{qt}
\begin{equation}
\begin{aligned}
\partial_{k^{\prime}} T_{j^{\prime}}^{i^{\prime}} &=\frac{\partial}{\partial x^{k^{\prime}}}\left(\frac{\partial x^{i^{\prime}}}{\partial x^{l}} \frac{\partial x^{m}}{\partial x^{j^{\prime}}} T_{m}^{l}\right) \\
&=\frac{\partial x^{n}}{\partial x^{k^{\prime}}} \frac{\partial x^{i^{\prime}}}{\partial x^{l}} \frac{\partial x^{m}}{\partial x^{j^{\prime}}} \partial_{n} T_{m}^{l}+\frac{\partial^{2} x^{i^{\prime}}}{\partial x^{n} \partial x^{l}} \frac{\partial x^{n}}{\partial x^{k^{\prime}}} \frac{\partial x^{m}}{\partial x^{j^{\prime}}} T_{m}^{l}+\frac{\partial x^{i^{\prime}}}{\partial x^{l}} \frac{\partial^{2} x^{m}}{\partial x^{k^{\prime}} \partial x^{j^{\prime}}} T_{m}^{l}
\end{aligned}
\end{equation}
\end{qt}

We now construct a covariant derivative of a tensor.
\begin{defi}
        The covariant derivative $\nabla_i$ satisfies the following two conditions:
        \begin{itemize}
            \item Tensor transformation rule when the new derivative is applied to a tensor
            \item In the cartesian coordiantes, $\{X^a\}$ the covariant derivative coincides with the usual partial derivative
            $$
\nabla_{a}=\partial_{a}=\frac{\partial}{\partial X^{a}}
$$
        \end{itemize}
\end{defi}

Let's now consider an arbitrary tensor, say mixed (1,1)-type one $W_i^j$. In order to define its covariant derivative, we perform the following steps:
1. Transform it to Cartesian coordinates
$$
W_{b}^{a}=\frac{\partial X^{a}}{\partial x^{i}} \frac{\partial x^{j}}{\partial X^{b}} W_{j}^{i}
$$
2. Take the partial derivative w.r.t. the Cartesian coordinates $X^c$
$$
\partial_{c} W_{b}^{a}=\frac{\partial}{\partial X^{c}}\left(\frac{\partial X^{a}}{\partial x^{i}} \frac{\partial x^{j}}{\partial X^{b}} W_{j}^{i}\right)
$$
3. Transform this derivative back to the original coordinates $x^i$
$$
\nabla_{k} W_{m}^{l}=\frac{\partial X^{c}}{\partial x^{k}} \frac{\partial x^{l}}{\partial X^{a}} \frac{\partial X^{b}}{\partial x^{m}} \partial_{c} W_{b}^{a}
$$
By construction, \textbf{the covariant derivative follows the Leibnitz rule for the product of two tensor A and B}:
\begin{equation}
\nabla_{i}(A \cdot B)=\nabla_{i} A \cdot B+A \cdot \nabla_{i} B
\label{leibnitz-rule}
\end{equation}
\begin{example}
Let us make an explicit calculation for the covariant vector $T_i$:
$$
\begin{aligned}
\nabla_{i} T_{j} &=\frac{\partial X^{a}}{\partial x^{i}} \frac{\partial X^{b}}{\partial x^{j}}\left(\partial_{a} T_{b}\right)=\frac{\partial X^{a}}{\partial x^{i}} \frac{\partial X^{b}}{\partial x^{j}} \cdot \frac{\partial}{\partial X^{a}}\left(\frac{\partial x^{k}}{\partial X^{b}} T_{k}\right) \\
&=\frac{\partial X^{a}}{\partial x^{i}} \frac{\partial X^{b}}{\partial x^{j}} \frac{\partial x^{k}}{\partial X^{b}} \frac{\partial x^{l}}{\partial X^{a}} \frac{\partial T_{k}}{\partial x^{l}}+\frac{\partial X^{a}}{\partial x^{i}} \frac{\partial X^{b}}{\partial x^{j}} T_{k} \frac{\partial^{2} x^{k}}{\partial X^{a} \partial X^{b}} \\
&=\partial_{i} T_{j}-\Gamma_{j i}^{k} T_{k}
\end{aligned}
$$
The last equivalence holds because we have the following relation for Kronecker symbol
$$
\frac{\partial X^{a}}{\partial x^{i}}\frac{\partial x^{l}}{\partial X^{a}}=\delta^l_i
$$
\end{example}
\begin{qt}
\begin{equation}
\Gamma_{j i}^{k}=-\frac{\partial X^{a}}{\partial x^{i}} \frac{\partial X^{b}}{\partial x^{j}} \frac{\partial^{2} x^{k}}{\partial X^{a} \partial X^{b}}
\end{equation}
\end{qt}
\begin{thm}
\textbf{Suppose the elements of the matrix $\wedge$ depend on the parameter $\kappa$ and $\wedge(\kappa)$ is a differentiable and invertible matrix for $\kappa\in(a, b) .$ Within the region $(a, b)$ the inverse matrix $\wedge^{-1}(\kappa)$ is also differentiable and its derivative is}
\begin{equation}
\frac{d \wedge^{-1}}{d \kappa}=-\wedge^{-1} \frac{\partial \wedge}{\partial \kappa} \wedge^{-1}
\label{wedge-theorem}
\end{equation}
\end{thm}
\textbf{Proof:} from $\wedge \cdot \wedge^{-1}=I$ we obtain
$$
\frac{d \wedge}{d \kappa} \wedge^{-1}+\wedge \frac{\partial \wedge^{-1}}{\partial \kappa}=0
$$
After multiplying this equation by $\wedge^{-1}$ from the left, we arrive at the theorem above. 

Consider now that
$$
\wedge_{k}^{b}=\frac{\partial X^{b}}{\partial x^{k}}
$$
and its inverse matrix is
$$
\left(\wedge^{-1}\right)_{a}^{k}=\frac{\partial x^{k}}{\partial X^{a}}
$$
Using (\ref{wedge-theorem}) with $x^{i}$ playing the role of parameter $\kappa,$ we arrive at
\begin{equation}
\begin{aligned}
&\frac{\partial^{2} X^{b}}{\partial x^{i} \partial x^{k}}=\frac{\partial}{\partial x^{i}} \wedge_{k}^{b}=-\wedge_{l}^{b} \frac{\partial\left(\wedge^{-1}\right)_{a}^{l}}{\partial x^{i}} \wedge_{k}^{a}\\
&=-\frac{\partial X^{b}}{\partial x^{l}}\left(\frac{\partial X^{c}}{\partial x^{i}} \frac{\partial}{\partial X^{c}} \frac{\partial x^{l}}{\partial X^{a}}\right) \frac{\partial X^{a}}{\partial x^{k}}=-\frac{\partial X^{b}}{\partial x^{l}} \frac{\partial X^{a}}{\partial x^{k}} \frac{\partial X^{c}}{\partial x^{i}} \frac{\partial^{2} x^{l}}{\partial X^{c} \partial X^{a}}
\end{aligned}
\end{equation}
Use the equation above, we find another expression for $\Gamma_{ki}^j$:
\begin{qt}
\begin{equation}
\Gamma_{k i}^{j}=\frac{\partial x^{j}}{\partial X^{b}} \frac{\partial^{2} X^{b}}{\partial x^{i} \partial x^{k}}=-\frac{\partial^{2} x^{j}}{\partial X^{b} \partial X^{a}} \frac{\partial X^{a}}{\partial x^{k}} \frac{\partial X^{b}}{\partial x^{i}}
\label{gamma-defi}
\end{equation}
\end{qt}
By making direct calculation and using the equation above, we have:
\begin{equation}
\nabla_{i} S^{j}=\partial_{i} S^{j}+\Gamma_{k i}^{j} S^{k}
\end{equation}
\begin{equation}
\nabla_{i} W_{k}^{j}=\partial_{i} W_{k}^{j}+\Gamma_{l i}^{j} W_{k}^{l}-\Gamma_{k i}^{l} W_{l}^{j}
\end{equation}
Using the definition of $\Gamma_{ij}^k$, we can \textbf{formulate the general rule for constructing a covariant derivative of an arbitrary tensor,}
\begin{qt}
\begin{equation}
    \begin{split}
    \nabla_iT^{j_1j_2\dots}{}_{k_1k_2\dots}&=\partial_iT^{j_1j_2\dots}{}_{k_1k_2\dots}+\Gamma^{j_1}_{li}T^{lj_2\dots}{}_{k_1k_2\dots}+\Gamma^{j_2}_{li}T^{j_1l\dots}{}_{k_1k_2\dots}\\
    &-\Gamma^{l}_{k_1i}T^{j_1j_2\dots}{}_{k_1l\dots}-\Gamma^{l}_{k_2i}T^{j_1j_2\dots}{}_{lk_2\dots}-\dots
\end{split}
\end{equation}
\end{qt}
According to the definition of \redp{affine connection} (\ref{gamma-defi}), we can derive the transformation rule for $\Gamma^i_{kj}$ as:
$$
\Gamma_{m^{\prime} n^{\prime}}^{l^{\prime}}=\frac{\partial x^{l^{\prime}}}{\partial X^{a}} \frac{\partial^{2} X^{a}}{\partial x^{m^{\prime}} \partial x^{n^{\prime}}}=\frac{\partial x^{l^{\prime}}}{\partial x^{i}} \frac{\partial x^{i}}{\partial X^{a}} \frac{\partial}{\partial x^{m^{\prime}}}\left[\frac{\partial X^{a}}{\partial x^{j}} \frac{\partial x^{j}}{\partial x^{n^{\prime}}}\right]
$$
$$
=\Gamma_{k j}^{i} \frac{\partial x^{l^{\prime}}}{\partial x^{i}} \frac{\partial x^{k}}{\partial x^{m^{\prime}}} \frac{\partial x^{j}}{\partial x^{n^{\prime}}}+\frac{\partial x^{l^{\prime}}}{\partial x^{i}} \frac{\partial^{2} x^{i}}{\partial x^{m^{\prime}} \partial x^{n^{\prime}}}
$$
Thus,
\begin{qt}
\begin{equation}
\Gamma_{j^{\prime} k^{\prime}}^{i^{\prime}}=\frac{\partial x^{i^{\prime}}}{\partial x^{l}} \frac{\partial x^{m}}{\partial x^{j^{\prime}}} \frac{\partial x^{n}}{\partial x^{k^{\prime}}} \Gamma_{m n}^{l}+\frac{\partial x^{i^{\prime}}}{\partial x^{r}} \frac{\partial^{2} x^{r}}{\partial x^{j^{\prime}} \partial x^{k^{\prime}}}
\end{equation}
\end{qt}
\textbf{The most useful form of the affine connection is expressed via the metric tensor.} In any coordinate system
\begin{equation}
\nabla_{i} g_{j k}=\frac{\partial X^{c}}{\partial x^{i}} \frac{\partial X^{b}}{\partial x^{j}} \frac{\partial X^{a}}{\partial x^{k}} \partial_{c} g_{a b}=0
\end{equation}
where $g_{a b} \equiv \delta_{a b}, \quad$ and hence $\quad \partial_{c} g_{a b} \equiv \nabla_{c} g_{a b} \equiv 0$. If we apply to the above equation, the explicit form of the covariant derivative, we arrive at the equation:
$$
\nabla_{i} g_{j k}=\partial_{i} g_{j k}-\Gamma_{j i}^{1} g_{l k}-\Gamma_{k i}^{l} g_{l j}=0
$$
Making permutations of indices, we get:
$$
\begin{array}{l}
{\partial_{i} g_{j k}=\Gamma_{j i}^{l} g_{l k}+\Gamma_{k i}^{l} g_{l j}} (i)\\
{\partial_{j} g_{i k}=\Gamma_{i j}^{l} g_{l k}+\Gamma_{k j}^{l} g_{i l}} (ii)\\
{\partial_{k} g_{i j}=\Gamma_{i k}^{l} g_{l j}+\Gamma_{j k}^{l} g_{l i}}(iii)
\end{array}
$$
Taking $(i)+(ii)-(iii)$, we arrive at the relationship:
$$
2 \Gamma_{i j}^{l} g_{l k}=\partial_{i} g_{j k}+\partial_{j} g_{i k}-\partial_{k} g_{i j}
$$
Contracting both parts with $\left.g^{k m} \text { (remember that } g^{k m} g_{m l}=\delta_{l}^{k}\right),$ we arrive at
\begin{qt}
\begin{equation}
\Gamma_{i j}^{m}=\frac{1}{2} g^{k m}\left(\partial_{i} g_{j k}+\partial_{j} g_{i k}-\partial_{k} g_{i j}\right)
\end{equation}
\end{qt}
As an application of these formulas, we can consider the derivation of the Laplace operator acting on scalar and vector fields. For any kind of field, the Laplace operator can be defined as
\begin{equation}
\Delta=g^{i j} \nabla_{i} \nabla_{j}
\end{equation}
In the case of a scalar field $\Psi,$ we have
\begin{equation}
\Delta \Psi=g^{i j} \nabla_{i} \nabla_{j} \Psi=g^{i j} \nabla_{i} \partial_{j} \Psi
\end{equation}
The second covariant derivative acts on the vector $\partial_i\Psi$, hence
\begin{equation}
\Delta \Psi=g^{i j}\left(\partial_{i} \partial_{j}-\Gamma_{i j}^{k} \partial_{k}\right) \Psi=\left(g^{i j} \partial_{i} \partial_{j}-g^{i j} \Gamma_{i j}^{k} \partial_{k}\right) \Psi
\end{equation}
Similarly, for the vector field $A^i(x)$, we obtain
\begin{equation}
\begin{split}
    \Delta A^{i}&=g^{j k} \nabla_{j} \nabla_{k} A^{i}=g^{j k}\left[\partial_{j}\left(\nabla_{k} A^{i}\right)-\Gamma_{k j}^{l} \nabla_{l} A^{i}+\Gamma_{l j}^{i} \nabla_{k} A^{l}\right]\\
    &=g^{j k}[\partial_{j}\left(\partial_{k} A^{i}+\Gamma_{i k}^{i} A^{l}\right)-\Gamma_{k j}^{l}\left(\partial_{l} A^{i}+\Gamma_{m l}^{i} A^{m}\right)+\Gamma_{l j}^{i}\left(\partial_{k} A^{l}+\Gamma_{m k}^{l} A^{m}\right)]\\
    &=g^{j k}(\partial_{j} \partial_{k} A^{i}+\Gamma_{i k}^{i} \partial_{j} A^{l}+A^{l} \partial_{j} \Gamma_{l k}^{i}-\Gamma_{k j}^{l} \partial_{l} A^{i}+\Gamma_{l j}^{i} \partial_{k} A^{l}\\
    &-\Gamma_{k j}^{l} \Gamma_{m l}^{i} A^{m}+\Gamma_{l j}^{i} \Gamma_{m k}^{l} A^{m})
\end{split}
\end{equation}
The formulas in the equations above for scalar and vector fields hold for any dimension $D$ and for an arbitrary choice of coordinates.
\begin{example}
Verify that for the scalar field $\Psi$, the following relation takes place:
$$
\Delta \Psi=div(grad\Psi)
$$
\textbf{Solution}:
$$
\operatorname{div} \operatorname{grad} \Psi=\nabla_{i}(\operatorname{grad} \Psi)^{i}=\nabla_{i}\left(g^{i j} \nabla_{j} \Psi\right)=\Delta \Psi
$$
The second equivalence is the result of raising subscript. And we also have $\nabla_{i}g^{i j}$.
\end{example}
\begin{example}
Prove the following relation between the contraction of the affine connection and the derivative of the metric determinant $g=det||g_{\mu\nu}||$:
\begin{equation}
\Gamma_{i j}^{j}=\frac{1}{2 g} \frac{\partial g}{\partial x^{i}}=\partial_{i} \ln \sqrt{g}
\end{equation}
\end{example}
\textbf{Solution:}
$$
\Gamma_{i j}^{j}=\frac{1}{2} \delta_{k}^{j} \Gamma_{i j}^{k}=\frac{1}{2} \delta_{k}^{j}g^{kl}\left(\partial_{i} g_{j l}+\partial_{j} g_{i l}-\partial_{l} g_{i j}\right)=\frac{1}{2} g^{j l} \partial_{i} g_{j l}=\frac{1}{2 g} \partial_{i} g
$$
The last equivalence holds because the derivative of the determinant $g$ equals:
$$
\dot{g}=\partial_i g=g g^{lj} \partial_i g_{jl}=g g^{jl} \partial_i g_{jl}
$$

\section{Grad,div and rot}
Here, we consider some important operations over vector and scalar fields. All the consideration here will be restricted to the Cartesian coordinates, but the results can be easily generalized to any other coordinates by using the transformation rule. \bluep{When transforming the relations into curvilinear coordinates, one has to take care to use only the Levi-Civita tensor $\epsilon_{ijk}$ and not the maximal antisymmetric symbol $E_{ijk}$}. The two objects are related as \redp{$\epsilon_{ijk}=\sqrt{g}E{ijk}$ and $E_{123}=1$.}

We already know
$$
\begin{array}{rl}
{\operatorname{div} \mathbf{V}=\partial_{\mathrm{a}}} & {\mathrm{V}^{\mathrm{a}}=\nabla \mathbf{V}} \\
{\operatorname{grad} \Psi=\hat{\mathbf{n}}^{a} \partial_{a} \Psi} & {=\nabla \Psi} \\
{\Delta} & {=g^{a b} \partial_{a} \partial_{b}}
\end{array}
$$
$$
\nabla=\mathbf{e}^{i} \nabla_{i}=g^{i j} \mathbf{e}_{j} \nabla_{i}
$$
$$
\operatorname{rot} \mathbf{V}=\left|\begin{array}{ccc}
{\mathbf{i}} & {\mathbf{j}} & {\mathbf{k}} \\
{\partial_{x}} & {\partial_{y}} & {\partial_{z}} \\
{V_{x}} & {V_{y}} & {V_{z}}
\end{array}\right|=\left|\begin{array}{ccc}
{\hat{\mathbf{n}}_{1}} & {\hat{\mathbf{n}}_{2}} & {\hat{\mathbf{n}}_{3}} \\
{\partial_{1}} & {\partial_{2}} & {\partial_{3}} \\
{V_{1}} & {V_{2}} & {V_{3}}
\end{array}\right|=\quad E^{a b c} \hat{\mathbf{n}}_{a} \partial_{b} V_{c}
$$
One can easily prove the following important relations using antisymmetry of $E^{abc}$:
\begin{qt}
\begin{equation}
\text { rot grad } \Psi \equiv 0
\end{equation}
\begin{equation}
\operatorname{div}(\operatorname{rot} \mathbf{V}) \equiv 0
\end{equation}
\end{qt}
\subsection{Some important definitions}
\begin{defi}
        Consider a differentiable vector $\mathbf{C}(\mathbf{r}) .$ It is called \textbf{potential vector field}, if it can be presented as a gradient of some scalar field $\Psi(\mathbf{r})$ (called potential)
        \begin{equation}
\mathbf{C}(\mathbf{r})=\operatorname{grad} \Psi(\mathbf{r})
\end{equation}
\end{defi}
\begin{defi}
        A differentiable vector field $\mathbf{B}(\mathbf{r})$ is called \textbf{solenoidal}, if it can be presented as a rotor of some vector field $\mathbf{A}(\mathbf{r})$
        \begin{equation}
\mathbf{B}(\mathbf{r})=\operatorname{rot} \mathbf{A}(\mathbf{r})
\end{equation}
\end{defi}
The most known physical example of the solenoidal vector field is the magnetic field B, which is derived from the vector potential $A$ exactly through the equation above.
\begin{thm}
Suppose $\mathbf{V}(\mathbf{r})$ is a smooth vector field, defined in the whole $3 D$ space, which falls sufficiently fast at infinity. Then $\mathbf{V}(\mathbf{r})$ has unique (up to a gauge transformation) representation as a sum
$$
\mathbf{V}=\mathbf{C}+\mathbf{B}
$$
where C and B are potential and solenoidal fields correspondingly.
\end{thm}

\section{Grad, div, rot in other coordinates}
In cylindrical coordinates, we have a scalar field $\Psi$ and vector field $\mathbf{V}$:
$$
(\operatorname{grad} \Psi)_{r}=\frac{\partial \Psi}{\partial r}, \quad(\operatorname{grad} \Psi)_{\varphi}=\frac{1}{r} \frac{\partial \Psi}{\partial \varphi}, \quad(\operatorname{grad} \Psi)_{z}=\frac{\partial \Psi}{\partial z}
$$
$$
\operatorname{div} \mathbf{V}=\frac{1}{r} \frac{\partial}{\partial r}\left(r V^{r}\right)+\frac{1}{r} \frac{\partial V^{\varphi}}{\partial \varphi}+\frac{\partial V^{z}}{\partial z}
$$
$$
(\operatorname{rot} \mathbf{V})^{z}=\frac{1}{r} \frac{\partial\left(r V^{\varphi}\right)}{\partial r}-\frac{1}{r} \frac{\partial V^{r}}{\partial_{\varphi}}, \quad(\operatorname{rot} \mathbf{V})^{r}=\frac{1}{r} \frac{\partial V^{z}}{\partial \varphi}-\frac{\partial V^{\varphi}}{\partial z}
$$
$$
\begin{aligned}
&(\operatorname{rot} \mathbf{V})^{\varphi}=\frac{\partial V^{r}}{\partial z}-\frac{\partial V^{z}}{\partial r}\\
&\Delta \Psi=\frac{1}{r} \frac{\partial}{\partial r}\left(r \frac{\partial \Psi}{\partial r}\right)+\frac{1}{r^{2}} \frac{\partial^{2} \Psi}{\partial \varphi^{2}}+\frac{\partial^{2} \Psi}{\partial z^{2}}
\end{aligned}
$$
In spherical coordinates
$$
(\operatorname{grad} \Psi)_{r}=\frac{\partial \Psi}{\partial r}, \quad(\operatorname{grad} \Psi)_{\varphi}=\frac{1}{r \sin \theta} \frac{\partial \Psi}{\partial \varphi}, \quad(\operatorname{grad} \Psi)_{\theta}=\frac{1}{r} \frac{\partial \Psi}{\partial \theta}
$$
$$
\operatorname{div} \mathbf{V}=\frac{1}{r^{2}} \frac{\partial}{\partial r}\left(r^{2} V^{r}\right)+\frac{1}{r \sin \theta} \frac{\partial V^{\varphi}}{\partial \varphi}+\frac{1}{r \sin \theta} \frac{\partial}{\partial \theta}\left(V^{\theta} \sin \theta\right)
$$
$$
\Delta \Psi=\frac{1}{r^{2}} \frac{\partial}{\partial r}\left(r^{2} \frac{\partial \Psi}{\partial r}\right)+\frac{1}{r^{2} \sin \theta} \frac{\partial}{\partial \theta}\left(\sin \theta \cdot \frac{\partial \Psi}{\partial \theta}\right)+\frac{1}{r^{2} \sin ^{2} \theta} \frac{\partial^{2} \Psi}{\partial \varphi^{2}}
$$
$$
(\operatorname{rot} \mathbf{V})^{r}=\frac{1}{r \sin \theta}\left[\frac{\partial}{\partial \theta}\left(V^{\varphi} \cdot \sin \theta\right)-\frac{\partial V^{\theta}}{\partial \varphi}\right]
$$
$$
(\operatorname{rot} \mathbf{V})^{\varphi}=\frac{1}{r} \frac{\partial}{\partial r}\left(r V^{\theta}\right)-\frac{1}{r} \frac{\partial V^{r}}{\partial \theta}, \quad(\operatorname{rot} \mathbf{V})^{\theta}=\frac{1}{r \sin \theta} \frac{\partial V^{r}}{\partial \varphi}-\frac{1}{r} \frac{\partial}{\partial r}\left(r V^{\varphi}\right)
$$

\section{D-dimensional Integrals}
The first integral we will formulate is the $D$ -dimensional volume integral in curvilinear coordinates $x^{i}=x^{i}\left(X^{a}\right) .$ For such a formulation, we shall use the metric tensor and Levi-Civita tensor in the curvilinear coordinates. The square of the distance between the two points $X^{a}$ and $Y^{a}$ in Cartesian and arbitrary global coordinates is given by
$$
s_{x y}^{2}=\sum_{a=1}^{D}\left(X^{a}-Y^{a}\right)^{2}=g_{i j}\left(x^{i}-y^{i}\right)\left(x^{j}-y^{j}\right)
$$
the similar formula holds for the infinitesimal distances
\begin{qt}
\begin{equation}
d l^{2}=g_{i j} d x^{i} d x^{j}
\end{equation}
\end{qt}
Therefore, if we have a curve in $D$ -dimensional space $x^{i}=x^{i}(\tau)$ (here $\tau$ is an arbitrary monotonic parameter along the curve), then the length of the curve between the points $A$ with the coordinates $x^{i}(a)$ and $B$ with the coordinates $x^{i}(b)$ is
\begin{qt}
\begin{equation}
l_{A B}=\int_{(A B)} d l=\int_{a}^{b} d \tau \sqrt{g_{i j} \frac{d x^{i}}{d \tau} \frac{d x^{j}}{d \tau}}
\end{equation}
\end{qt}
\bluep{The direct geometric sense of the metric is that it defines a distance between two infinitesimally close points and also the length of the finite curve in the D-dimensional space, in arbitrary curvilinear coordinates}.

For the infinitesimal volume in D-dimensional space, we have
\begin{qt}
\begin{equation}
d V=\frac{1}{D !}\left|\varepsilon_{i_{1} i_{2} . . i_{p}}\right| d x^{i_{1}} \ldots d x^{i_{D}}=\sqrt{g} \cdot d x^{1} d x^{2} \ldots d x^{D}
\end{equation}
The quantity
\begin{equation}
J=\sqrt{g}=\sqrt{\operatorname{det}\left\|g_{i j}\right\|}=\left|\operatorname{det}\left(\frac{\partial X^{a}}{\partial x^{i}}\right)\right|=\left|\frac{D\left(X^{a}\right)}{D\left(x^{i}\right)}\right|
\end{equation}
is nothing but the well-known \textbf{Jacobian of the coordinate transformation.}
\end{qt}

\subsection{Curvilinear Integrals}
We now start to consider integrals over curves and curved surfaces.

\begin{defi}
        \textbf{curvilinear integral of the first type}
        
        Consider the continuous function $f(\mathbf{r})=f\left(x^{i}\right),$ where $x^{i}$ are the coordinates in $D$ -dimensional space, $x^{i}=\left\{x^{1}, x^{2}, \ldots, x^{D}\right\}$ defined along the curve $L=\int_{(L)} d l=\int_{a}^{b} \sqrt{g_{i j} \dot{x}^{i} \dot{x}^{j}} d t$. One can define the curvilinear integral of the first type
        \begin{equation}
I_{1}=\int_{(L)} f(\mathbf{r}) d l
\end{equation}
It is easy to see that this sum is also an integral sum for the Riemann integral.
\begin{equation}
\int_{a}^{b} \sqrt{g_{i j} \dot{x}^{i} \dot{x}^{j}} f\left(x^{i}(t)\right) d t
\end{equation}
\end{defi}
\begin{defi}
        \textbf{curvilinear integral of the second type}
        
 Consider a vector field $\mathbf{A}(\mathbf{r})$ defined along the curve $(L) .$ Let us consider the $3 D$ case, one can construct the following infinitesimal scalar:
\begin{equation}
\mathbf{A} \cdot d \mathbf{r}=A_{x} d_{x}+A_{y} d_{y}+A_{z} d_{z}=A_{i} d x^{i}
\end{equation}
This expression can be integrated along the curve (L) to give
\begin{equation}
\int_{(L)} \mathbf{A} \cdot d \mathbf{r}=\int_{(L)} A_{x} d x+A_{y} d y+A_{z} d z=\int_{(L)} A_{i} d x^{i}
\end{equation}
\end{defi}
If the curve is parametrized by the continuous monotonic parameter t, the last expression
can be presented as
\begin{equation}
A_{i} d x^{i}=\mathbf{A} \cdot d \mathbf{r}=\left(A_{x} \dot{x}+A_{y} \dot{y}+A_{z} \dot{z}\right) d t=A_{i} \dot{x}^{i} d t
\end{equation}
It is easy to establish the following relation between the curvilinear integral of the second type, the curvilinear integral of the first type, and the Riemann integral:
\begin{equation}
\int_{(L)} A_{x} d x+A_{y} d y+A_{z} d z=\int_{(L)} A_{i} \frac{d x^{i}}{d l} d l=\int_{a}^{b} A_{i} \frac{d x^{i}}{d t} d t
\end{equation}
where $l$ is a natural parameter along the curve
$$
l=\int_{a}^{t} \sqrt{g_{i j} \dot{x}^{i} \dot{x}^{j}} d t, \quad 0 \leq l \leq L
$$
In Cartesian coordinates, we have
\begin{equation}
\begin{aligned}
&\int_{(L)} \mathbf{A} \cdot d \mathbf{r}=\int_{0}^{L}\left(A_{x} \cos \alpha+A_{y} \cos \beta+A_{z} \cos \gamma\right) d l\\
&=\int_{a}^{b}\left(A_{x} \cos \alpha+A_{y} \cos \beta+A_{z} \cos \gamma\right) \sqrt{\dot{x}^{2}+\dot{y}^{2}+\dot{z}^{2}} d t
\end{aligned}
\end{equation}
where we used $d l^{2}=g_{a b} \dot{X}^{a} \dot{X}^{b} d t^{2}=\left(\dot{x}^{2}+\dot{y}^{2}+\dot{z}^{2}\right) d t^{2}$.
The main properties of the curvilinear integral of the first type are
\begin{qt}
\[
\text { additivity } \int_{A B}+\int_{B C}=\int_{A C} \text { and symmetry } \int_{A B}=\int_{B A}
\]
\end{qt}
The main properties of the curvilinear integral of the second type are
\begin{qt}
\[
\text { additivity } \int_{A B}+\int_{B C}=\int_{A C} \text { and antisymmetry } \int_{A B}=-\int_{B A}
\]
\end{qt}

\subsection{2D surfaces Integrals in a 3D Space}
Consider the integrals over the $2 D$ surface, $(S)$ in the $3 D$ space $(S) \subset \mathbb{R}^{3} .$ Suppose the surface is defined by the three smooth functions
$$
x=x(u, v), \quad y=y(u, v), \quad z=z(u, v)
$$
where $u$ and $v$ are independent variables which are called internal coordinates on the surface. One can regard the relations above as a mapping of a figure $(G)$ on the plane with coordinates $u$ and $v$ into the $3 D$ space with the coordinates $x, y, z$. 

Let us start by considering the infinitesimal line element $dl$ linking two points of the surface. We can suppose that these two infinitesimally close points belong to the same smooth curve $u=u(t),v=v(t)$, situated on the surface. Then the line element is
$$
d l^{2}=d x^{2}+d y^{2}+d z^{2}
$$
where
$$
\begin{aligned}
&d x=\left(\frac{\partial x}{\partial u} \frac{\partial u}{\partial t}+\frac{\partial x}{\partial v} \frac{\partial v}{\partial t}\right) d t=x_{u}^{\prime} d u+x_{v}^{\prime} d v\\
&d y=\left(\frac{\partial y}{\partial u} \frac{\partial u}{\partial t}+\frac{\partial y}{\partial v} \frac{\partial v}{\partial t}\right) d t=y_{u}^{\prime} d u+y_{v}^{\prime} d v\\
&d z=\left(\frac{\partial z}{\partial u} \frac{\partial u}{\partial t}+\frac{\partial z}{\partial v} \frac{\partial v}{\partial t}\right) d t=z_{u}^{\prime} d u+z_{v}^{\prime} d v
\end{aligned}
$$
Thus, 
\begin{equation}
dl^2=g_{u u} d u^{2}+2 g_{u v} d u d v+g_{v d} d v^{2}
\end{equation}
where
\begin{equation}
\begin{aligned}
&g_{u u}=x_{u}^{\prime 2}+y_{u}^{\prime 2}+z_{u}^{\prime 2}\\
&g_{u v}=x_{u}^{\prime} x_{v}^{\prime}+y_{u}^{\prime} y_{v}^{\prime}+z_{u}^{\prime} z_{v}^{\prime}\\
&g_{vv}=x_{v}^{\prime 2}+y_{v}^{\prime 2}+z_{v}^{\prime 2}
\end{aligned}
\end{equation}
are the elements of the matrix which is called \textbf{induced metric on the surface}.

We now calculate the infinitesimal element of \textbf{the area of the surface}. Here, we introduce two basis vectors on the surface in such a way that the scalar products of these vectors are equal to the corresponding metric components:
\begin{equation}
\mathbf{r}_{u}=\frac{\partial \mathbf{r}}{\partial u} \quad \text { and } \quad \mathbf{r}_{v}=\frac{\partial \mathbf{r}}{\partial v}
\end{equation}
Below we suppose that at any point of the surface $\mathbf{r}_{u} \times \mathbf{r}_{v} \neq 0 .$ This means that \bluep{the lines of constant coordinates $u$ and $v$ are not parallel or, equivalently, that the internal coordinates are not degenerate.}
\begin{equation}
g_{u u}=\mathbf{r}_{u} \cdot \mathbf{r}_{u}, \quad g_{u v}=\mathbf{r}_{u} \cdot \mathbf{r}_{v}, \quad g_{v v}=\mathbf{r}_{v} \cdot \mathbf{r}_{v}
\end{equation}
After we map the infinitesimal surface element $dudv$ on (G) to a curve surface (S), we have a parallelogram spanned on the vectors $\mathbf{r}_{u} d u$ and $\mathbf{r}_{v} d v$.The area of this parallelogram equals to the absolute value of the vector product of the two vectors,
\begin{equation}
\begin{aligned}
d S &=(d \mathbf{A})^{2}=\left|\mathbf{r}_{u} \times \mathbf{r}_{v}\right|^{2} d u^{2} d v^{2}=r_{u}^{2} r_{v}^{2} \sin ^{2} \alpha \cdot d u^{2} d v^{2} \\
&=d u^{2} d v^{2}\left\{r_{u}^{2} r_{v}^{2}\left(1-\cos ^{2} \alpha\right)\right\}=d u^{2} d v^{2}\left\{\mathbf{r}^{2} \cdot \mathbf{r}^{2}-\left(\mathbf{r}_{u} \cdot \mathbf{r}_{v}\right)^{2}\right\} \\
&=\left(g_{u u} \cdot g_{w}-g_{u v}^{2}\right) d u^{2} d v^{2}=g \cdot d u^{2} d v^{2}
\end{aligned}
\end{equation}
where $g$ is the determinant of the induced metric. Finally, the area $S$ of the whole surface (S) may be defined as an integral 
\begin{qt}
\begin{equation}
S=\iint_{(G)} \sqrt{g} d u d v
\end{equation}
\end{qt}
\begin{defi}
        Suppose we have a surface (S) defined as a mapping of the closed finite figure (G) in a $uv$-plane, and a function $f(u,v)$ on this surface. The surface integral of the first type is defined as:
        \begin{equation}
\mathcal{I}_{1}=\iint_{(S)} f(u, v) d S=\iint_{(G)} f(u, v) \sqrt{g} d u d v
\end{equation}
\end{defi}
This surface integral is a scalar, if the function $f(u,v)$ is a scalar. when we change the internal coordinates on the surface to $(u^{\prime},v^{\prime})$, the following aspects must change:
\begin{qt}
\begin{itemize}
    \item The form of the area $(G) \rightarrow\left(G^{\prime}\right)$
    \item the surface element $\sqrt{g} \rightarrow \sqrt{g^{\prime}}$
    \item the form of the integrand $f^{\prime}\left(u^{\prime}, v^{\prime}\right)=f\left(u\left(u^{\prime}, v^{\prime}\right), v\left(u^{\prime}, v^{\prime}\right)\right)$
\end{itemize}
\end{qt}
Below we define \textbf{the surface integral of the second type on smooth two-sided oriented surfaces}, for non-oriented (one-sided) surface, like the Möbius band does not have the property of
$$
\left|\mathbf{r}_{u} \times \mathbf{r}_{v}\right| \neq 0
$$
where $\mathbf{r}_{u}=\frac{\partial \mathbf{r}}{\partial u} \quad$ and $\quad \mathbf{r}_{v}=\frac{\partial \mathbf{r}}{\partial v}$. The area of the corresponding parallelogram on $(S)$ is $d S=\left|\mathbf{r}_{u} \times \mathbf{r}_{v}\right| d u d v$, and $d\mathbf{S}=\vec{\mathbf{n}}\cdot dS$. For calculational purposes, it is useful to introduce the components of $\hat{\mathbf{n}} ;$ using the same cosines, we already considered for the curvilinear second-type integral,
\begin{equation}
\cos \alpha=\hat{\mathbf{n}} \cdot \hat{\mathbf{i}}, \quad \cos \beta=\hat{\mathbf{n}} \cdot \hat{\mathbf{j}}, \quad \cos \gamma=\hat{\mathbf{n}} \cdot \hat{\mathbf{k}}
\end{equation}
\begin{defi}
         Consider a continuous vector field $\mathbf{A}(\mathbf{r})$ defined on the surface $(S) .$ The surface integral of the second type is
         \begin{equation}
\iint_{(S)} \mathbf{A} \cdot d \mathbf{S}=\iint_{S} \mathbf{A} \cdot \mathbf{n} d S
\end{equation}
By construction, the surface integral of the second type equals to the following double integral over the figure $(G)$ in the $u v$ -plane:
\begin{equation}
\iint_{(S)} \mathbf{A} \cdot d \mathbf{S}=\iint_{(G)}\left(A_{x} \cos \alpha+A_{y} \cos \beta+A_{z} \cos \gamma\right) \sqrt{g} d u d v
\end{equation}
\end{defi}

In a more general situation, we have an $n-$dimensional surface embedded into $D-$dimensional space $R^D$, with $D>n$. Introducing internal coordinates $u^i$ on the surface, we obtain its parametric equation of the surface in the form
\begin{equation}
x^{\mu}=x^{\mu}\left(u^{i}\right)
\end{equation}
where
$$
\left\{\begin{array}{c}
{i=1, \ldots, n} \\
{\mu=1, \ldots, D}
\end{array}\right.
$$
The expression for the distance between the two infinitesimally close points is then
\begin{equation}
d s^{2}=g_{\mu v} d x^{\mu} d x^{v}=\frac{\partial x^{\mu}}{\partial u^{i}} \frac{\partial x^{v}}{\partial u^{j}} g_{\mu v} d u^{i} d u^{j}
\end{equation}
Therefore, in this general case, we meet the same relation as for the usual coordinate transformation
\begin{equation}
g_{i j}(u)=\frac{\partial x^{\mu}}{\partial u^{i}} \frac{\partial x^{v}}{\partial u^{j}} g_{\mu v}(x)
\end{equation}
\bluep{The main difference between this relation and the usual tensor law of transforming metric is that the matrix $\frac{\partial x^{\mu}}{\partial u^{i}}$ has dimension $n \times D$ and hence it cannot be inverted.} The volume element of the area (surface element) in the n-dimensional case also looks as
\begin{equation}
d S=J d u^{1} d u^{2} \ldots d u^{n}, \quad \text { where } \quad J=\sqrt{g}
\end{equation}

\section{Theorems of Green, Stokes, and Gauss}
\begin{thm}
For the surface (S) dividable to a finite number of pieces $(S_i)$ of the form:
$$
\left\{\begin{array}{l}
{x=a} \\
{x=b}
\end{array} \quad \text { and } \quad\left\{\begin{array}{l}
{y=\alpha(x)} \\
{y=\beta(x)}
\end{array}\right.\right.
$$
and for the functions $P(x, y), \quad Q(x, y)$ that are continuous on (S) together with their first derivatives $P_{x}^{\prime}, \quad P_{y}^{\prime}, \quad Q_{x}^{\prime}, \quad Q_{y}^{\prime}$, the following relation holds:
\begin{equation}
\iint_{(S)}\left(\frac{\partial Q}{\partial x}-\frac{\partial P}{\partial y}\right) d x d y=\oint_{(\partial S+)} P d x+Q d y
\end{equation}
\end{thm}
One can use the Green's formula to calculate the area of the surface. To this end, one can take $Q=x, P=0$ or $Q=0, P=-y,$ or $Q=x / 2, P=-y / 2$ etc. Then
\begin{equation}
S=\oint_{\left(\partial S^{+}\right)} x d y=-\oint_{\left(\partial S^{+}\right)} y d x=\frac{1}{2} \oint_{\left(\partial S^{+}\right)} x d y-y d x
\end{equation}
\begin{thm}
\textbf{Stoke's Theorem}

For any continuous vector function $\mathbf{F}$ (r) with continuous partial derivatives $\partial_{i} F_{j},$ the following relation holds:
\begin{equation}
\iint_{(S)} \operatorname{rot} \mathbf{F} \cdot d \mathbf{S}=\oint_{(\partial S+)} \mathbf{F} \cdot d \mathbf{r}
\end{equation}
The last integral is called circulation of the vector field.
\end{thm}
At any point of the space
$$
\mathbf{F}=\mathbf{F}(u, v, w)=F^{u} \hat{\mathbf{n}}_{u}+F^{v} \hat{\mathbf{n}}_{v}+F^{w} \hat{\mathbf{n}}_{w}
$$
The components of the vectors $\mathbf{F},$ rot $\mathbf{F}, d \mathbf{S},$ and $d \mathbf{r}$ transform in a standard way, such that the products
$$
\operatorname{rot} \mathbf{F} \cdot d \mathbf{S}=\varepsilon^{i j k}\left(d S_{i}\right) \partial_{j} F_{k} \quad \text { and } \quad \mathbf{F} \cdot d \mathbf{r}=F_{i} d x^{i}
$$
$$
\oint_{(\partial S+)} \mathbf{F} \cdot d \mathbf{r}=\oint_{\left(\partial G^{+}\right)} \mathbf{F} \cdot d \mathbf{r}=\oint_{\left(\partial G^{+}\right)} F_{u} d u+F_{v} d v
$$
because the contour $\left(\partial G^{+}\right)$ lies in the $(u, v)$ plane. Further, since the figure $(G)$ belongs to the $(u, v)$ plane, the oriented area vector $d \mathbf{G}$ is parallel to the $\hat{\mathbf{n}}_{w}$ axis. Therefore, 
$$
(d \mathbf{G})_{u}=\left(d \mathbf{G}\right)_{v}=0
$$
Then
\begin{equation}
\begin{aligned}
\iint_{(S)} \operatorname{rot} \mathbf{F} \cdot d \mathbf{S} &=\iint_{(G)} \operatorname{rot} \mathbf{F} \cdot d \mathbf{G}=\int_{(G)}(\operatorname{rot} \mathbf{F})_{w} d G^{w} \\
&=\iint_{(G)}\left(\frac{\partial F_{v}}{\partial u}-\frac{\partial F_{u}}{\partial v}\right) d u d v
\end{aligned}
\end{equation}
and
\begin{equation}
\oint_{\left(\partial G^{+}\right)} F_{u} d u+F_{v} d v=\iint_{(G)}\left(\frac{\partial F_{v}}{\partial u}-\frac{\partial F_{u}}{\partial v}\right) d u d v
\end{equation}
\begin{thm}
 For the $\mathbf{F}(\mathbf{r})=\operatorname{grad} U(\mathbf{r}),$ where $U(\mathbf{r})$ is a smooth function of coordinates, the curvilinear integral between two points $A$ and $B$ doesn't depend on the path $(A B)$ and is equal to the difference
 \begin{equation}
\int_{(A B)} \mathbf{F} \cdot d \mathbf{r}=U(B)-U(A)
\end{equation}
\end{thm}
A useful criterion of $\mathbf{F}$ being $\operatorname{grad} U$ is
\begin{qt}
\begin{equation}
\frac{\partial F_{x}}{\partial z}=\frac{\partial F_{z}}{\partial x}
\end{equation}
and the same for any couple of partial derivatives.
\end{qt}
\begin{thm}
 \textbf{Gauss–Ostrogradsky Theorem}
 
 Consider a $3 D$ figure $(V) \subset R^{3}$ and also define $\left(\partial V^{+}\right)$ to be the externally oriented boundary of $(V) .$ Consider a vector field $\mathbf{E}(\mathbf{r})$ defined on $(V)$ and on its boundary $\left(\partial V^{+}\right)$ and suppose that the components of this vector field are continuous, as are their partial derivatives
 $$
\frac{\partial E^{x}}{\partial x}, \quad \frac{\partial E^{y}}{\partial y}, \quad \frac{\partial E^{z}}{\partial z}
$$
Then these components satisfy the following integral relation:
\begin{equation}
\oiint_{(\partial V+)} \mathbf{E} \cdot d \mathbf{S}=\iiint_{(V)} \operatorname{div} \mathbf{E} d V
\end{equation}
\end{thm}