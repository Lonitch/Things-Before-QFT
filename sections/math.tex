\chapter{Some Not-so-crazy Math}

\section{Total and Partial Derivatives}
To understand the different kinds of derivatives, let's say we have a function $\rho(t, x(t), p(t))$ which, in general, depends on the location $x(t)$ and momentum $p(t)$ plus the time $t.$ A key observation is that the location $x(t)$ and momentum $p(t)$ are functions of $t$ too. Therefore, we need to be extremely careful what we mean when we calculate the derivative with respect to the time $t .$
$$
\frac{d \rho}{d t}=\lim _{\Delta t \rightarrow 0} \frac{\rho(t+\Delta t, x(t+\Delta t), p(t+\Delta t))-\rho(t, x(t), p(t))}{\Delta t}
$$
\textbf{The result is the total rate of change of $\rho$}.
$$
\frac{\partial \rho}{\partial t}=\lim _{\Delta t \rightarrow 0} \frac{\rho(t+\Delta t, x(t), p(t))-\rho(t, x(t), p(t))}{\Delta t}
$$
The key difference is that we only vary $t$ if it appears explicitly
in $\rho$ but not if it only appears implicitly because $x(t)$ and $p(t)$
also depend on $t .$ Thus
$$
\frac{d \rho}{d t}=\frac{\partial \rho}{\partial x} \frac{d x}{d t}+\frac{\partial \rho}{\partial p} \frac{d p}{d t}+\frac{\partial \rho}{\partial t}
$$
\section{Taylor Expansion}
In general, we want to estimate the value of some function $f(x)$ at some value of $x$ by using our knowledge of the function's value at some fixed point $a .$ The Taylor series then reads
\begin{equation}
\begin{aligned}
f(x)=& \sum_{n=0}^{\infty} \frac{f^{(n)}(a)(x-a)^{n}}{n !} \\
=& \frac{f^{(0)}(a)(x-a)^{0}}{0 !}+\frac{f^{(1)}(a)(x-a)^{1}}{1 !}+\frac{f^{(2)}(a)(x-a)^{2}}{2 !} \\
&+\frac{f^{(3)}(a)(x-a)^{3}}{3 !}+\ldots
\end{aligned}
\end{equation}
or
\begin{equation}
f(x+a)=f(x)+(a \cdot \partial) f(x)+\frac{1}{2}(a \cdot \partial)^{2} f(x)+\cdots
\end{equation}
Taylor expansion of a scalar field (function $f$ that maps $\mathbb{R}^{n}$ to $\mathbb{R}$).Now, identify $\partial f / \partial t$ as $\hat{\boldsymbol{n}} \cdot \nabla f .$ In addition, see that $t \hat{\boldsymbol{n}}=\boldsymbol{x}-\boldsymbol{x}_{0} .$ Some clever recombining of terms gives
\begin{equation}
f(x)=f\left(x_{0}\right)+\left.\left(x-x_{0}\right) \cdot \nabla f\right|_{x_{0}}+\left.\frac{1}{2}\left(\left[x-x_{0}\right] \cdot \nabla\right)^{2} f\right|_{x_{0}}+\ldots
\end{equation}
and
\begin{equation}
\hat{\boldsymbol{n}} \cdot \nabla=\partial_{t}
\end{equation}

\section{Vector Identities}
\begin{equation}
\begin{aligned}
&\vec{\nabla} \cdot(\vec{\nabla} \times \vec{A}) \equiv \operatorname{div}(\operatorname{rot} \vec{A})=(\vec{\nabla} \times \vec{\nabla}) \cdot \vec{A} \equiv 0\\
&\vec{\nabla} \times(\vec{\nabla} \varphi) \equiv \operatorname{rot} \operatorname{grad} \varphi=(\vec{\nabla} \times \vec{\nabla}) \varphi \equiv 0
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
&\vec{\nabla} \cdot(\vec{A} \varphi)=\varphi \vec{\nabla} \cdot \vec{A}+\vec{A} \cdot \vec{\nabla} \varphi \quad \Longleftrightarrow \quad \operatorname{div}(\vec{A} \varphi)=\varphi \operatorname{div} \vec{A}+\vec{A} \cdot \operatorname{grad} \varphi\\
&\vec{\nabla} \times(\vec{A} \varphi)=\varphi \vec{\nabla} \times \vec{A}-\vec{A} \times \vec{\nabla} \varphi \quad \Longleftrightarrow \quad \operatorname{rot}(\vec{A} \varphi)=\varphi \operatorname{rot} \vec{A}-\vec{A} \times \operatorname{grad} \varphi\\
&\vec{\nabla} \cdot(\vec{A} \times \vec{B})=\vec{B} \cdot(\vec{\nabla} \times \vec{A})-\vec{A} \cdot(\vec{\nabla} \times \vec{B}) \quad \Longleftrightarrow \quad \operatorname{div}(\vec{A} \times \vec{B})=\vec{B} \cdot \operatorname{rot} \vec{A}-\vec{A} \cdot \operatorname{rot} \vec{B}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\vec{\nabla} \times(\vec{A} \times \vec{B}) &=(\vec{B} \cdot \vec{\nabla}) \vec{A}-(\vec{A} \cdot \vec{\nabla}) \vec{B}+\vec{A}(\vec{\nabla} \cdot \vec{B})-\vec{B}(\vec{\nabla} \cdot \vec{A}) \\
& \Longleftrightarrow \operatorname{rot}(\vec{A} \times \vec{B})=(\vec{B} \operatorname{grad}) \vec{A}-(\vec{A} \operatorname{grad}) \vec{B}+\vec{A}(\operatorname{div} \vec{B})-\vec{B}(\operatorname{div} \vec{A})
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
\vec{\nabla}(\vec{A} \cdot \vec{B})=&(\vec{B} \cdot \vec{\nabla}) \vec{A}+(\vec{A} \cdot \vec{\nabla}) \vec{B}+\vec{A} \times(\vec{\nabla} \times \vec{B})+\vec{B} \times(\vec{\nabla} \times \vec{A}) \\
& \Longleftrightarrow \operatorname{grad}(\vec{A} \cdot \vec{B})=(\vec{B} \cdot \operatorname{grad}) \vec{A}+(\vec{A} \cdot \operatorname{grad}) \vec{B}+\vec{A} \times \operatorname{rot} \vec{B}+\vec{B} \times \operatorname{rot} \vec{A}
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
&\vec{\nabla} \cdot(\vec{\nabla} \varphi) \equiv \operatorname{div}(\operatorname{grad} \varphi) \equiv \Delta \varphi=\frac{\partial^{2} \varphi}{\partial x^{2}}+\frac{\partial^{2} \varphi}{\partial y^{2}}+\frac{\partial^{2} \varphi}{\partial z^{2}}, \quad \Delta=\text { Laplace Operator }\\
&\vec{\nabla} \times(\vec{\nabla} \times \vec{A}) \equiv \operatorname{rot}(\operatorname{rot} \vec{A})=\vec{\nabla}(\vec{\nabla} \cdot \vec{A})-(\vec{\nabla} \cdot \vec{\nabla}) \vec{A} \equiv \operatorname{grad} \operatorname{div} \vec{A}-\Delta \vec{A}
\end{aligned}
\end{equation}

\section{Tensor Algebra and Analysis}
\subsection{Linear Spaces, Vectors, and Tensors}
The linear space, say $L$, consists of the elements (vectors) that \textbf{permit linear operations with the properties described below}:
\begin{itemize}
    \item Summing up the vectors
    \item Multiplication by a number
\end{itemize}
\begin{qt}
    Consider the set of vectors, elements of the linear space L
    $$
\left\{\mathbf{a}_{1}, \mathbf{a}_{2}, \ldots, \mathbf{a}_{n}\right\}=\left\{\mathbf{a}_{i} | i=1, \ldots, n\right\}
$$
and the set of real numbers $k_{1}, k_{2}, \ldots k_{n} .$ Vector
$$
\mathbf{k}=\sum_{i=1}^{n} k^{i} \mathbf{a}_{i}=\mathbf{k}^{i} \mathbf{a}_{i}=0\Longrightarrow \sum_{i=1}^{n}\left(k^{i}\right)^{2}=0
$$
then vectors $\left\{\mathbf{a}_{\mathbf{i}}, i=1, \ldots, n\right\}$ are called \textbf{linearly independent}. The last condition means that no one of the coefficients $k_{i}$ can be different from zero.
\end{qt}
For example, in 2D, two vectors are linearly dependent if and only if they are parallel; in 3D, three vectors are linearly dependent if and only if they belong to the same plane, etc.
\begin{defi}
        The maximal number of linearly independent elements of the linear space $L$ is called its dimension. It proves useful to denote $L_{D}$ a linear space of dimension $D .$
\end{defi}
\begin{thm}
Consider a $D$-dimensional linear space $L_{D}$ and the set of linearly independent vectors $\mathbf{e}_{i}=\left(\mathbf{e}_{1}, \mathbf{e}_{2}, \ldots, \mathbf{e}_{D}\right) .$ Then, for any vector a one can write
\begin{equation}
    \mathbf{a}=\sum_{i=1}^{D} a^{i} \mathbf{e}_{i}=a^{i} \mathbf{e}_{i}
    \label{vector-basis}
\end{equation}
where the coefficients $a^{i}$ are defined in a unique way.
\end{thm}
\bluep{The coefficients $a^{i}$ are called components or \textbf{contravariant components of the vector $\mathbf{a}$}}.The word “contravariant” here means that the components $a^i$ have upper indices.

\subsection{Direct product of the two linear spaces}
Let's consider the example of phase space in the classic mechanics. The coordinate system in the linear space $L_{D}$ consists of the initial point $O$ and the basis $\mathbf{e}_{i} .$ The position of a point $P$ can be characterized by \textbf{its position vector or radius vector $\mathbf{r}=\overrightarrow{O P}=x^{i} \mathbf{e}_{i}$}.

If the phase space is describing one particle, the space is composed of the radius vectors $\mathbf{r}$ and velocities $\mathbf{v}$ of the particle:
$$
\mathbf{r}=x^{i} \mathbf{e}_{i}, \quad \mathbf{v}=v^{j} \mathbf{f}_{j}
$$
The two set of bases can be related and independent. This example is a particular case of the linear space which is called \redp{a direct product
of the two linear spaces}. The element of the direct product of the two linear spaces $L_{D_{1}}$ and $L_{D_{2}}$ (\textbf{they can have different dimensions}) is the ordered set of the elements of each of the two spaces $L_{D_{1}}$ and $L_{D_{2}} .$
\begin{qt}
    The notation for the basis in the case of configuration space is $\mathbf{e}_{i} \otimes \mathbf{f}_{j}$. Hence, the state of the point-like particle in the phase space is characterized by the element of this linear space, which can be presented as:
    $$
x^{i} v^{j} \mathbf{e}_{i} \otimes \mathbf{f}_{j}
$$
In general, one can define the space that is a direct product of several linear spaces with different individual basis sets $\mathbf{e}_{i}$ each. In this case, we will have $\mathbf{e}_{i}^{(1)}, \mathbf{e}_{i}^{(2)}, \ldots$ $\mathbf{e}_{i}^{(N)} .$ The basis in the direct product space will be
$$
\mathbf{e}_{i_{1}}^{(1)} \otimes \mathbf{e}_{i_{2}}^{(2)} \otimes \ldots \otimes \mathbf{e}_{i_{N}}^{(N)}
$$
and the element becomes
$$
T^{i_{1} i_{2} \ldots i_{N}} \mathbf{e}_{i_{1}}^{(1)} \otimes \mathbf{e}_{i_{2}}^{(2)} \otimes \ldots \otimes \mathbf{e}_{i_{N}}^{(N)}
$$
\end{qt}
\subsection{Vector basis and its transformation}
Let us start from the components of the vector, which were defined in (\ref{vector-basis}). Consider, along with the original basis $\mathbf{e}_{i},$ another basis $\mathbf{e}_{i}^{\prime} .$ since each vector of the new basis belongs to the same space, it can be expanded using the original basis as
\begin{equation}
\mathbf{e}_{i}^{\prime}=\wedge_{i^{\prime}}^{j} \mathbf{e}_{j}
\label{basis-transform}
\end{equation}
and
$$
\mathbf{a}=a^{i} \mathbf{e}_{i}=a^{j^{\prime}} \mathbf{e}_{j^{\prime}}=a^{j^{\prime}} \wedge_{j^{\prime}}^{i} \mathbf{e}_{i}
$$
\begin{qt}
\begin{equation}
    a^{i}=a^{j^{\prime}} \wedge_{j^{\prime}}^{i}
    \label{cotravariant-coord-transform}
\end{equation}
\end{qt}

Similarly, we can make inverse transformation:
$$
a^{k^{\prime}}=\left(\wedge^{-1}\right)_{l}^{k^{\prime}} a^{l}
$$
and
\[
\left(\wedge^{-1}\right)_{l}^{k^{\prime}} \cdot \wedge_{i^{\prime}}^{l}=\delta_{i^{\prime}}^{k^{\prime}} \quad \text { and } \quad \wedge_{i^{\prime}}^{l} \cdot\left(\wedge^{-1}\right)_{k}^{i^{\prime}}=\delta_{k}^{l}
\]
\begin{qt}
Taking the partial derivatives, we arrive at the relations
\[
\wedge_{j^{\prime}}^{i}=\frac{\partial x^{i}}{\partial x^{j^{\prime}}} \quad \text { and } \quad\left(\wedge^{-1}\right)_{l}^{k^{\prime}}=\frac{\partial x^{k^{\prime}}}{\partial x^{l}}
\]
and
$$
\left(\wedge^{-1}\right)_{l}^{k^{\prime}} \cdot \wedge_{k^{\prime}}^{i}=\frac{\partial x^{k^{\prime}}}{\partial x^{l}} \frac{\partial x^{i}}{\partial x^{k^{\prime}}}=\frac{\partial x^{i}}{\partial x^{l}}=\delta_{l}^{i}
$$
is nothing but the chain rule for partial derivatives.
\end{qt}

\subsection{Scalar, vector, and tensor fields}
\begin{defi}
        Function $\varphi(x)$ is called scalar field or simply scalar if it does not transform under the change of coordinates
        \begin{equation}
\varphi(x)=\varphi^{\prime}\left(x^{\prime}\right)
\end{equation}
\end{defi}
Let us give a clarifying example in 1D. Consider a function
$$
y=x^2
$$
Now, let us change the variables 
$$
x^{\prime}=x+1
$$
The function $y=\left(x^{\prime}\right)^{2},$ obviously, represents another parabola. \bluep{In order to preserve the plot intact, we need to modify the form of the function, that is, to go from $\varphi$ to $\varphi^{\prime}$.} The new function $y^{\prime}=\left(x^{\prime}-1\right)^{2}$ will represent the original parabola,because the change of the variable is completely compensated by the change of the form of the function.
\begin{mybox}
\begin{center}
    Discuss whether the three numbers temperature $T(x)$, pressure $p(x)$ and density $\rho(x)$ form a contravariant vector
\end{center}
\end{mybox}
\begin{mybox2}
We can only form contravariant vector if the numbers transform by following the rule (\ref{cotravariant-coord-transform}). Since these numbers are scalar fields, it transforms like $T(\mathbf{r})=T^{\prime}(\mathbf{r}^{\prime})$. Thus, these three parameters can not form a contravariant vector.
\end{mybox2}
\begin{example}
From the definition above we know $\varphi^{\prime}(x) \neq \varphi(x)$ and $\varphi\left(x^{\prime}\right) \neq \varphi(x)$. Let us calculate these quantities explicitly for the special case of infinitesimal transformation $x^{\prime i}=x^{i}+\xi^{i}$ where $\xi$ are constant coefficients. Now we have
$$
\varphi\left(x^{\prime}\right)=\varphi\left(x+\xi\right)\overset{Taylor}{=}\varphi(x)+\frac{\partial \varphi}{\partial x^{i}} \xi^{i}
$$
and,
$$
\varphi^{\prime}\left(x^{i}\right)=\varphi^{\prime}\left(x^{i}-\xi^{i}\right)=\varphi^{\prime}\left(x^{\prime}\right)-\frac{\partial \varphi^{\prime}}{\partial x^{i^{\prime}}} \cdot \xi^{i}
$$
rewrite the two equations above, we find:
$$
\frac{\partial \varphi^{\prime}}{\partial x^{\prime i}}=\frac{\varphi^{\prime}\left(x^{\prime i}\right)-\varphi^{\prime}\left(x^{i}-\xi^{i}\right)}{\xi^i}
$$
$$
\frac{\partial \varphi}{\partial x^{i}}=\frac{\varphi(x^{\prime i})-\varphi(x^i)}{\xi^i}
$$
At the limit of $\xi\rightarrow0$, we have
$$
\frac{\partial \varphi^{\prime}}{\partial x^{\prime i}}=\frac{\partial \varphi(x)}{\partial x^{i}}+\mathcal{O}(\xi)
$$
As mentioned above, 
$$
\varphi^{\prime}\left(x^{i}\right)=\varphi^{\prime}\left(x^{\prime}\right)-\frac{\partial \varphi^{\prime}}{\partial x^{\prime i}} \cdot \xi^{i}\\
=\varphi^{\prime}\left(x^{\prime}\right)-\frac{\partial \varphi(x)}{\partial x^{i}}\cdot\xi^i
$$
Since $\varphi(x)=\varphi^{\prime}\left(x^{\prime}\right)$, we have
$$
\varphi^{\prime}(x)=\varphi(x)-\xi^{i} \partial_{i} \varphi
$$
where we have introduced a useful notation $\partial_{i}=\partial / \partial x^{i}$.
\end{example}